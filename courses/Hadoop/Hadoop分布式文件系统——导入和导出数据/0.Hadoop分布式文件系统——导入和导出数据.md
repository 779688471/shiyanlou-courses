# Hadoop 分布式文件系统 -- 导入和导出数据

## 一、实验介绍

在一个经典的数据架构中，Hadoop 是处理复杂数据流的核心。数据往往是从许多分散的系统中收集而来，并导入 Hadoop 分布式文件系统（HDFS）中，然后通过 MapReduce 或者其他基于 MapReduce 封装的语言（如 Hive、Pig 和 Cascading 等）进行处理，最后将这些已经过滤、转换和聚合过的结果导出到一个或多个外部系统中。

举个比较具体的例子，一个大型网站可能会做一些关于网站点击率的基础数据分析。从多个服务器中采集页面访问日志，并将其推送到 HDFS 中。启动一个 MapReduce 作业，并将这些数据作为 MapReduce 的输入，接下来数据将被解析、汇总以及与 IP 地址进行关联计算，最终得出 URL、页面访问量和每个 cookie 的地理位置数据。生成的相关结果可以导入关系型数据库中。即席查询（Ad-hoc query）此时就可以构建在这些数据上了。分析师可以快速地生成各种报表数据，例如，当前的独立用户数、用户访问最多的页面、按地区对用户进行拆分及其他的数据汇总。

本节的重点将关注 HDFS 数据的导入与导出，主要内容包含与本地文件系统、关系数据库、NoSQL 数据库、分布式数据库以及其他 Hadoop 集群之间数据的互相导入和导出。

### 1.1 课程来源

本课程基于 [异步社区](http://www.epubit.com.cn/) 的 [《Hadoop 实战手册》](http://www.epubit.com.cn/book/details/1494) 第 1 章制作，感谢 [异步社区](http://www.epubit.com.cn/) 授权实验楼发布。如需系统的学习本书，请购买[《Hadoop 实战手册》](http://www.epubit.com.cn/book/details/1494)。

为了保证可以在实验楼环境中完成本次实验，我们在原书内容基础上补充了一系列的实验指导，比如实验截图，代码注释，帮助您更好得实战。

如果您对于实验有疑惑或者建议可以随时在讨论区中提问，与同学们一起探讨。

### 1.2 实验知识点

- 使用 Hadoop shell 命令导入和导出数据到 HDFS
- Pig 脚本来演示下 getmerge 命令的功能
- 使用 distcp 实现集群间数据复制
- 使用 Sqoop 从 MySQL 数据库导入数据到 HDFS
- 使用 Sqoop 从 HDFS 导出数据到 MySQL

### 1.3 实验环境

- Hadoop-2.6.1
- Sqoop-1.4.5
- mysql
- Xfce 终端

### 1.4 适合人群

本课程难度为中等，适合具大数据基础的用户，如果对数据存储 模块有了解会更快的上手。

## 二、实验部分

`注意：实验楼环境里已经下载并安装 hadoop，及配置了环境变量，您只需要做以下步骤。`

```
su hadoop
#hadoop 密码是 hadoop
hadoop namenode -format 
#格式化后会最下面出现下面提示才表征格式化成功
#17/05/18 08:47:25 INFO common.Storage: Storage directory #/home/hadoop/tmp/dfs/na
#me has been successfully formatted.....

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495163502204.png/wm)

```
#启动 hadoop，并用 jps 检查是否启动进程
start-all.sh
jps

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495163870264.png/wm)

### 2.1 使用 Hadoop shell 命令导入和导出数据到 HDFS

HDFS 提供了许多 shell 命令来实现访问文件系统的功能，这些命令都是构建在 HDFS FileSystem API 之上的。Hadoop 自带的 shell 脚本是通过命令行来执行所有操作的。这个脚本的名称叫做 hadoop，通常安装在`$HADOOP_BIN`目录下，其中`$HADOOP_BIN`是 hadoop/bin 文件完整的安装目录，同时有必要将 `$HADOOP_BIN` 配置到 `$PATH` 环境变量中，这样所有的命令都可以通过 hadoop fs -command 这样的形式来执行。

如果需要获取文件系统的所有命令，可以运行 hadoop 命令传递不带参数的选项 fs。

```
hadoop fs

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495163949405.png/wm)

这些按照功能进行命名的命令的名称与 Unix shell 命令非常相似。使用 help 选项可以获得某个具体命令的详细说明。

```
hadoop fs –help ls

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495163995665.png/wm)

这些 shell 命令和其简要的参数描述可在官方在线文档 [http://hadoop.apache.org/docs/r2.6.1/hadoop-project-dist/hadoop-common/FileSystemShell.html](http://hadoop.apache.org/docs/r2.6.1/hadoop-project-dist/hadoop-common/FileSystemShell.html) 中进行查阅。

在这一节中，我们将使用 Hadoop shell 命令将数据导入 HDFS 中，以及将数据从 HDFS 中导出。这些命令更多地用于加载数据，下载处理过的数据，管理文件系统，以及预览相关数据。掌握这些命令是高效使用 HDFS 的前提。

**操作步骤：**

你需要在[实验楼](http://labfile.oss.aliyuncs.com/courses/832/weblog_entries.txt)网站上下载数据集 `weblog_entries.txt`。

```
sudo wget  http://labfile.oss.aliyuncs.com/courses/832/weblog_entries.txt

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495164098305.png/wm)

#### 1)．在 HDFS 中创建一个新文件夹，用于保存 `weblog_entries.txt` 文件：

```
hadoop fs -mkdir  -p /data/weblogs

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495164518434.png/wm)

#### 2)．将 `weblog_entries.txt` 文件从本地文件系统复制到 HDFS 刚创建的新文件夹下：

```
hadoop fs -copyFromLocal weblog_entries.txt /data/weblogs

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495164616703.png/wm)

#### 3)．列出 HDFS 上 `weblog_entries.txt` 文件的信息：

```
hadoop fs -ls /data/weblogs/weblog_entries.txt

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495164686163.png/wm)

**工作原理：**

Hadoop shell 非常轻量地封装在 HDFS FileSystem API 之上。在执行 hadoop 命令时，如果传进去的参数是 `fs`，实际上执行的是 org.apache.hadoop.fs.FsShell 这个类。在 0.20.2 版本中 FsShell 实例化了一个 org.apache.hadoop.fs.FileSystem 对象，并且将命令行参数与类方法映射起来。比如，执行 hadoop fs –mkdir /data/weblogs 相当于调用 FileSystem.mkdirs(new Path("/data/weblogs"))。同样，运行 hadoop fs –copyFromLocal weblog_entries.txt /data/weblogs 相当于在调用 FileSystem.copyFromLocal(newPath("weblog_entries.txt"), new Path("/data/weblogs"))。HDFS 数据复制到本地系统的实现方式也是一样，等同于调用 FileSystem.copyToLocal(newPath("/data/weblogs/ weblog_entries.txt"), new Path("./weblog_entries.txt"))。

更多关于文件系统的接口信息描述可以见官方文档:

[http://hadoop.apache.org/docs/r2.6.1/hadoop-project-dist/hadoop-common/FileSystemShell.html。](http://hadoop.apache.org/docs/r2.6.1/hadoop-project-dist/hadoop-common/FileSystemShell.html%E3%80%82)

mkdir 可以通过 hadoop fs -mkdir PATH1 PATH2 的形式来执行。例如，hadoop fs –mkdir /data/weblogs/20160511 /data/weblogs/20160501 将会在 HDFS 系统上分别创建两个文件夹 /data/weblogs/20160511 和 /data/weblogs/20160501。如果文件夹创建成功则返回 0，否则返回 - 1。

```
hadoop fs -mkdir /data/weblogs/20160511 /data/weblogs/20160501
hadoop fs -ls /data/weblogs

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495165596850.png/wm)

`copyFromLocal` 可以通过 hadoop fs –copyFromLocal LOCALFILEPATH URI 的形式来执行，如果 URI 的地址（指的是 HDFS://filesystemName:9000 这个串）没有明确给出，则默认会读取`core-site.xml` 中的 `fs.default.name` 这个属性。上传成功返回 0，否则返回 - 1。

`copyToLocal` 命令可以通过 hadoop fs –copyToLocal [-ignorecrc] [-crc] URILOCAL_FILE_PATH 的形式来执行。如果 URI 的地址没有明确的给出，则默认会读取 `core-site.xml` 中的 `fs.default.name` 这个属性。`copyToLocal` 会执行 CRC（Cyclic Redundancy Check）校验保证已复制的数据的正确性，一个失败的副本复制可以通过 参数 `–ignorecrc` 来强制执行，还可以通过 `-crc` 参数在复制文件的同时也复制 `crc` 校验文件。

上文介绍的`get`和`copyToLocal`只能对文件进行复制，无法对整个文件夹进行复制。当然 Hadoop 提供了`getmerge` 命令，可以将文件系统中的多个文件合并成一个单独的文件下载到本地文件系统。

### 2.2 Pig 脚本来演示下 `getmerge` 命令的功能

接下来我们将通过 Pig 脚本来演示下 `getmerge` 命令的功能。

1．下载 Pig 并解压：

```
cd /opt
#网络可能慢点，需要耐心等待
sudo wget http://mirrors.aliyun.com/apache/pig/pig-0.15.0/pig-0.15.0.tar.gz 
sudo tar xvf  pig-0.15.0.tar.gz

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495176658965.png/wm)

2．配置 pig 环境变量：

```
sudo vi ~/.bashrc
#下拉到最底部，添加pig路径，注意您打开的.bashrc里面hadoop变量已经添加，此处只需补充pig环境变量即可
export PIG_HOME=/opt/pig-0.15.0
export PIG_CLASSPATH=/opt/hadoop-2.6.1/conf
export HADOOP_HOME=/opt/hadoop-2.6.1

export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop-2.6.1/bin:/opt/hadoop-2.6.1/sbin:/opt/pig-0.15.0/bin

#输入：wq ，保存退出

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495176525459.png/wm)

刷新文件使配置生效。

```
source  ~/.bashrc

```

3．Pig 演示 getmerge 命令的功能

reduce 输出的数据存储在 HDFS 中，可以通过文件夹的名称来引用。若文件夹作为一个作业的输入，那么该文件夹下的所有文件都会被处理。上文介绍的 `get` 和 `copyToLocal` 只能对文件进行复制，无法对整个文件夹进行复制。当然 Hadoop 提供了 `getmerge` 命令，可以将文件系统中的多个文件合并成一个单独的文件下载到本地文件系统。

下面 Pig 脚本来演示下`getmerge` 命令的功能：

`weblogs_md5_group_pig.sh` 脚本如下：

```
weblogs = load '/data/weblogs/weblog_entries.txt' as
                (md5:chararray,
                  url:chararray,
                  date:chararray,
                  time:chararray,
                  ip:chararray);

md5_grp = group weblogs by md5 parallel 4;

store md5_grp into '/data/weblogs/weblogs_md5_groups.bcp';

```

```
#新建 weblogs_md5_group_pig.sh 脚本
sudo vi weblogs_md5_group_pig.sh
sudo chmod 777 -R weblogs_md5_group_pig.sh
more weblogs_md5_group_pig.sh

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495419388615.png/wm)

Pig 脚本可以通过下面的命令行来执行：

```
cd /opt/pig-0.15.0/bin
#脚本执行时间可能过长，请耐心等待
./pig -f /home/shiyanlou/weblogs_md5_group_pig.sh

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495419735131.png/wm)

`您可能会遇到如下错误`

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495420958622.png/wm)

`解决办法:继续等待输出，不用任何操作，下图为部分截图。`

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495421110453.png/wm)

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495421219989.png/wm)

该脚本逐行读取 HDFS 上的 `weblog_entries.txt` 文件，并且按照 `md5` 的值进行分组。`parallel` 是 Pig 脚本用来设置 reduce 个数的方法。由于启动了 4 个 reduce 任务，所以会在输出的目录 `/data/weblogs/weblogs_md5_groups.bcp` 中生成 4 个文件。

`注意，weblogs_md5_groups.bcp 实际上是一个文件夹，显示该文件夹的列表信息可以看到:`

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495421321208.png/wm)

在`/data/weblogs/weblogs_md5_groups.bcp` 中包含 4 个文件，即 `part-r-00000`、`part-r-00001`、`part-r-00002`和`part-r-00003`。

`getmerge` 命令可以用来将 4 个文件合并成一个文件，并且复制到本地的文件系统中，操作完我们可以看到本地文件列表，具体命令如下：

```
sudo chmod 777 -R /home/shiyanlou/
hadoop fs -getmerge /data/weblogs/weblogs_md5_groups.bcp weblogs_md5_groups.bcp
ll weblogs_md5_groups.bcp

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495421850337.png/wm)

### 2.3 使用 distcp 实现集群间数据复制

Hadoop 分布式复制（`distcp`）是 Hadoop 集群间复制大量数据的高效工具。`distcp` 是通过启动 MapReduce 实现数据复制的。使用 MapReduce 的好处包含可并行性、高容错性、作业恢复、日志记录、进度汇报等。Hadoop 分布式复制（`distcp`）对在开发集群环境、研究集群环境和生产集群环境之间进行数据复制十分有用。

**准备工作：**

首先必须保证复制源和复制目的地能够互相访问。

最好关闭复制源集群 map 任务的推测机制，可以在配置文件 `mapred-site.xml` 中将 `mapred.map.tasks.speculative.execution` 的值设置为 false 来实现，这样就可以避免在 map 任务失败的时候产生任何不可知的行为。

源集群和目的集群的 RPC 协议必须是一致。这意味着两个集群之间安装的 Hadoop 版本必须一致。

**操作步骤：**

`注意：由于实验环境的限制，下面只介绍步骤，并没有配置截图,仅供参考。`

完成以下几个步骤实现集群间的文件夹复制。

1．将集群 A 的 weblogs 文件夹复制到集群 B 上：

```
hadoop distcp hdfs://namenodeA/data/weblogs  hdfs://namenodeB/data/weblogs

```

2．将集群 A 的 weblogs 文件夹复制到集群 B 并覆盖已存在文件：

```
hadoop distcp –overwrite hdfs://namenodeA/data/weblogs hdfs://namenodeB/ data/weblogs

```

3．同步集群 A 和集群 B 之间的 weblogs 文件夹：

```
hadoop distcp –update hdfs://namenodeA/data/weblogs hdfs://namenodeB/data/ weblogs

```

**工作原理：**

在原集群，文件夹中的内容将被复制为一个临时的大文件。将会启动一个只有 map（map-only）的 MapReduce 作业来实现两个集群间的数据复制。默认情况下，每个 map 就将会分配到一个 256 MB 的数据块文件。举个例子，如果 weblogs 文件夹总大小为 10 GB，默认将会启动 40 个 map，每个 map 会复制大约 256 MB 的数据。`distcp` 复制也可以通过参数手动设置启动的 map 数量。

```
hadoop distcp –m 10 hdfs://namenodeA/data/weblogs hdfs://namenodeB/data/ weblogs

```

在上面这个例子中，将会启动 10 个 map 进行数据复制。如果 weblogs 文件夹的总大小是 10 GB，那么每个 map 会复制大约 1 GB 的数据。

如果要在运行的 Hadoop 版本不一样的两个集群之间进行数据复制，一般建议在复制源集群使用 `HftpFileSystem`。`HftpFileSystem` 是一个只读的文件系统。相应的 `distcp` 命令只能在目标服务器上运行：

```
hadoop distcp hftp://namenodeA:port/data/weblogs hdfs://namenodeB/data/ weblogs

```

在上面这条命令中，`port`的值要与配置文件`hdfs-site.xml`中`dfs.http.address`属性的端口值一致。

### 2.4 使用 Sqoop 从 MySQL 数据库导入数据到 HDFS

Sqoop 是 Apache 基金下的一个项目，是庞大 Hadoop 生态圈中的一部分。在很多方面 Sqoop 和 `distcp` 很相似。这两个工具都是构建在 MapReduce 之上的，利用了 MapReduce 的并行性和容错性。与集群间的数据复制不同，Sqoop 设计通过 JDBC 驱动连接实现 Hadoop 集群与关系数据库之间的数据复制。

它的功能非常广泛，本节将以网络日志条目为例展示如何使用 Sqoop 从 MySQL 数据库导入数据到 HDFS。

**准备工作：**

#### 1). 下载 Sqoop 并解压：

```
cd /opt/
sudo wget http://mirror.bit.edu.cn/apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
sudo tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495511149989.png/wm)

#### 2). 修改配置文件：

```
sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop-1.4.6 
#设置conf/sqoop-env.sh配置文件
cd sqoop-1.4.6/conf
sudo cp sqoop-env-template.sh  sqoop-env.sh
sudo vi sqoop-env.sh

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495511804646.png/wm)

设置 hadoop 运行程序所在路径和 hadoop-*-core.jar 路径 (Hadoop2.X 需要配置)。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495510153983.png/wm)

```
cd ../bin
sudo vi configure-sqoop
#注释掉HBase和Zookeeper等检查（除非使用HBase和Zookeeper等HADOOP上的组件）

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495512423783.png/wm)

输入如下命令验证是否正确安装 sqoop，如果正确安装则出现 sqoop 提示。

```
./sqoop help

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495518519799.png/wm)

#### 3). 下载 Mysql 驱动

在 `/opt` 目录下下载 mysql-connector 并解压。

```
cd /opt
sudo wget http://labfile.oss.aliyuncs.com/courses/809/mysql-connector-java-5.0.8.tar.gz
sudo tar -zxvf mysql-connector-java-5.0.8.tar.gz

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495518894341.png/wm)

#### 4). 将 MySQL JDBC 驱动包复制到 $SQOOP_HOME/lib 目录下

```
sudo cp /opt/mysql-connector-java-5.0.8/mysql-connector-java-5.0.8-bin.jar  /opt/sqoop-1.4.6/lib

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495519115029.png/wm)

#### 5). 实验楼环境默认已经安装 mysql ，启动即可。

```
sudo service mysql start
#设置 mysql root 用户密码
mysqladmin -uroot password "123456"
#用root密码登录验证是否成功
mysql -root -p123456 
exit

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495519494759.png/wm)

**操作步骤：**

完成以下步骤实现将 MySQL 表数据导出到 HDFS 中。

1．在 MySQL 实例中创建一个新数据库：

```
mysql  -uroot -p123456
CREATE DATABASE logs;

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495521147556.png/wm)

2．创建并载入表 weblogs：

```
use logs;
create table weblogs (
    md5            VARCHAR(32),
    url            VARCHAR(64),
    request_date   DATE,
    request_time   TIME,
    ip             VARCHAR(15)
);
show tables;

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495522885651.png/wm)

3．加载数据到表 weblogs：

```
load data infile '/home/shiyanlou/weblog_entries.txt' into table weblogs ;

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495524514001.png/wm)

4．查询 weblogs 表的行数：

```
select count(*) from weblogs;
exit

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495524655591.png/wm)

5．将 MySQL 数据导出到 HDFS：

使用如下命令列出 MySql 中所有数据库：

```
cd sqoop-1.4.6/bin
./sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root --password 123456

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495525430851.png/wm)

使用如下命令把 logs 数据库 weblogs 表数据导入到 HDFS 中

```
./sqoop import     --connect jdbc:mysql://localhost:3306/logs --username root --password 123456 --table weblogs --target-dir /data/weblogs/import

```

`注意：您可能会遇到下面的问题：`

连接 Jar 包版本问题, 可能`mysql-connector-java-5.0.8-bin.jar`版本太低。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495526458181.png/wm)

`解决方法:`

下载一个较高版本`mysql-connector-java-5.1.32.tar.gz`之后解压, 替换`$SQOOP_HOME/lib`原有 jar 包。

```
cd /opt
sudo wget http://labfile.oss.aliyuncs.com/courses/809/mysql-connector-java-5.1.32.tar.gz
sudo tar -zxf mysql-connector-java-5.1.32.tar.gz
sudo cp mysql-connector-java-5.1.32/mysql-connector-java-5.1.32-bin.jar  sqoop-1.4.6/lib/
sudo rm -rf   sqoop-1.4.6/lib/mysql-connector-java-5.0.8-bin.jar

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495527171428.png/wm)

继续尝试使用如下命令把 logs 数据库 weblogs 表数据导入到 HDFS 中。

```
./sqoop import     --connect jdbc:mysql://localhost:3306/logs --username root --password 123456 --table weblogs --target-dir /data/weblogs/import -m 1

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495527666372.png/wm)

输出结果将会是：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495527785454.png/wm)

**工作原理：**

Sqoop 连接数据库的 JDBC 驱动在 `--connect` 语句中定义，并从 `$SQOOP_HOME/lib`目录中加载相应的包，其中`$SQOOP_HOME` 为 Sqoop 安装的绝对路径。`--username`和`--password`选项用于验证用户访问 MySQL 实例的权限，`--target-dir` 选项指定导出数据的存放路径，`-m 1` 选项指定 map 的数量。mysql.user 表必须包含 Hadoop 集群每个节点的主机域名以及相应的用户名，否则 Sqoop 将会抛出异常，表明相应的主机不允许被连接到 MySQL 服务器。

```
mysql -uroot -p123456
use mysql
select host,user from user;

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495528935999.png/wm)

在这个例子中，我们使用 `root` 用户连接到 MySQL 服务器。`--table` 变量告诉 Sqoop 哪个表需要被导入。在我们的例子中，是要导入 `weblogs` 这个表到 `HDFS`。`--target-dir`变量决定了导出的表数据将被存储在`HDFS`的哪个目录下：

通过使用 `hadoop fs -ls` 命令 查看 `hdfs` 是否导入成功：

```
hadoop fs -lsr /data/weblogs/import

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495528099046.png/wm)

双击桌面浏览器，输入 [localhost：8088](http://www.shiyanlou/) [此处输入链接的描述](http://www.shiyanlou/)，可以看到刚才使用的`sqoop import`命令最终翻译成 MR 执行。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495528443345.png/wm)

默认情况下，导入的数据将按主键进行分割。如果导入的表并不包含主键，必须指定 `-m`或者`--split-by`参数决定导入的数据如何分割。在前面的例子中，使用了`-m`参数。`-m`参数决定了将会启动多少个 mapper 来执行数据导入。因为将`-m`设置为 1，所以就启动了一个 mapper 用于导入数据。每个 mapper 将产生一个独立的文件。

这行命令背后隐藏了相当复杂的逻辑。Sqoop 利用数据库中存储的元数据生成每一列的`DBWritable`类，这些类使用了`DBInputFormat`。`DBInputFormat`是 Hadoop 用来格式化读取数据库任意查询的结果。在前面的例子中，启动了一个使用 `DBInputFormat` 索引 weblogs 表内容的 MapReduce 作业。整个 weblogs 表被扫描并存储在 HDFS 的路径`/data/weblogs/import`下。

**更多参考：**

使用 Sqoop 导入数据还有很多有用的参数可以配置。Sqoop 可以分别通过参数`--as-avrodatafile`和`--as-sequencefile`将数据导入为 Avro 文件和序列化的文件。通过`-z`或者`--compress`参数可以在导入的过程中对数据进行压缩。默认的压缩方式为`GZIP`压缩，可以通过`--compression-codec <CODEC>`参数使用 Hadoop 支持的任何压缩编码。可以查看第 2 章的使用 LZO 压缩数据那一节的介绍。另一个有用的参数是`--direct`，该参数指示 Sqoop 直接使用数据库支持的本地导入导出工具。在前面的例子中，如果`--direct`被添加为参数，Sqoop 将使用`mysqldump`工具更快地导出 weblogs 表的数据。`--direct`参数非常重要以至于我们在运行前面的日志会打印出如下的日志信息：

```
WARN manager.MySQLManager: It looks like you are importing from mysql. 
WARN manager.MySQLManager: This transfer can be faster! Use the --direct 
WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.

```

### 2.5 使用 Sqoop 从 HDFS 导出数据到 MySQL

Sqoop 是 Apache 基金会下的一个项目，是庞大 Hadoop 生态圈中的一部分。在很多方面 Sqoop 和 distcp 很相似。这两个工具都是构建在 MapReduce 之上的，利用了 MapReduce 的并行性和容错性。与集群间的数据复制不同，Sqoop 设计通过 JDBC 驱动连接实现 Hadoop 集群与关系数据库之间的数据复制。它的功能非常广泛，本节将以网络日志条目为例展示如何使用 Sqoop 从 HDFS 导入数据到 MySQL 数据库。

它的功能非常广泛，本节将以网络日志条目为例展示如何使用 Sqoop 从 HDFS 导入数据到 MySQL 数据库。

**操作步骤：**

完成以下步骤实现将 HDFS 数据导出到 MySQL 表中。

1．在 MySQL 实例中创建表 weblogs_from_hdfs：

```
mysql -uroot -p123456
use logs;
create table weblogs_from_hdfs (
    md5             VARCHAR(32),
    url             VARCHAR(64),
    request_date    DATE,
    request_time    TIME,
    ip              VARCHAR(15)
);

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495529958102.png/wm)

2．从 HDFS 导出`weblog_entries.txt`文件到 MySQL：

```
./sqoop export --connect jdbc:mysql://localhost:3306/logs  --username root --password 123456 --table weblogs_from_hdfs --export-dir '/data/weblogs/weblog_entries.txt' -m 1 --fields-terminated-by '\t'

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495530896610.png/wm)

输出结果如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2951timestamp1495530990633.png/wm)

**工作原理：**

Sqoop 连接数据库的 JDBC 驱动可使用 `-connect` 参数声明，并从`$SQOOP_HOME/lib`目录中加载相应的包。其中`$SQOOP_HOME`为 Sqoop 安装的绝对路径。`--username`和`--password`选项用于验证用户访问 MySQL 实例的权限。`mysql.user`表必须包含 Hadoop 集群每个节点的主机域名以及相应的用户名，否则 Sqoop 将会抛出异常，表明相应的主机不允许被连接到 MySQL 服务器。

在这个例子中，我们使用`root`用户连接到 MySQL 服务器。`--table`参数决定了 HDFS 导出的数据将存储在哪个 MySQL 表中。这个表必须在执行 Sqoop `export`语句之前创建好。Sqoop 通过表的元数据信息、列数量以及列类型来校验 HDFS 需要导出目录中的数据并生成相应的插入语句。举个例子，导出作业可以被想象为逐行读取 HDFS 的`weblogs_entries.txt`文件并产生以下输出：

```
INSERT INTO weblogs_from_hdfs 
VALUES('aabba15edcd0c8042a14bf216c5', '/jcwbtvnkkujo.html', '2012-05- 10',   
'21:25:44', '148.113.13.214'); 

INSERT INTO weblogs_from_hdfs 
VALUES('e7d3f242f111c1b522137481d8508ab7', '/ckyhatbpxu.html', '2012- 05-10',   
'21:11:20', '4.175.198.160');

INSERT INTO weblogs_from_hdfs 
VALUES('b8bd62a5c4ede37b9e77893e043fc1', '/rr.html', '2012-05-10', '21:32:08',   
'24.146.153.181'); 
...

```

Sqoop `export`默认情况下是创建新增语句。如果`--update-key`参数被设置了，则将是创建更新语句。如果前面的例子使用了参数`--update-key md5`那么生成的 Sql 代码将运行如下：

```
UPDATE weblogs_from_hdfs SET url='/jcwbtvnkkujo.html', request_ date='2012-   
05-10'request_time='21:25:44' 
ip='148.113.13.214'WHERE md5='aabba15edcd0c8042a14bf216c5' 

UPDATE weblogs_from_hdfs SET url='/jcwbtvnkkujo.html', request_ date='2012-05-   
10' request_time='21:11:20' ip='4.175.198.160' WHERE md5='e7d3f242f111c1b   
522137481d8508ab7' 

UPDATE weblogs_from_hdfs SET url='/jcwbtvnkkujo.html', request_ date='2012-   
05-10'request_time='21:32:08' ip='24.146.153.181' WHERE md5='b8bd62a5c4ede37b   
9e77893e043fc1'

```

如果`--update-key`设置的值并没找到，可以设置`--update-mode`为`allowinsert`允许新增这行数据。

`-m`参数决定将配置几个 mapper 来读取 HDFS 上文件块。每个 mapper 各自建立与 MySQL 服务器的连接。每个语句将会插入 100 条记录。当完成 100 条语句也就是插入 10000 条记录，将会提交当前事务。一个失败的 map 任务，很可能导致数据的不一致，从而出现插入冲突数据或者插入重复数据。这种情况可以通过使用参数`--staging-table`来解决。这会促使任务将数据插入一个临时表，等待一个事务完成再将数据从临时表复制到`--table`参数配置的表中。临时表结构必须与最终表一致。临时表必须是一个空表否则需要配置参数`--clear-staging-table`。

## 三、实验总结

本章主要目的是演示 Hadoop shell 命令导入和导出数据到 HDFS，使用 distcp 实现集群间数据复制，Sqoop 在 MySQL 与 HDFS 之间互相导入数据，以及 Pig 脚本测试 getmerge 功能。

如果学完本课程，对书籍其他内容感兴趣欢迎点击以下链接购买书籍：

- [立即购买《Hadoop 实战手册》](http://www.epubit.com.cn/book/details/1494)