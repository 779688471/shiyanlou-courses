# 使用 Flume 收集数据

## 一、实验介绍

### 1.1 实验内容

Flume 是分布式的日志收集系统，可以处理各种类型各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义等，本节课主要讲解 Flume 的应用案例。

### 1.2 先学课程

- Flume 介绍与安装 [https://www.shiyanlou.com/courses/237](https://www.shiyanlou.com/courses/237)
- Hadoop 部署及管理 [https://www.shiyanlou.com/courses/35](https://www.shiyanlou.com/courses/35)

为了保证可以在实验楼环境中完成本次实验，我们在原书内容基础上补充了一系列的实验指导，比如实验截图，代码注释，帮助您更好得实战。

如果您对于实验有疑惑或者建议可以随时在讨论区中提问，与同学们一起探讨。

### 1.3 实验知识点

- Flume 核心概念 agent
- agent 里面包含 3 个核心组件：source、channel、sink。
- sink 组件是用于把数据发送到目的地的组件，目的地包括 hdfs、logger、avro、thrift、ipc、file、null、hbase、solr、自定义。

### 1.4 实验环境

- Hadoop-2.6.1
- Flume-1.6.0
- Xfce 终端

### 1.5 适合人群

本课程属于中等难度级别，适合具有大数据 hadoop 基础的用户，如果对数据采集了解，能够更好的上手本课程。

## 二、实验步骤

我们已经在实验楼环境里下载并配置启动 hadoop-2.6.1 所需的文件，免除您配置文件的麻烦，您可以在 `/opt` 找到，只需格式化并启动 hadoop 进程即可。

### 2.1　准备工作

双击打开桌面上的 Xfce 终端，用 `sudo` 命令切换到 hadoop 用户，hadoop 用户密码为 hadoop，用 `cd` 命令进入 `/opt`目录。

```
$ su hadoop
$ cd /opt/

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2657timestamp1491894012889.png/wm)

在 `/opt` 目录下格式化 hadoop。

```
$ hadoop-2.6.1/bin/hdfs namenode -format

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2657timestamp1491894114007.png/wm)

在 `/opt` 目录下启动 hadoop 进程。

```
$ hadoop-2.6.1/sbin/start-all.sh

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2657timestamp1491894286864.png/wm)

用 `jps` 查看 hadoop 进程是否启动。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2657timestamp1491894495396.png/wm)

您可以通过下面命令将 Flume 下载到实验楼环境中，进行安装配置

```
$ hadoop@907b3dc56edb:/home/shiyanlou$ cd /opt/
$ hadoop@907b3dc56edb:/opt$ sudo wget http://labfile.oss.aliyuncs.com/courses/785/apache-flume-1.6.0-bin.tar.gz
$ hadoop@945f39ae074b:/opt$ sudo tar -zxvf apache-flume-1.6.0-bin.tar.gz

```

修改 flume-env.sh

```
hadoop@907b3dc56edb:/opt$ cd apache-flume-1.6.0-bin/conf/
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin/conf$ sudo cp flume-env.sh.template  flume-env.sh   
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin/conf$ sudo vi flume-env.sh

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid18510labid2683timestamp1491983201736.png/wm)

flume-env.sh 文件需修改内容：

```
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
# Give Flume more memory and pre-allocate, enable remote monitoring via JMX
export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote"

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid18510labid2683timestamp1491983164525.png/wm)

修改 flume-conf.properties

```
hadoop@945f39ae074b:/opt/apache-flume-1.6.0-bin/conf$sudo cp flume-conf.properties.template  flume-conf.properties

```

用 `mkdir` 命令在 Flume 的解压包下创建 logs 目录，并用 `chmod` 命令给以权限。

```
$ sudo mkdir apache-flume-1.6.0-bin/logs 
$ sudo chmod 777 -R apache-flume-1.6.0-bin

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491374183860.png/wm)

### 2.2 案例一之 Spool

Spool 监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点：

- 拷贝到 spool 目录下的文件不可以再打开编辑。
- spool 目录下不可包含相应的子目录。

在 `/opt` 创建 agent 的配置文件 spool.conf。

```
hadoop@907b3dc56edb:/opt$ sudo vi apache-flume-1.6.0-bin/conf/spool.conf
# 添加如下内容

```

```
# Describe the agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.channels = c1
a1.sources.r1.spoolDir = /opt/apache-flume-1.6.0-bin/logs
a1.sources.r1.fileHeader = true

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

```

启动 Flume 代理。

```
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin$ bin/flume-ng agent -c conf -f conf/spool.conf -n a1 -Dflume.root.logger=INFO,console

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491374776212.png/wm)

另外开启一个 Xfce 终端，追加文件到 apache-flume-1.6.0-bin/logs 目录。

```
hadoop@907b3dc56edb:/opt$ echo "Hello World" > apache-flume-1.6.0-bin/logs/spool.log

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491375308448.png/wm)

在 Flume 代理这个 Xfce 终端可以看到以下相关信息：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491375497300.png/wm)

用快捷键 `Ctrl + c` 可以结束 Xfce 终端。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491376502335.png/wm)

查看 logs 目录下文件输出信息：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491375660760.png/wm)

### 2.3 案例二之 Exec

EXEC 执行一个给定的命令获得输出的源。

在 `/opt` 创建 agent 的配置文件 exec.conf。

```
hadoop@907b3dc56edb:/opt$ sudo vi apache-flume-1.6.0-bin/conf/exec.conf
# 添加如下内容

```

```
# Describe the agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe the source
a1.sources.r1.type = exec
a1.sources.r1.channels = c1
a1.sources.r1.command = tail -F /opt/apache-flume-1.6.0-bin/logs/log_exec_tail

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
~

```

启动 Flume 代理。

```
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin$ bin/flume-ng agent -c conf -f conf/exec.conf -n a1 -Dflume.root.logger=INFO,console

```

另外开启一个 Xfce 终端，用脚本输出信息到 /opt/apache-flume-1.6.0-bin/log_exec_tail，如图示：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491379038464.png/wm)

```
for i in {1..1000}
do
echo "exec tail$i" >> /opt/apache-flume-1.6.0-bin/logs/log_exec_tail
done

```

在 Flume 代理这个 Xfce 终端可以看到以下相关信息：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491378976957.png/wm)

用快捷键 `Ctrl + c` 可以结束 Xfce 终端。

查看 logs 目录下文件输出信息：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491379143930.png/wm)

### 2.4 案例三之 JSONHandler

从远程客户端接收数据。

在 `/opt` 创建 agent 的配置文件 json.conf。

```
hadoop@907b3dc56edb:/opt$ sudo vi apache-flume-1.6.0-bin/conf/json.conf
# 添加如下内容

```

```
# Describe the agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe the source
a1.sources.r1.type = org.apache.flume.source.http.HTTPSource
a1.sources.r1.port = 8888
a1.sources.r1.channels = c1


# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100


# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
~

```

启动 Flume 代理。

```
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin$ bin/flume-ng agent -c conf -f conf/json.conf -n a1 -Dflume.root.logger=INFO,console

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491381956189.png/wm)

生成 JSON 格式的 POST request。

```
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin$  curl -X POST -d '[{ "headers" :{"a" : "a1","b" : "b1"},"body" : "shiyanlou.org_body"}]' http://localhost:8888

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491382624807.png/wm)

在 Flume 代理这个终端可以看到以下相关信息：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491382668742.png/wm)

用快捷键 `Ctrl + c` 可以结束 Xfce 终端。

### 2.5 案例四之 Syslogtcp

接下来，我们将要介绍如何把数据写入 HDFS。

在 `/opt` 创建 agent 配置文件 syslogtcp.conf。

```
hadoop@907b3dc56edb:/opt$ sudo vi apache-flume-1.6.0-bin/conf/syslogtcp.conf
# 添加如下内容

```

```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = syslogtcp
a1.sources.r1.port = 4444
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = hdfs://localhost:9000/user/hadoop/syslogtcp
a1.sinks.k1.hdfs.filePrefix = Syslog
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
~
~

```

启动 Flume 代理。

```
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin$ bin/flume-ng agent -c conf -f conf/syslogtcp.conf -n a1 -Dflume.root.logger=INFO,console

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491383608772.png/wm)

测试产生 syslog。

```
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin$  echo "hello  flume" | nc localhost 4444

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491383829292.png/wm)

在 Flume 代理这个终端可以看到以下相关信息：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491383881118.png/wm)

用快捷键 `Ctrl + c` 可以结束 Xfce 终端。

再次打开一个 Xfce 终端，检查 hdfs 上是否生成文件。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491384124633.png/wm)

### 2.6 案例五之 File Roll Sink

接下来，我们将要介绍写入稍微复杂的文件数据，把动态生成的时间戳和数据一同写入 HDFS。

在 `/opt` 创建 agent 的配置文件 file_roll.conf。

```
hadoop@907b3dc56edb:/opt$ sudo vi apache-flume-1.6.0-bin/conf/file_roll.conf
# 添加如下内容

```

```
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = syslogtcp
a1.sources.r1.port = 5555
a1.sources.r1.host = localhost
a1.sources.r1.channels = c1

# Describe the sinkmior

a1.sinks.k1.type = file_roll
a1.sinks.k1.sink.directory = /opt/apache-flume-1.6.0-bin/logs

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
~

```

启动 Flume 代理。

```
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin$ bin/flume-ng agent -c conf -f conf/file_roll.conf -n a1 -Dflume.root.logger=INFO,console

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491442934870.png/wm)

另外开启一个 Xfce 终端中，测试产生 log。

```
hadoop@907b3dc56edb:/opt/apache-flume-1.6.0-bin$ echo "Hello world!"|nc localhost 5555

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491386460978.png/wm)

在 Flume 代理这个 Xfce 终端可以看到以下相关信息：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491443426224.png/wm)

用快捷键 `Ctrl + c` 可以结束 Xfce 终端。

再次打开一个 Xfce 终端，查看 /opt/apache-flume-1.6.0-bin/logs 下是否生成文件，默认每 30 秒生成一个新文件。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2797timestamp1491443668257.png/wm)

## 三、实验总结

经过这么多 flume 的例子测试，你会发现 flume 的功能真的很强大，可以进行各种搭配来完成你想要的工作，由于受实验环境的限制，在这里没有进行更为复杂的多级流测试，如果你有多个节点的话，不妨尝试下 Fan-in、Fan-out，多个 agent 协同工作，快去实验吧！

## 四、扩展阅读

- [https://cwiki.apache.org//confluence/display/FLUME/Getting+Started](https://cwiki.apache.org//confluence/display/FLUME/Getting+Started)
- [http://www.ituring.com.cn/book/1168](http://www.ituring.com.cn/book/1168)