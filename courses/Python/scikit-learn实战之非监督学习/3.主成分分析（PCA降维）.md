# scikit-learn 实战之非监督学习

## 一、实验介绍

### 1.1 实验内容

非监督学习（英语：Unsupervised learning）是机器学习中十分重要的一个分支。这是本实验课程的第 4 章节，将带你认识主成分分析，并学会使用 scikit-learn 完成数据降维操作。

### 1.2 实验知识点

- 主成分分析
- PCA 降维

### 1.3 实验环境

- python2.7
- Xfce 终端
- ipython 终端

### 1.4 适合人群

本课程难度为一般，属于初级级别课程，适合具有 Python 基础和线性代数基础，并对机器学习中分类问题感兴趣的用户。

### 1.5 代码获取

你可以通过下面命令将代码下载到实验楼环境中，作为参照对比进行学习。

```
$ wget http://labfile.oss.aliyuncs.com/courses/880/digits_pca_kmeans.py

```

## 二、什么是主成分分析？

主成分分析是多元线性统计里面的概念，它的英文是 Principal Components Analysis，简称 PCA。主成分分析旨在降低数据的维数，通过保留数据集中的主要成分来简化数据集。简化数据集在很多时候是非常必要的，因为复杂往往就意味着计算资源的大量消耗。通过对数据进行降维，我们就能在不较大影响结果的同时，减少模型学习时间。

主成分分析的数学基原理非常简单，通过对协方差矩阵进行特征分解，从而得出主成分（特征向量）与对应的权值（特征值）。然后剔除那些较小特征值（较小权值）对应的特征，从而达到降低数据维数的目的。

## 三、PCA 方法使用

下面，介绍一下 scikit-learn 中 PCA 方法的参数定义及简单使用，这是完成 PCA 主成分分析的基础。

导入 PCA 方法可以通过 `from sklearn.decomposition import PCA` 完成。PCA 函数及其默认参数如下所示。

```
sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto')

```

其中：

- `n_components=` 表示需要保留主成分（特征）的数量。
- `copy=` 表示针对原始数据降维还是针对原始数据副本降维。当参数为 False 时，降维后的原始数据会发生改变，这里默认为 True。
- `whiten=` 白化表示将特征之间的相关性降低，并使得每个特征具有相同的方差。
- `svd_solver=` 表示奇异值分解 SVD 的方法。有 4 参数，分别是：`auto`, `full`, `arpack`, `randomized`。

在使用 PCA 降维时，我们也会使用到 `PCA.fit()` 方法。`.fit()` 是 scikit-learn 训练模型的通用方法，但是该方法本身返回的是模型的参数。所以，通常我们会使用 `PCA.fit_transform()` 方法直接返回降维后的数据结果。

现在，可以从在线环境左下角`应用程序菜单 > 附件`打开 ipython 终端，使用 6 行代码简单测试一下上面提到的方法。

```
import numpy as np # 导入数值计算模块
from sklearn.decomposition import PCA # 导入 PCA 模块

data = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) # 新建一个 2 维数组
new_data = PCA(n_components=1).fit_transform(data) # 降维成 1 维并返回值

print(data) # 输出原数据
print(new_data) # 输出降维后的数据

```

我们可以看到，原先的 2 维数组已经降维为 1 维数组。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214893labid3190timestamp1499916126703.png/wm)

## 四、手写数字识别聚类

如果你完成了「scikit-learn 实战之监督学习」的课程，你应该记得里面有一个手写数字识别的分类实验。这里再简单描述一下，手写数字数据集一共由 1797 张 8x8 的影像组成。该数据集可以直接通过 scikit-learn 导入，无需到外部下载。

下面，我们可以先输出前 5 张图像预览一下。

```
from sklearn import datasets  # 导入数据集模块

import matplotlib.pyplot as plt  # 导入绘图模块

# 载入数据集
digits_data = datasets.load_digits()

# 绘制数据集前 5 个手写数字的灰度图
for index, image in enumerate(digits_data.images[:5]):
    plt.subplot(2, 5, index+1)
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')

plt.show()

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214893labid3158timestamp1499222020958.png/wm)

应该能大致看出来，上面的 5 张数字图像依次为 0，1，2，3，4。之前，我们用支持向量机完成了分类，即预测哪一张图像代表哪一个数字。现在，我们采用相同的数据集完成聚类分析，即将全部数据集聚为 10 个类别。

首先，我们导入常用的 numpy 数值计算模块和 matplotlib 绘图模块。由于原数据集维度达到 16，所以这里要进行 PCA 降维。

```
from sklearn import datasets
from matplotlib import pyplot as plt
from sklearn import decomposition
from sklearn.cluster import KMeans
import numpy as np

# 导入数据集
digits_data = datasets.load_digits()
X = digits_data.data
y = digits_data.target

# PCA 将数据降为 2 维
estimator = decomposition.PCA(n_components=2)
reduce_data = estimator.fit_transform(X)

```

接下来，将降维后的数据聚为 10 类，并将聚类后的结果、聚类中心点、聚类决策边界绘制出来。

```
# 建立 K-Means 并输入数据
model = KMeans(n_clusters=10)
model.fit(reduce_data)

# 计算聚类过程中的决策边界
x_min, x_max = reduce_data[:, 0].min() - 1, reduce_data[:, 0].max() + 1
y_min, y_max = reduce_data[:, 1].min() - 1, reduce_data[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, .05), np.arange(y_min, y_max, .05))

result = model.predict(np.c_[xx.ravel(), yy.ravel()])

# 将决策边界绘制绘制出来
result = result.reshape(xx.shape)

plt.contourf(xx, yy, result, cmap=plt.cm.Greys)
plt.scatter(reduce_data[:, 0], reduce_data[:, 1], c=y, s=15)

# 绘制聚类中心点
center = model.cluster_centers_
plt.scatter(center[:, 0], center[:, 1], marker='p', linewidths=2, color='b', edgecolors='w', zorder=20)

# 图像参数设置
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)

plt.show()

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214893labid3190timestamp1499930941300.png/wm)

图中，不同的色块区域代表一类。这里色块的颜色没有意义，只表示类别。散点代表数据，散点的颜色表示数据原始类别。我们可以看出，虽然原始数据已经从 16 维降维 2 维，但某几个数字的依旧有明显的成团现象。

## 五、实验总结

本次试验中，我们通过对原始数据集进行 PCA 降维操作，再用来进行聚类。其实，除了聚类以外，PCA 降维的思想应该被算作是数据预处理环节中必不可少的一部分。因为在一些机器学习模型中，维度就意味着灾难。

## 六、课后习题

1. 尝试降维时选择保留 3 个特征，再完成聚类实验。