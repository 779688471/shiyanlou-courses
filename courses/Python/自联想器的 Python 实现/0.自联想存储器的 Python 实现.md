#  自联想存储器的python实现 

## 一、课程介绍 

### 1.1 内容简介 

鸡尾酒会问题是人工智能领域中最有趣也是最有挑战性的问题之一。这是因为在酒会中，我们需要在众多的说话者中将注意力集中在某个说话者身上，且要随时在多个说话者之间切换注意力。人脑能够很好地解决这个问题，目前已有很多种尝试在机器中模拟这种能力的方法。

在这门课程中，我们将介绍其中一种基于Hebb学习规则的方法——_自联想存储器_。自联想存储器是当一个已存储序列的片段，或者加上噪音的版本再次出现时，能够回忆起该序列原貌的存储器。它能够有效地降低输入的噪音，或移除输入中的其他干扰。

### 1.2 知识点

- 生物神经元，神经突触
- Hebb 学习规则
- Hebb 学习规则的应用——自联想存储器

## 二、实验原理

### 生物神经元
在目前大火的机器学习与认知科学领域中，人工神经网络（以下简称为神经网络）是其中一种应用比较广泛的技术。神经网络是一种模拟生物神经网络的结构和功能的数学模型或计算模型。神经元作为生物大脑神经网络的基础，自然也是神经网络数据模型与计算模型模仿的基础。

![neuron](https://dn-anything-about-doc.qbox.me/document-uid291340labid2210timestamp1477648827821.png/wm)



神经元间的通信并不是通过直接接触，而是经由神经突触（间隙）的电信号或是化学信号的传递。当来自突触的信号量大于细胞膜的阈值时神经元激发，向轴突发送信号，其他情况则不发送信号，即不产生动作电位。

![synapse](https://dn-anything-about-doc.qbox.me/document-uid291340labid2210timestamp1477651888174.png/wm)

神经元的特点：

- 多输入
- 单输出

产生的动作电位类型：

- 兴奋型(EPSP, Excitatory Postsynaptic Potentials 兴奋性突触后电位)
- 抑制型(IPSP, Inhibitory Postsynaptic Potentials 抑制性突触后电位)


### 生物神经元数学模型

![model](https://dn-anything-about-doc.qbox.me/document-uid291340labid2210timestamp1477651977465.png/wm)

神经元输入抽象, 神经元在神经网络中作为一个计算组件，将输入 `$$x_1,x_2,\cdots,x_n$$` 与一个 `$$+1$$` 的偏置单元经过神经元后得到输出 `$$y$$` 。

`$$y=f(\sum_{i=1}^nw_ix_i+b)$$` 

其中 `$$b$$` 为偏置单元。


### Hebb 学习规则

神经网络的学习即是神经元间的连接权值调整过程 `$$w_i^{old}\rightarrow w_i^{new}$$` ，
基于 Hebb 理论：“当一个神经元 A 的轴突与神经元B很近足以刺激 B ，且持续或者重复地参与了对 B 的兴奋时，这两个神经元或其中一个就会发生某些生长过程或代谢变化”，换句话说即是，当突触 `$$w_{ij}$$` 两侧连接的神经元 `$$x_i,x_j$$` 同时兴奋，它们之间的连接权值就会增大。

### 自联想存储器的实现
自联想存储器的任务是学习输入输出向量间 `$${x_i,y_i}$$` 的转换，下图描述了自联想存储器的最简单的版本，所有的输入与输出为全连接的单层前向网络结构：

![network](https://dn-anything-about-doc.qbox.me/document-uid291340labid2210timestamp1477652000123.png/wm)

基于 hebb 规则，该网络中的权值 `$$w_{ij}$$` 的调整会根据以下规则进行调整：

`$$w_{ij}^{new}=w_{ij}^{old}+\alpha\cdot x_i\cdot y_j$$`


在训练阶段，自联想存储器的任务是学习模式对 `$$\{0,0\},\{1,1\},\{2,2\},\{3,3\}$$` 的匹配。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid291340labid2210timestamp1477652027272.png/wm)

学习规则：

`$$w_{ij}  =  w_{ij}+\alpha \cdot x_i(1) \cdot y_i(1) $$`

`$$ w_{ij}  =  w_{ij} + \alpha \cdot x_i(2) \cdot y_i(2) $$`

`$$ w_{ij}  = w_{ij} + \alpha \cdot x_i(3) \cdot y_i(3) $$`

`$$ w_{ij}  =  w_{ij} + \alpha \cdot x_i(4) \cdot y_i(4) $$`

在测试阶段，我们会对这些数字加了遮挡或是噪声之后作为输入。

![testData](https://dn-anything-about-doc.qbox.me/document-uid291340labid2210timestamp1477652058553.png/wm)

使用训练阶段学习到的 `$$w$$` 权值矩阵，并且由于为控制输出结果向量值为 0 或 1 ，我们采用硬限幅输函数作为激发函数：

`$$ 
hardlim(n)= 
\begin{cases}
1,  n \geq 0 \\
0,  n \lt 0 
\end{cases}
$$`

`$$hardlim$$` 硬限幅激活函数： 在给定网络的输入矢量或矩阵S时，返回该矢量或矩阵S的输出。当S中的元素值大于等于0时返回值为1，小于0时返回值为0. 即模拟突触中，当网络的输入达到阈值时，则硬限幅激活函数的输出为1，否则为0.

因此测试阶段的输出为：

`$$
y_j = hardlim(\sum_{i=1}^n w_{ij} \cdot x_i)
$$`

## 三、实验操作
### 3.1 安装 pillow, numpy 及 matplot
由于需要对训练集及测试集中的图片进行读取，及结果的展示和数值计算操作。因此我们需先下载安装 pillow, numpy及 matplot 这三个 Python 的第三方库。

```bash
$ sudo pip install pillow
$ sudo pip install numpy
$ sudo pip install matplotlib
```
由于需要导入第三方包因此还需安装python-tk
```bash
sudo apt-get install python-tk
```
### 3.2 程序实现

获取训练与测试集图片，并解压后进入文件目录创建 associativeMemory.py 文件
```bash
wget http://labfile.oss.aliyuncs.com/courses/679/exam.tar.gz
tar zxvf exam.tar.gz &amp;&amp; cd exam
touch associativeMemory.py
```

在编辑associativeMemory.py时，先导入实验相关的模块
```python
# -*- coding:utf-8 -*-
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
```
对输入图片作处理
```
def transform(img):
     # 将图片转换为列向量的形式，一般在机器学习的训练中都用一个列向量表示某个样本
     result = np.int_(img.reshape((-1,1)))
     length = len(result)
     # 由于输入的二值图片像素值为0(黑色)或非0(白色)
     # 为便于分类训练色像素点对应的分量值置为-1，其他置为1 
     for i in range(length):
        if result[i] == 0: 
            result[i] = -1            
        else:
            result[i] = 1;          
     return result 
```
训练函数：
```
def training(W,img,al): 
    p = transform(img)
    pLen = len(p)
    # 初始化权值矩阵W全0矩阵
    if W is None:
        W = np.zeros((pLen,pLen))
    t = p
    # 实现训练阶段基于hebb规则的学习过程
    # 使用np.dot()函数实现向量的内积运算
    result = W + al * np.dot(p,t.T);
    return result
```
实现hardlim硬限传输激发函数
```
def hardlim(a):   
    a[ a &gt;= 0 ] = 1
    a[a &lt; 0 ] = 0
    return a

```
使用训练阶段得到的W矩阵，实现神经元中的输出&#39;$$ y = f(W \cdot X) $$&#39;, 这里的`$$f$$`函数即是hardlim硬限传输激发函数， `$$W$$`为训练阶段得到的权值举证。
```
def testing(W, img):
     #调用激发函数hardlim控制输出结果为0或1    
     result = hardlim(np.dot(transform(img).T,W))      
     return result  
```

主函数处理从文件中读入数据，调用训练函数及测试函数测试训练结果并展示：
```
def main():
    #学习步长设置为0.5，可设置成不同值查看训练效果
    alpha = 0.5 
    #分别存放训练集与测试集的文件目录相对路径
    trainDir = &#34;train/&#34;
    testDir = &#34;test/&#34;
    trainImg = os.listdir(trainDir)
    trainImg.sort()
    
    trnLen = len(trainImg)
    W = None;
    
    # training
    for i in range(trnLen):
        img = np.array(Image.open(trainDir+trainImg[i]))
        #调用训练函数进行自联想存储器的训练
        W = training(W, img, alpha)
        
    #展示训练集中的数字图片
    count = 1
    for i in range(trnLen):
             img = Image.open(trainDir+trainImg[i])
             plt.subplot(1,trnLen,i+1)
             plt.title(str(i))
             plt.imshow(img,cmap=plt.cm.gray)
             
    # testing
    dirsAll = os.listdir(testDir)
    dirsAll.sort()
    dirLen = len(dirsAll) 
    count = 1
    #设置输出测试结果面板的大小
    plt.figure(figsize=(10,10))
    # 则测试集中，每个数字的图片都放置在一个文件夹中，因此先对每个子目录进行遍历
    for i in range(dirLen):
            files = os.listdir(testDir+dirsAll[i])
            files.sort()
            filesLen = len(files)
            # 进入每个数字图片对应的文件目录中
            # 取出对应于每个数字图片的遮挡或加噪音图片进行自联想存储器的测试
            for j in range(filesLen):
                 fullPath = testDir + dirsAll[i] + &#39;/&#39; + files[j]
                 img = np.array(Image.open(fullPath))
                 #利用训练得到的权值矩阵进行测试
                 imageResult = testing(W, img)
                     
                 DisSource = hardlim(transform(img).reshape(img.shape))
                 # 将对测试集的联想结果从列向量的形式转换为与输入图片一样的矩阵形式
                 DisResult = imageResult.reshape(img.shape)
                
                 plt.subplot(dirLen, 2 * filesLen , count)                 
                 plt.title(&#39;source&#39;)
                 count = count + 1
                 plt.imshow(DisSource,cmp=plt.cm.gray)
                     
                 plt.subplot(dirLen, 2 * filesLen, count)
                 plt.title(&#39;result&#39;)
                 count = count + 1                 
                 plt.imshow(DisResult,cmap=plt.cm.gray)
                 
    fig = plt.get_current_fig_manager()
    fig.window.wm_geometry(&#34;+100+0&#34;)             
    plt.show() 
```
## 四、实验结果
输入 `python associativeMemory.py` 运行自联想存储器，查看在数字 0, 1, 2, 3 在加入遮挡或噪音后的联想结果。

训练集：

![originalImage](https://dn-anything-about-doc.qbox.me/document-uid291340labid2210timestamp1477652086743.png/wm)

测试结果：

![learningResult](https://dn-anything-about-doc.qbox.me/document-uid291340labid2210timestamp1477652107768.png/wm)

## 五、总结
在本次课程中，我们介绍了生物神经元模型，及神经元突触间的联系和学习过程，并且介绍了 Hebb 规则及其简单应用——自联想存储器。

通过本次实验我们可以看出自联想存储器具有降噪或从众多输入中去除干扰的功能，这也是自联想存储器能够在解决鸡尾酒问题中发挥初步作用的原因。从实验中可看出在输入的数字有噪音或是遮挡的情况下，自联想存储器的联想结果也不是特别一致，与其训练效果有关的有学习步长或是整个网络层次结构的设置。目前许多机器学习里的训练算法都是基于 Hebb 规则进行改进，使得整个神经网络的学习性能更佳，不管是在物体，图像或是语音识别上都取得了不错的效果。因此通过此门课程学习到的 Hebb 规则，及通过设置自联想存储器帮助理解其工作原理后，有助于后续机器学习神经网络中训练算法的学习。

可通过以下命令获取此次实验的源码及数据:
```bash
wget http://labfile.oss.aliyuncs.com/courses/679/Associative_memory.tar.gz
```

## 六、参考材料及延伸阅读

- [人工神经网络与模拟进化计算-第11章](http://www.banshujiang.cn/e_books/1003)
- [人工神经网络简介](http://muchong.com/html/200506/99916.html)
- [自联想映射存储器](http://mall.cnki.net/magazine/Article/TYGY903.012.htm)