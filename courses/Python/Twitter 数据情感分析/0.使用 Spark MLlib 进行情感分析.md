# 使用Spark MLlib进行情感分析

## 一、实验说明

在当今这个互联网时代，人们对于各种事情的舆论观点都散布在各种社交网络平台或新闻提要中。我们可以在移动设备或是个人PC上轻松地发布自己的观点。对于这种网上海量分布地数据，我们可以利用文本分析来挖掘各种观点。如下图中，`CognoviLabs`利用Twitter上人们发布对于美国大选两个候选人的推特，进行情感分析的结果。从这张图我们也可以直观地感受到民意所向（此图发表日期为10月10日，早于今年美国大选的日子）。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid291340labid2380timestamp1480583362613.png/wm)

&gt; 图片来源：CognoviLabs / Twitris

本次课程，我们将利用推特上的数据结合Spark MLlib实现人们对美国这两位总统的情感分析，查看在美国不同地方的网民们对于他们的看法如何。

### 1.1 知识点

-  Twitter数据
-  Spark MLlib情感分析
-  Python 地图可视化工具Basemap

### 1.2 实验流程

1. 获取twitter流数据
2. tweet数据进行情感分析
3. 分析结果可视化

## 二、Twitter流数据

[Twitter Streaming API](https://dev.twitter.com/streaming/overview)为开发者提供了可用于获得*推特*上数据的开发者接口。大家可根据[Twitter Scrape](https://github.com/dataquestio/twitter-scrape)中的操作获取Twitter中有关__Trump__和__Hillary__的推特数据。

&gt; 由于对Twitter的操作需要翻墙，为方便大家进行后续实验操作，我们已将数据保存至`tweets.json`, `donald.json`及`hillary.json`中，__json__ 文件地址将在后续实验操作时给出。**tweets.json**包括和川普及希拉里有关的推特，而**donald.json**和**hillary.json**仅包括其文件名代表的候选人有关推特。

使用`Tweepy`接口获得的推特中包含很多数据，但由于我们只关心其发表内容的情感属性值，因此我们仅保存了其中的推特内容，及发表推特用户所在地。
我们所使用的Twitter数据内容：

| 数据名      | 数据描述       |
| ------------ |:-------------:|
|polarity|该twitter的情感属性值（0:积极，1:无感，2：消极）|
|text|推特内容|
|user_localtion|用户所在地区|

其中polarity的内容由python[自然语言处理包 TextBlob](https://textblob.readthedocs.io/en/dev/)获得，这里我们对textblob返回的polarity进行了特殊处理，设置polarity&gt;0时为积极情感（polarity=0）,polarity=0时为无感（polarity=1），polarity&lt;0时为消极情绪（polarity=2）


## 三、Spark MLLib进行情感分析

### 3.1环境配置

`Apach Spark`的Python接口`pyspark`安装：

```
#安装pyspark
$ wget http://labfile.oss.aliyuncs.com/courses/722/spark-2.0.2-bin-hadoop2.7.tgz
$ tar zxvf spark-2.0.2-bin-hadoop2.7.tgz
$ sudo mv spark-2.0.2-bin-hadoop2.7 /usr/bin/spark-2.0.2
```

进入目录`/usr/bin/spark-2.0.2/conf`中设置配置文件`spark-env.sh`。

```bash
$ cd /usr/bin/spark-2.0.2/conf

# 使用模板文件复制一份配置文件进行设置
$ sudo cp spark-env.sh.template spark-env.sh
$ vim spark-env.sh #编辑spark-env.sh，所使用的编辑器可根据个人爱好

# 使用vim或gedit编辑文件spark-env.sh
# 添加以下内容设置spark的环境变量
export SPARK_HOME=/usr/bin/spark-2.0.2
```

安装python相关的包：
```bash
wget http://labfile.oss.aliyuncs.com/courses/722/numpy-1.11.3-cp27-cp27mu-manylinux1_x86_64.whl
wget http://labfile.oss.aliyuncs.com/courses/722/pyparsing-2.1.10-py2.py3-none-any.whl
wget http://labfile.oss.aliyuncs.com/courses/722/pytz-2016.10-py2.py3-none-any.whl
wget http://labfile.oss.aliyuncs.com/courses/722/cycler-0.10.0-py2.py3-none-any.whl
wget http://labfile.oss.aliyuncs.com/courses/722/python_dateutil-2.6.0-py2.py3-none-any.whl
wget http://labfile.oss.aliyuncs.com/courses/722/matplotlib-1.5.3-cp27-cp27mu-manylinux1_x86_64.whl

sudo pip install numpy-1.11.3-cp27-cp27mu-manylinux1_x86_64.whl
sudo pip install pyparsing-2.1.10-py2.py3-none-any.whl
sudo pip install pytz-2016.10-py2.py3-none-any.whl
sudo pip install cycler-0.10.0-py2.py3-none-any.whl
sudo pip install python_dateutil-2.6.0-py2.py3-none-any.whl
sudo pip install matplotlib-1.5.3-cp27-cp27mu-manylinux1_x86_64.whl
```

可视化分析结果使用的是**python**的地图可视化第三方包`Basemap`，**basemap**是Python中一个可用于地理信息可视化的包，安装__basemap__过程如下：
```bash
$ wget http://labfile.oss.aliyuncs.com/courses/722/basemap-1.0.7.tar.gz
$ tar zxvf basemap-1.0.7.tar.gz
$ cd basemap-1.0.7
$ cd geos-3.3.3
$ ./configure
$ make
$ sudo make install
```

上述过程需等待一段比较漫长的时间，这期间可以先浏览后续实验步骤。当执行完毕后返回目录`basemap-1.0.7`并安装`basemap`
```bash
$ cd ..
# 先更新依赖包,再进行安装
$ sudo apt-get build-dep python-lxml
$ sudo python setup.py install

# 安装后更新，并安装python-tk
$ sudo apt-get update
$ sudo apt-get install python-tk
```

进入`examples`目录运行程序`simplestest.py`查看是否安装成功
```bash
$ cd examples
$ python simpletest.py
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2380timestamp1483682069033.png/wm)

### 3.2 实验操作

进入**Code**目录中并创建 **shiyanlou_cs722** 目录，通过以下命令获得`tweets.json`, `donald.json`及`hillary.json`文件。
```bash
$ wget http://labfile.oss.aliyuncs.com/courses/722/tweets.json
$ wget http://labfile.oss.aliyuncs.com/courses/722/donald.json
$ wget http://labfile.oss.aliyuncs.com/courses/722/hillary.json
```

创建`sparkSA.py`文件开始我们的实验。

&gt; **注意**： 本次实验操作的所有文件及代码都应存放在同一个目录——`shiyanlou_cs722`下。

导入相关模块

```python
# -*- coding=utf8 -*- 
from __future__ import print_function
import json
import re
import string
import numpy as np

from pyspark import SparkContext, SparkConf
from pyspark import SQLContext
from pyspark.mllib.classification import NaiveBayes
from pyspark.mllib.tree import RandomForest
from pyspark.mllib.feature import Normalizer
from pyspark.mllib.regression import LabeledPoint
```

使用**Spark**前的准备,因为*实验楼*有spark相关课程，这里就不做过多介绍
```python
conf = SparkConf().setAppName(&#34;sentiment_analysis&#34;)
sc = SparkContext(conf=conf)
sc.setLogLevel(&#34;WARN&#34;)
sqlContext = SQLContext(sc)
```

进行情感分析的处理步骤如下图所示:

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid291340labid2380timestamp1482875853328.png/wm)

&gt; 实验中我们利用获得的 __tweets.json__ 作为`情感分析`分类器模型的训练集，这里测试集是用__hillary.json__，想要分析川普的，只需将测试集对应换成__donald.json__即可

据图可知，我们首先将每一条的推特数据分词，由于这里分析的是英文的tweet推特内容，因此只需将每个单词分开即可（与中文情感分析的不同），并定义`stop_words 停用词`一些无关情感分析的词

```python
#寻找推文的协调性
#符号化推文的文本
#删除停用词，标点符号，url等
remove_spl_char_regex = re.compile(&#39;[%s]&#39; % re.escape(string.punctuation))  # regex to remove special characters
stopwords = [u&#39;rt&#39;, u&#39;re&#39;, u&#39;i&#39;, u&#39;me&#39;, u&#39;my&#39;, u&#39;myself&#39;, u&#39;we&#39;, u&#39;our&#39;,               u&#39;ours&#39;, u&#39;ourselves&#39;, u&#39;you&#39;, u&#39;your&#39;,
             u&#39;yours&#39;, u&#39;yourself&#39;, u&#39;yourselves&#39;, u&#39;he&#39;, u&#39;him&#39;, u&#39;his&#39;, u&#39;himself&#39;, u&#39;she&#39;, u&#39;her&#39;, u&#39;hers&#39;,
             u&#39;herself&#39;, u&#39;it&#39;, u&#39;its&#39;, u&#39;itself&#39;, u&#39;they&#39;, u&#39;them&#39;, u&#39;their&#39;, u&#39;theirs&#39;, u&#39;themselves&#39;, u&#39;what&#39;,
             u&#39;which&#39;, u&#39;who&#39;, u&#39;whom&#39;, u&#39;this&#39;, u&#39;that&#39;, u&#39;these&#39;, u&#39;those&#39;, u&#39;am&#39;, u&#39;is&#39;, u&#39;are&#39;, u&#39;was&#39;, u&#39;were&#39;,
             u&#39;be&#39;, u&#39;been&#39;, u&#39;being&#39;, u&#39;have&#39;, u&#39;has&#39;, u&#39;had&#39;, u&#39;having&#39;, u&#39;do&#39;, u&#39;does&#39;, u&#39;did&#39;, u&#39;doing&#39;, u&#39;a&#39;,
             u&#39;an&#39;, u&#39;the&#39;, u&#39;and&#39;, u&#39;but&#39;, u&#39;if&#39;, u&#39;or&#39;, u&#39;because&#39;, u&#39;as&#39;, u&#39;until&#39;, u&#39;while&#39;, u&#39;of&#39;, u&#39;at&#39;, u&#39;by&#39;,
             u&#39;for&#39;, u&#39;with&#39;, u&#39;about&#39;, u&#39;against&#39;, u&#39;between&#39;, u&#39;into&#39;, u&#39;through&#39;, u&#39;during&#39;, u&#39;before&#39;, u&#39;after&#39;,
             u&#39;above&#39;, u&#39;below&#39;, u&#39;to&#39;, u&#39;from&#39;, u&#39;up&#39;, u&#39;down&#39;, u&#39;in&#39;, u&#39;out&#39;, u&#39;on&#39;, u&#39;off&#39;, u&#39;over&#39;, u&#39;under&#39;,
             u&#39;again&#39;, u&#39;further&#39;, u&#39;then&#39;, u&#39;once&#39;, u&#39;here&#39;, u&#39;there&#39;, u&#39;when&#39;, u&#39;where&#39;, u&#39;why&#39;, u&#39;how&#39;, u&#39;all&#39;,
             u&#39;any&#39;, u&#39;both&#39;, u&#39;each&#39;, u&#39;few&#39;, u&#39;more&#39;, u&#39;most&#39;, u&#39;other&#39;, u&#39;some&#39;, u&#39;such&#39;, u&#39;no&#39;, u&#39;nor&#39;, u&#39;not&#39;,
             u&#39;only&#39;, u&#39;own&#39;, u&#39;same&#39;, u&#39;so&#39;, u&#39;than&#39;, u&#39;too&#39;, u&#39;very&#39;, u&#39;s&#39;, u&#39;t&#39;, u&#39;can&#39;, u&#39;will&#39;, u&#39;just&#39;, u&#39;don&#39;,
             u&#39;should&#39;, u&#39;now&#39;]
             
# tokenize函数对tweets内容进行分词
def tokenize(text):
    tokens = []
    text = text.encode(&#39;ascii&#39;, &#39;ignore&#39;)  # to decode
    text = re.sub(&#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39;, &#39;&#39;,
                  text)  # to replace url with &#39;&#39;
    text = remove_spl_char_regex.sub(&#34; &#34;, text)  # Remove special characters
    text = text.lower()

    for word in text.split():
        if word not in stopwords \
                and word not in string.punctuation \
                and len(word) &gt; 1 \
                and word != &#39;``&#39;:
            tokens.append(word)
    return tokens
```
**Spark MLlib**中提供的机器学习模型处理的是向量形式的数据，因此我们需将文本转换为向量形式，为了节省时间，这里我们利用Spark提供的`Word2Vec`功能结合其提供的`text8`文件中的一部分单词进行了`word2vec`模型的预训练，并将模型保存至`word2vecM_simple`文件夹中，因此本次实验中将`tweets`转换为向量时直接调用此模型即可，从以下地址中获得此离线模型并解压：
```bash
cd ~
wget http://labfile.oss.aliyuncs.com/courses/722/word2vecM_simple.zip
unzip word2vecM_simple.zip
```

&gt; 由于实验楼的在线环境限制，同学们可线下使用`text8`自行训练词向量转换模型，或线上搜索利用tweets进行分词训练的`word2vec`模型

定义分词文本转换为向量的函数
```python
def doc2vec(document):
    # 100维的向量
    doc_vec = np.zeros(100)
    tot_words = 0

    for word in document:
        try:
        # 查找该词在预训练的word2vec模型中的特征值
            vec = np.array(lookup_bd.value.get(word)) + 1
            # print(vec)
            # 若该特征词在预先训练好的模型中，则添加到向量中
            if vec != None:
                doc_vec += vec
                tot_words += 1
        except:
            continue

    vec = doc_vec / float(tot_words)
    return vec
```

读入预先训练好的文本向量化模型`word2vecM`
```python
lookup = sqlContext.read.parquet(&#34;/home/shiyanlou/word2vecM_simple/data&#34;).alias(&#34;lookup&#34;)
lookup.printSchema()
lookup_bd = sc.broadcast(lookup.rdd.collectAsMap())
```

情感分析相关的函数定义好后，我们便可从__json__文件中读入数据，创建**RDD**对象，利用`spark mllib`的分类器进行情感分析：

```python
# 读入tweets.json作为分类器训练数据集
with open(&#39;tweets.json&#39;, &#39;r&#39;) as f:
    rawTrn_data = json.load(f)
    f.close()

trn_data = []
for obj in rawTrn_data[&#39;results&#39;]:
    token_text = tokenize(obj[&#39;text&#39;]) # 规范化推特文本，进行分词
    tweet_text = doc2vec(token_text) # 将文本转换为向量
    # 使用LabeledPoint 将文本对应的情感属性polariy：该条训练数据的标记label，tweet_text：训练分类器的features特征，结合成可作为spark mllib分类训练的数据类型
    trn_data.append(LabeledPoint(obj[&#39;polarity&#39;], tweet_text)) 

trnData = sc.parallelize(trn_data)
#print(trnData)
print(&#34;------------------------------------------------------&#34;)

# 读入hillary.json作为分类器测试数据集
with open(&#39;hillary.json&#39;, &#39;r&#39;) as f:
    rawTst_data = json.load(f)
    f.close()

tst_data = []
for obj in rawTst_data[&#39;results&#39;]:
    token_text = tokenize(obj[&#39;text&#39;])
    tweet_text = doc2vec(token_text)
    tst_data.append(LabeledPoint(obj[&#39;polarity&#39;], tweet_text))

tst_dataRDD = sc.parallelize(tst_data)

# 训练随机森林分类器模型
model = RandomForest.trainClassifier(trnData, numClasses=3, categoricalFeaturesInfo={},
                                     numTrees=3, featureSubsetStrategy=&#34;auto&#34;,
                                     impurity=&#39;gini&#39;, maxDepth=4, maxBins=32)

# 利用训练好的模型进行模型性能测试
predictions = model.predict(tst_dataRDD.map(lambda x: x.features))
labelsAndPredictions = tst_dataRDD.map(lambda lp: lp.label).zip(predictions)
# 计算分类错误率
testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(tst_dataRDD.count())
print(&#39;Test Error = &#39; + str(testErr))
print(&#39;Learned classification tree model:&#39;)
# 输出训练好的随机森林的分类决策模型
print(model.toDebugString())

```

在编写完代码后，在`shiyanlou_cs722`目录下，通过`spark-submit`命令提交程序运行：
```bash
$ /usr/bin/spark-2.0.2/bin/spark-submit --master=local sparkSA.py 
 --master=local sparkSA.py
```
&gt; 在下一节的可视化的函数编写后，也是通过此命令提交运行程序。

训练得到的`随机森林`模型：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid291340labid2380timestamp1483652170622.png/wm)

## 四、分析结果可视化

基于上一小节的`RandomForest随机森林`的训练结果，我们利用该分类器对**推特**上关于__hillary__的情感分析结果，结合`basemap`将其展示到美国的**48**个州（除去阿拉斯加及夏威夷）上，观察这**48**个州对于 __hillary__ 的看法。

我们将在函数`res_visulization`中绘制可视化结果，首先我们需要定义该函数。

导入作图相关模块。

```python
from mpl_toolkits.basemap import Basemap
from mpl_toolkits.basemap import cm
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.colors import rgb2hex
from matplotlib.patches import Polygon
```

我们本次通过美国的`shapefile`结合basemap进行绘图，因此需要以下三个文件（这个三个文件需放置在目录`shiyanlou_cs722`下）：

```bash
wget http://labfile.oss.aliyuncs.com/courses/722/st99_d00.shp
wget http://labfile.oss.aliyuncs.com/courses/722/st99_d00.dbf
wget http://labfile.oss.aliyuncs.com/courses/722/st99_d00.shx
```

函数`res_visulization`可视化情感分析结果

```python
# pred_result：利用spark mllib 情感分析结果
def res_visulization(pred_result):
    # popdensity_ori 用于保存基于我们事先给定的推特情感极性，不同州的情感属性
    popdensity_ori = {&#39;New Jersey&#39;:  0., &#39;Rhode Island&#39;: 0., &#39;Massachusetts&#39;: 0., &#39;Connecticut&#39;: 0.,
                      &#39;Maryland&#39;: 0.,&#39;New York&#39;: 0., &#39;Delaware&#39;: 0., &#39;Florida&#39;: 0., &#39;Ohio&#39;: 0., &#39;Pennsylvania&#39;: 0.,
                      &#39;Illinois&#39;: 0., &#39;California&#39;: 0., &#39;Hawaii&#39;: 0., &#39;Virginia&#39;: 0., &#39;Michigan&#39;:    0.,
                      &#39;Indiana&#39;: 0., &#39;North Carolina&#39;: 0., &#39;Georgia&#39;: 0., &#39;Tennessee&#39;: 0., &#39;New Hampshire&#39;: 0.,
                      &#39;South Carolina&#39;: 0., &#39;Louisiana&#39;: 0., &#39;Kentucky&#39;: 0., &#39;Wisconsin&#39;: 0., &#39;Washington&#39;: 0.,
                      &#39;Alabama&#39;:  0., &#39;Missouri&#39;: 0., &#39;Texas&#39;: 0., &#39;West Virginia&#39;: 0., &#39;Vermont&#39;: 0.,
                      &#39;Minnesota&#39;:  0., &#39;Mississippi&#39;: 0., &#39;Iowa&#39;: 0., &#39;Arkansas&#39;: 0., &#39;Oklahoma&#39;: 0.,
                      &#39;Arizona&#39;: 0., &#39;Colorado&#39;: 0., &#39;Maine&#39;: 0., &#39;Oregon&#39;: 0., &#39;Kansas&#39;: 0., &#39;Utah&#39;: 0.,
                      &#39;Nebraska&#39;: 0., &#39;Nevada&#39;: 0., &#39;Idaho&#39;: 0., &#39;New Mexico&#39;:  0., &#39;South Dakota&#39;:	0.,
                      &#39;North Dakota&#39;: 0., &#39;Montana&#39;: 0., &#39;Wyoming&#39;: 0., &#39;Alaska&#39;: 0.}
    # popdensity 用于保存基于随机森林分析的推特情感极性，不同州的情感属性
    popdensity  = {&#39;New Jersey&#39;:  0., &#39;Rhode Island&#39;: 0., &#39;Massachusetts&#39;: 0., &#39;Connecticut&#39;: 0.,
                      &#39;Maryland&#39;: 0.,&#39;New York&#39;: 0., &#39;Delaware&#39;: 0., &#39;Florida&#39;: 0., &#39;Ohio&#39;: 0., &#39;Pennsylvania&#39;: 0.,
                      &#39;Illinois&#39;: 0., &#39;California&#39;: 0., &#39;Hawaii&#39;: 0., &#39;Virginia&#39;: 0., &#39;Michigan&#39;:    0.,
                      &#39;Indiana&#39;: 0., &#39;North Carolina&#39;: 0., &#39;Georgia&#39;: 0., &#39;Tennessee&#39;: 0., &#39;New Hampshire&#39;: 0.,
                      &#39;South Carolina&#39;: 0., &#39;Louisiana&#39;: 0., &#39;Kentucky&#39;: 0., &#39;Wisconsin&#39;: 0., &#39;Washington&#39;: 0.,
                      &#39;Alabama&#39;:  0., &#39;Missouri&#39;: 0., &#39;Texas&#39;: 0., &#39;West Virginia&#39;: 0., &#39;Vermont&#39;: 0.,
                      &#39;Minnesota&#39;:  0., &#39;Mississippi&#39;: 0., &#39;Iowa&#39;: 0., &#39;Arkansas&#39;: 0., &#39;Oklahoma&#39;: 0.,
                      &#39;Arizona&#39;: 0., &#39;Colorado&#39;: 0., &#39;Maine&#39;: 0., &#39;Oregon&#39;: 0., &#39;Kansas&#39;: 0., &#39;Utah&#39;: 0.,
                      &#39;Nebraska&#39;: 0., &#39;Nevada&#39;: 0., &#39;Idaho&#39;: 0., &#39;New Mexico&#39;:  0., &#39;South Dakota&#39;:	0.,
                      &#39;North Dakota&#39;: 0., &#39;Montana&#39;: 0., &#39;Wyoming&#39;: 0., &#39;Alaska&#39;: 0.}
    idx = 0
    for obj in rawTst_data[&#39;results&#39;]:
        user_location = obj[&#39;user_location&#39;]
        popdensity_ori[user_location] += (obj[&#39;polarity&#39;] - 1)
        popdensity[user_location] += (pred_result[idx] - 1)
        idx += 1
    # 在终端上输出不同的州的情感属性
    # 由于我们设置的 polarity 积极：0 正常：1 消极：2
    # 因此对应的不同的州对于新总统的情感值越大则越消极，越小则越积极
    print(&#39;popdensity_ori&#39;)
    print(popdensity_ori)
    print(&#34;---------------------------------------------------------&#34;)
    print(&#39;popdensity&#39;)
    print(popdensity)
    print(&#34;---------------------------------------------------------&#34;)

    # Lambert Conformal map of lower 48 states.
    fig = plt.figure(figsize=(14, 6))
    # 使用ax1, ax3 分别展示测试数据的原本情感属性值，及基于模型情感分析的结果
    ax1 = fig.add_axes([0.05, 0.20, 0.40, 0.75])
    ax3 = fig.add_axes([0.50, 0.20, 0.40, 0.75])
    # 初始化Basemap对象，获得在美国范围的地图m1
    m1 = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,
                projection=&#39;lcc&#39;,lat_1=33,lat_2=45,lon_0=-95, ax = ax1)
    # draw state boundaries.
    # data from U.S Census Bureau
    # http://www.census.gov/geo/www/cob/st2000.html
    shp_info = m1.readshapefile(&#39;st99_d00&#39;,&#39;states&#39;,drawbounds=True)
    print(shp_info)
```

在可视化结果中加上各个州的缩写，这里的我们只给出了部分州的缩写，有些美国较小的东部城市没有具体列出。
```python
# 各个州的缩写
    cities = [&#39;WA&#39;, &#39;OR&#39;, &#39;CA&#39;, &#39;NV&#39;, &#39;MT&#39;,&#39;ID&#39;,&#39;WY&#39;,&#39;UT&#39;,
          &#39;CO&#39;, &#39;AZ&#39;, &#39;NM&#39;, &#39;ND&#39;, &#39;SD&#39;, &#39;NE&#39;, &#39;KS&#39;,
          &#39;OK&#39;, &#39;TX&#39;, &#39;MN&#39;, &#39;IA&#39;, &#39;MO&#39;, &#39;AR&#39;, &#39;LA&#39;,
          &#39;WI&#39;, &#39;IL&#39;, &#39;MI&#39;, &#39;IN&#39;, &#39;OH&#39;, &#39;KY&#39;, &#39;TN&#39;,
          &#39;MS&#39;,	&#39;AL&#39;, &#39;PA&#39;, &#39;WV&#39;, &#39;GA&#39;, &#39;ME&#39;, &#39;VT&#39;,
          &#39;NY&#39;, &#39;VA&#39;, &#39;NC&#39;, &#39;SC&#39;, &#39;FL&#39;, &#39;AL&#39;]
    lat = [47.40, 44.57, 36.12, 38.31, 46.92, 44.24,
           42.75, 40.15, 39.06, 33.73, 34.84, 47.53,
           44.30, 41.125, 38.526, 35.565, 31.05,
           45.69, 42.01, 38.46, 34.97, 31.17, 44.27,
           40.35, 43.33, 39.85, 40.39, 37.67, 35.75,
           32.74, 61.37, 40.59, 38.49, 33.04, 44.69,
           44.045, 42.165, 37.77, 35.63, 33.86, 27.77,
           32.81]
    lon = [-121.49, -122.07, -119.68, -117.05, -110.45,
           -114.48, -107.30, -111.86, -105.31, -111.43,
           -106.25, -99.93, -99.44, -98.27, -96.726,
           -96.93, -97.56, -93.90, -93.21, -92.29,
           -92.37, -91.87, -89.62, -88.99, -84.54,
           -86.26, -82.76, -84.67, -86.70, -89.68,
           -152.40, -77.21, -80.95, -83.64, -69.38,
           -72.71, -74.95, -78.17, -79.81, -80.945,
           -81.67, -86.79]
```
&gt; 有关美国的各个州的经纬度可在[链接](https://inkplant.com/code/state-latitudes-longitudes)中查阅。

```python
 # choose a color for each state based on population density.
    colors={}
    colors2 = {}
    statenames=[]
    # 使用cm.GMT_polar 渐变色用以展示不同情感
    cmap = cm.GMT_polar
    
    # 分别获得基于测试数据原本，及基于模型分析的48个大洲情感属性的最大及最小值
    # 测试数据本有标签属性值
    inverse = [(value, key) for key, value in popdensity_ori.items()]
    vmin = min(inverse)[0]
    vmax = max(inverse)[0]  # set range.
    # 测试数据情感分析结果
    inverse = [(value, key) for key, value in popdensity.items()]
    vmin_pred = min(inverse)[0]
    vmax_pred = max(inverse)[0]
    
    print(&#39;vmax:&#39;)
    print(vmax)
    print(m1.states_info[0].keys())
    for shapedict in m1.states_info:
        statename = shapedict[&#39;NAME&#39;]
        # skip DC and Puerto Rico.
        if statename not in [&#39;District of Columbia&#39;,&#39;Puerto Rico&#39;]:
            pop = popdensity_ori[statename]
            pop_pred = popdensity[statename]
            # calling colormap with value between 0 and 1 returns
            # rgba value.  Invert color range (hot colors are high
            # population), take sqrt root to spread out colors more.
            # 使用的cm.GMT_polar渐变色，不同州的颜色随情感越积极越红，越消极越蓝，无感状态时为无色
            # 蓝色为小于cmap的0.5，无色为cmap的0.5，红色为大于cmap的0.5
            if pop == 0:
               colors[statename] = cmap(0.5)[:3]
            elif pop &lt; 0:
               colors[statename] = cmap(1.0 - np.sqrt((pop - vmin)/(0-vmin)))[:3]
            else:
               colors[statename] = cmap(0.5 - np.sqrt((pop - 0)/(vmax-0)))[:3]
               
            # 同上，colors2保存基于mllib情感分析结果的颜色
            if pop_pred == 0:
                colors2[statename] = cmap(0.5)[:3]
            elif pop_pred &lt; 0:
                colors2[statename] = cmap(1.0 - np.sqrt((pop_pred - vmin_pred) / (0 - vmin_pred)))[:3]
            else:
                colors2[statename] = cmap(0.5 - np.sqrt((pop_pred - 0) / (vmax_pred - 0)))[:3]
        statenames.append(statename)
    # cycle through state names, color each one.
    #ax = plt.gca() # get current axes instance
    for nshape,seg in enumerate(m1.states):
        # skip DC and Puerto Rico.
        if statenames[nshape] not in [&#39;District of Columbia&#39;,&#39;Puerto Rico&#39;]:
            color = rgb2hex(colors[statenames[nshape]])
            #print(statenames[nshape])
            poly = Polygon(seg,facecolor=color,edgecolor=color)
            ax1.add_patch(poly)
            
    # 绘制经纬线
    m1.drawparallels(np.arange(25,65,20),labels=[1,0,0,0])
    m1.drawmeridians(np.arange(-120,-40,20),labels=[0,0,0,1])
    # 在地图上添加不同州的缩写
    x, y = m1(lon, lat)
    for city, xc, yc in zip(cities, x, y):
        ax1.text(xc - 60000, yc - 50000, city)
    ax1.set_title(&#39;Twitter-based sentiment analysis about Hillary &#39;)

    m2 = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,
                projection=&#39;lcc&#39;,lat_1=33,lat_2=45,lon_0=-95, ax = ax3)
    m2.readshapefile(&#39;st99_d00&#39;, &#39;states&#39;, drawbounds=True)
    for nshape, seg in enumerate(m2.states):
        # skip DC and Puerto Rico.
        if statenames[nshape] not in [&#39;District of Columbia&#39;, &#39;Puerto Rico&#39;]:
            color = rgb2hex(colors2[statenames[nshape]])
            # print(statenames[nshape])
            poly = Polygon(seg, facecolor=color, edgecolor=color)
            ax3.add_patch(poly)

    ax3.set_title(&#39;Random Forest prediction&#39;)
    m2.drawparallels(np.arange(25,65,20),labels=[1,0,0,0])
    m2.drawmeridians(np.arange(-120,-40,20),labels=[0,0,0,1])
    x, y = m2(lon, lat)
    for city, xc, yc in zip(cities, x, y):
        ax3.text(xc - 60000, yc - 50000, city)
        
    # 添加渐变色条colorbar,方便对应颜色查看情感属性
    ax2 = fig.add_axes([0.05, 0.10, 0.9, 0.05])
    norm = mpl.colors.Normalize(vmin=-1, vmax=1)
    cb1 = mpl.colorbar.ColorbarBase(ax2, cmap=cmap,
                                    norm=norm,
                                    orientation=&#39;horizontal&#39;,
                                    ticks=[-1, 0, 1])
    cb1.ax.set_xticklabels([&#39;negative&#39;, &#39;natural&#39;, &#39;positive&#39;])
    cb1.set_label(&#39;Sentiment&#39;)

    plt.show()
```

函数定义完成后，在 sparkSA.py 的最后一行调用该函数以执行它。在代码最后加上此行：

```python
res_visulization(predictions.collect())
```

保存该文件后退出编辑，在 `Shiyanlou_cs722` 目录下， 通过`spark-submit`命令提交所编写好的Python脚本`SparkSA.py`执行程序：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2380timestamp1483684068753.png/wm)

程序运行结束，我们可看到每个州的原始数据及模型预测的情感属性值：
![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid291340labid2380timestamp1483652113876.png/wm)

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2380timestamp1483683858820.png/wm)

## 五、总结

本次课程我们介绍了`twitter api`一个Twitter官方提供可用于获得推特数据的接口，并且基于此接口获得了**推特**上的 *流式* 数据,在实际应用中可利用`spark streaming`获得这些流式数据进行实时处理并显现当下网友的情感状态。而本次实验我们是利用spark mllib的随机森林模型进行twitter的情感分析，并且利用`basemap`进行可视化，通过可视化结果，我们可以直观的感受到美国今年曾经的候选总统在美国各个州的受欢迎程度。基于此课程，同学们也可以利用`twitter`或者`微博`提供的开发者接口获取网络上的数据，对不同方面（房价、物价、交通之类）的情感进行分析。

情感分析最终效果图(有时可视化效果未显现程序就已经结束，此时只需重复提交即可)：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid291340labid2380timestamp1482877939765.png/wm)

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid291340labid2380timestamp1482878032809.png/wm)

本次课程的所有源码可通过以下命令获得：
```bash
wget http://labfile.oss.aliyuncs.com/courses/722/sparkSA.py
```
## 六、课后习题

在使用spark mllib进行情感分析的例子中，很多都是使用`朴素贝叶斯`模型，这里我们使用的是`随机森林模型`，在此请同学们使用`spark mllib`的`朴素贝叶斯`模型进行代码改写，并输出情感分析结果。

&gt; 可参考[spark mllib Naive Bayes example](http://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes)

## 七、参考阅读

- [实时分析社交媒体数据](https://www.ibm.com/developerworks/cn/analytics/blog/analyze-social-media-data-real-time/index.html)
- [tweepy教程](https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/)
- [basemap地图可视化](http://introtopython.org/visualization_earthquakes.html#Adding-detail)
