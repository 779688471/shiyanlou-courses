#Python标准库（中）

作者：Vamei 出处：http://www.cnblogs.com/vamei 欢迎转载，也请保留这段声明。谢谢！


##一、子进程 (subprocess包)

这里的内容以Linux进程基础和Linux文本流为基础。subprocess包主要功能是**执行外部的命令和程序**。比如说，我需要使用wget下载文件。我在Python中调用wget程序。从这个意义上来说，subprocess的功能与shell类似。

###1、subprocess以及常用的封装函数

当我们运行python的时候，我们都是在创建并运行一个进程。一个进程可以fork一个子进程，并让这个子进程exec另外一个程序。在Python中，我们通过标准库中的subprocess包来fork一个子进程，并运行一个外部的程序。

subprocess包中定义有数个创建子进程的函数，这些函数分别以不同的方式创建子进程，所以我们可以根据需要来从中选取一个使用。另外subprocess还提供了一些管理标准流(standard stream)和管道(pipe)的工具，从而在进程间使用**文本通信**。

使用subprocess包中的函数创建子进程的时候，要注意:

- 1 在创建子进程之后，父进程是否暂停，并等待子进程运行。

- 2 函数返回什么

- 3 当returncode不为0时，父进程如何处理。

(1)、subprocess.call()

父进程等待子进程完成

返回**退出信息**(returncode，相当于exit code)

(2)、subprocess.check\_call()

父进程等待子进程完成

返回0

检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try...except...来检查(见Python错误处理)。

(3)、subprocess.check\_output()

父进程等待子进程完成

返回子进程向标准输出的**输出结果**

检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性和output属性，output属性为标准输出的输出结果，可用try...except...来检查。

这三个函数的使用方法相类似，我们以subprocess.call()来说明:

```
import subprocess
rc = subprocess.call(["ls","-l"])
```

我们将程序名(ls)和所带的参数(-l)一起放在一个表中传递给subprocess.call()

可以通过一个shell来解释一整个字符串:

```
import subprocess
out = subprocess.call("ls -l", shell=True)
out = subprocess.call("cd ..", shell=True)
```

我们使用了**shell=True**这个参数。这个时候，我们使用**一整个字符串**，而**不是一个表**来运行子进程。Python将**先运行一个shell**，再用这个shell来解释这整个字符串。

shell命令中有一些是shell的内建命令，这些命令必须通过shell运行，$cd。shell=True允许我们运行这样一些命令。

###2、Popen()

实际上，我们上面的三个函数都是基于**Popen()**的封装(wrapper)。这些封装的目的在于让我们容易使用子进程。当我们想要更个性化我们的需求的时候，就要转向Popen类，该类生成的对象用来代表子进程。

与上面的封装不同，Popen对象创建后，主程序不会自动等待子进程完成。我们必须调用对象的**wait()**方法，父进程才会等待 (也就是阻塞block)：

```
import subprocess
child = subprocess.Popen(["ping","-c","5","www.google.com"])
print("parent process")
```

从运行结果中看到，父进程在开启子进程之后并没有等待child的完成，而是直接运行print。

对比等待的情况:

```
import subprocess
child = subprocess.Popen(["ping","-c","5","www.google.com"])
child.wait()
print("parent process")
```

此外，你还可以在父进程中对子进程进行其它操作，比如我们上面例子中的child对象:

- child.poll()           # 检查子进程状态

- child.kill()           # 终止子进程

- child.send_signal()    # 向子进程发送信号

- child.terminate()      # 终止子进程

子进程的PID存储在child.pid。

###3、子进程的文本流控制

(沿用child子进程) 子进程的标准输入，标准输出和标准错误也可以通过如下属性表示:

```
child.stdin
child.stdout
child.stderr
```

我们可以在Popen()建立子进程的时候改变标准输入、标准输出和标准错误，并可以利用**subprocess.PIPE**将多个子进程的输入和输出连接在一起，构成**管道**(pipe):

```
import subprocess
child1 = subprocess.Popen(["ls","-l"], stdout=subprocess.PIPE)
child2 = subprocess.Popen(["wc"], stdin=child1.stdout,stdout=subprocess.PIPE)
out = child2.communicate()
print(out)
```

subprocess.PIPE实际上为文本流提供一个缓存区。child1的stdout将文本输出到缓存区，随后child2的stdin从该PIPE中将文本读取走。child2的输出文本也被存放在PIPE中，直到communicate()方法从PIPE中读取出PIPE中的文本。

要注意的是，**communicate()**是Popen对象的一个方法，该方法会阻塞父进程，直到子进程完成。

我们还可以利用communicate()方法来使用PIPE给子进程输入:

```
import subprocess
child = subprocess.Popen(["cat"], stdin=subprocess.PIPE)
child.communicate("vamei")
```

我们启动子进程之后，cat会等待输入，直到我们用communicate()输入"vamei"。

**通过使用subprocess包，我们可以运行外部程序。这极大的拓展了Python的功能。如果你已经了解了操作系统的某些应用，你可以从Python中直接调用该应用(而不是完全依赖Python)，并将应用的结果输出给Python，并让Python继续处理。shell的功能(比如利用文本流连接各个应用)，就可以在Python中实现。**


##二、信号 (signal包)

signal包负责在Python程序内部处理信号，典型的操作包括预设信号处理函数，暂停并等待信号，以及定时发出SIGALRM等。要注意，signal包主要是针对UNIX平台(比如Linux, MAC OS)，而Windows内核中由于对信号机制的支持不充分，所以在Windows上的Python不能发挥信号系统的功能。

###1、定义信号名

signal包定义了各个信号名及其对应的整数，比如：

```
import signal
print signal.SIGALRM
print signal.SIGCONT
```

Python所用的信号名和Linux一致。你可以通过以下命令查询：

```
$man 7 signal
```

###2、预设信号处理函数

signal包的核心是使用**signal.signal()**函数来**预设(register)信号处理函数**，如下所示：

```
singnal.signal(signalnum, handler)
```

signalnum为某个信号，handler为该信号的处理函数。我们在信号基础里提到，进程可以无视信号，可以采取默认操作，还可以自定义操作。当handler为**signal.SIG\_IGN**时，信号被无视(ignore)。当handler为**singal.SIG\_DFL**，进程采取默认操作(default)。当handler为一个函数名时，进程采取函数中定义的操作。

```
import signal
# Define signal handler function
def myHandler(signum, frame):
    print('I received: ', signum)

# register signal.SIGTSTP's handler 
signal.signal(signal.SIGTSTP, myHandler)
signal.pause()
print('End of Signal Demo')
```

在主程序中，我们首先使用**signal.signal()**函数来**预设**信号处理函数。然后我们执行**signal.pause()**来**让该进程暂停以等待信号**，以等待信号。当信号SIGUSR1被传递给该进程时，进程从暂停中恢复，并根据预设，执行SIGTSTP的信号处理函数myHandler()。myHandler的两个参数一个用来识别信号(signum)，另一个用来获得信号发生时，进程栈的状况(stack frame)。这两个参数都是由signal.singnal()函数来传递的。

上面的程序可以保存在一个文件中(比如test.py)。我们使用如下方法运行:

```
$python test.py
```

以便让进程运行。当程序运行到signal.pause()的时候，进程暂停并等待信号。此时，通过按下CTRL+Z向该进程发送**SIGTSTP信号**。我们可以看到，进程执行了myHandle()函数, 随后返回主程序，继续执行。(当然，也可以用\$ps查询process ID, 再使用$kill来发出信号。)

> (进程并不一定要使用signal.pause()暂停以等待信号，它也可以在进行工作中接受信号，比如将上面的signal.pause()改为一个需要长时间工作的循环。)

我们可以根据自己的需要更改myHandler()中的操作，以针对不同的信号实现个性化的处理。

###3、定时发出SIGALRM信号

一个有用的函数是**signal.alarm()**，它被用于在**一定时间之后**，向进程自身发送**SIGALRM**信号:

```
import signal
# Define signal handler function
def myHandler(signum, frame):
    print("Now, it's the time")
    exit()

# register signal.SIGALRM's handler 
signal.signal(signal.SIGALRM, myHandler)
signal.alarm(5)
while True:
    print('not yet')
```

我们这里用了一个无限循环以便让进程持续运行。在signal.alarm()执行5秒之后，进程将向自己发出SIGALRM信号，随后，信号处理函数myHandler开始执行。

###4、发送信号

signal包的核心是设置信号处理函数。除了signal.alarm()向自身发送信号之外，并没有其他发送信号的功能。但在os包中，有类似于linux的kill命令的函数，分别为：

```
os.kill(pid, sid)

os.killpg(pgid, sid)
```

分别向进程和进程组发送信号。sid为信号所对应的整数或者singal.SIG*。

实际上signal, pause，kill和alarm都是Linux应用编程中常见的C库函数，在这里，我们只不过是用Python语言来实现了一下。实际上，Python 的解释器是使用C语言来编写的，所以有此相似性也并不意外。此外，在Python 3.4中，signal包被增强，信号阻塞等功能被加入到该包中。我们暂时不深入到该包中。


##三、多线程与同步 (threading包)

Python主要通过标准库中的**threading**包来实现多线程。在当今**网络**时代，每个服务器都会接收到大量的请求。服务器可以利用多线程的方式来处理这些请求，以提高对网络端口的读写效率。Python是一种网络服务器的后台工作语言，所以多线程也就很自然被Python语言支持。

> (关于多线程的原理和C实现方法，请参考[Linux多线程与同步](http://www.cnblogs.com/vamei/archive/2012/10/09/2715393.html)，要了解race condition, mutex和condition variable的概念)

###1、多线程售票以及同步

我们使用Python来实现Linux多线程与同步文中的售票程序。我们使用mutex (也就是Python中的**Lock**类对象) 来实现线程的同步:

```
# A program to simulate selling tickets in multi-thread way
# Written by Vamei

import threading
import time
import os

# This function could be any function to do other chores.
def doChore():
    time.sleep(0.5)

# Function for each thread
def booth(tid):
    global i
    global lock
    while True:
        lock.acquire()                # Lock; or wait if other thread is holding the lock
        if i != 0:
            i = i - 1                 # Sell tickets
            print(tid,':now left:',i) # Tickets left
            doChore()                 # Other critical operations
        else:
            print("Thread_id",tid," No more tickets")
            os._exit(0)              # Exit the whole process immediately
        lock.release()               # Unblock
        doChore()                    # Non-critical operations

# Start of the main function
i    = 100                           # Available ticket number 
lock = threading.Lock()              # Lock (i.e., mutex)

# Start 10 threads
for k in range(10):
    new_thread = threading.Thread(target=booth,args=(k,))   # Set up thread; target: the callable (function) to be run, args: the argument for the callable 
    new_thread.start()                                      # run the thread
```

我们使用了两个全局变量，一个是i，用以储存剩余票数；一个是lock对象，用于同步线程对i的修改。此外，在最后的for循环中，我们总共设置了10个线程。每个线程都执行booth()函数。线程在调用start()方法的时候正式启动 (实际上，计算机中最多会有11个线程，因为主程序本身也会占用一个线程)。Python使用**threading.Thread**对象来代表线程，用**threading.Lock**对象来代表一个互斥锁 (mutex)。

有两点需要注意:

- 我们在函数中使用**global**来声明变量为全局变量，从而让多线程共享i和lock (在C语言中，我们通过将变量放在所有函数外面来让它成为全局变量)。如果不这么声明，由于i和lock是**不可变数据对象**，它们将被当作一个局部变量。如果是**可变数据对象**的话，则不需要global声明。我们甚至可以将**可变数据对象**作为参数来传递给线程函数。这些线程将共享这些可变数据对象。

- 我们在booth中使用了两个**doChore()**函数。可以在未来改进程序，以便让线程除了进行i=i-1之外，做更多的操作，比如打印剩余票数，找钱，或者喝口水之类的。第一个doChore()依然在Lock内部，所以可以安全地**使用共享资源** (critical operations, 比如打印剩余票数)。第二个doChore()时，Lock已经被释放，所以不能再去使用共享资源。这时候可以做一些**不使用共享资源**的操作 (non-critical operation, 比如找钱、喝水)。我故意让doChore()等待了0.5秒，以代表这些额外的操作可能花费的时间。你可以定义的函数来代替doChore()。

###2、OOP创建线程

上面的Python程序非常类似于一个面向过程的C程序。我们下面介绍如何通过**面向对象** (OOP， object-oriented programming) 的方法实现多线程，其核心是继承**threading.Thread**类。我们上面的for循环中已经利用了threading.Thread()的方法来创建一个Thread对象，并将函数booth()以及其参数传递给改对象，并调用start()方法来运行线程。OOP的话，通过修改Thread类的**run()**方法来定义线程所要执行的命令。

```
# A program to simulate selling tickets in multi-thread way
# Written by Vamei

import threading
import time
import os

# This function could be any function to do other chores.
def doChore():
    time.sleep(0.5)

# Function for each thread
class BoothThread(threading.Thread):
    def __init__(self, tid, monitor):
        self.tid          = tid
        self.monitor = monitor
        threading.Thread.__init__(self)
    def run(self):
        while True:
            monitor['lock'].acquire()                          # Lock; or wait if other thread is holding the lock
            if monitor['tick'] != 0:
                monitor['tick'] = monitor['tick'] - 1          # Sell tickets
                print(self.tid,':now left:',monitor['tick'])   # Tickets left
                doChore()                                      # Other critical operations
            else:
                print("Thread_id",self.tid," No more tickets")
                os._exit(0)                                    # Exit the whole process immediately
            monitor['lock'].release()                          # Unblock
            doChore()                                          # Non-critical operations

# Start of the main function
monitor = {'tick':100, 'lock':threading.Lock()}

# Start 10 threads
for k in range(10):
    new_thread = BoothThread(k, monitor)
    new_thread.start()
```

我们自己定义了一个类**BoothThread**, 这个类**继承自thread.Threading类**。然后我们把上面的booth()所进行的操作统统放入到BoothThread类的**run()**方法中。注意，我们没有使用全局变量声明global，而是使用了一个**词典** monitor存放全局变量，然后把词典作为参数传递给线程函数。由于词典是**可变数据对象**，所以当它被传递给函数的时候，函数所使用的依然是同一个对象，相当于被多个线程所共享。这也是多线程乃至于多进程编程的一个技巧 (应尽量避免上面的global声明的用法，因为它并不适用于windows平台)。

上面OOP编程方法与面向过程的编程方法相比，并没有带来太大实质性的差别。

###3、其他

**threading.Thread**对象： 我们已经介绍了该对象的start()和run(), 此外：

- **join()**方法，调用该方法的线程将等待直到该Thread对象完成，再恢复运行。这与进程间调用wait()函数相类似。
 

下面的对象用于处理**多线程同步**。对象一旦被建立，可以被多个线程共享，并根据情况阻塞某些进程。

**threading.Lock**对象: mutex, 有acquire()和release()方法。

**threading.Condition**对象: condition variable，建立该对象时，会包含一个Lock对象 (因为condition variable总是和mutex一起使用)。可以对Condition对象调用acquire()和release()方法，以控制潜在的Lock对象。此外:

- **wait()**方法，相当于cond_wait()

- **notify_all()**，相当与cond_broadcast()

- **nofify()**，与notify_all()功能类似，但只唤醒一个等待的线程，而不是全部

**threading.Semaphore**对象: semaphore，也就是计数锁(semaphore传统意义上是一种进程间同步工具)。创建对象的时候，可以传递一个整数作为**计数上限** (sema = threading.Semaphore(5))。它与Lock类似，也有Lock的两个方法。

**threading.Event**对象: 与threading.Condition相类似，相当于没有潜在的Lock保护的condition variable。对象有True和False两个状态。可以多个线程使用wait()等待，直到某个线程调用该对象的**set()**方法，将对象设置为True。线程可以调用对象的**clear()**方法来重置对象为False状态。


##四、进程信息 (部分os包)

Python的os包中有查询和修改**进程信息**的函数。学习Python的这些工具也有助于理解Linux体系。

###1、进程信息

os包中相关函数如下：

(1)、uname() 返回操作系统相关信息。类似于Linux上的uname命令。

(2)、umask() 设置该进程创建文件时的权限mask。类似于Linux上的umask命令

(3)、get*() 查询 (*由以下代替)

    uid, euid, resuid, gid, egid, resgid ：权限相关，其中resuid主要用来返回saved UID

    pid, pgid, ppid, sid                 ：进程相关

(4)、put\*() 设置 (\*由以下代替)

    euid, egid： 用于更改euid，egid。

    uid, gid  ： 改变进程的uid, gid。只有super user才有权改变进程uid和gid (意味着要以$sudo python的方式运行Python)。

    pgid, sid ： 改变进程所在的进程组(process group)和会话(session)。

(5)、getenviron()：获得进程的环境变量

(6)、setenviron()：更改进程的环境变量

例1，进程的real UID和real GID：

```
import os
print(os.getuid())
print(os.getgid())
```

将上面的程序保存为py\_id.py文件，分别用\$python py\_id.py和\$sudo python py\_id.py看一下运行结果。

###2、saved UID和saved GID

我们希望saved UID和saved GID如我们在[Linux用户与“最小权限”原则](http://www.cnblogs.com/vamei/archive/2012/10/07/2713593.html)中描述的那样工作，但这很难。原因在于，当我们写一个Python脚本后，我们**实际运行的是python这个解释器**，而不是Python脚本文件。对比C，C语言直接运行由C语言编译成的执行文件。我们必须更改python解释器本身的权限来运用saved UID机制，然而这么做又是**异常危险**的。

比如说，我们的python执行文件为/usr/bin/python (你可以通过$which python获知)

我们先看一下：

```
$ls -l /usr/bin/python
```

的结果：

```
-rwxr-xr-x root root
```

我们修改权限以设置set UID和set GID位 (参考[Linux用户与“最小权限”原则](http://www.cnblogs.com/vamei/archive/2012/10/07/2713593.html))：

```
$sudo chmod 6755 /usr/bin/python

/usr/bin/python的权限成为:

-rwsr-sr-x root root
```

随后，我们运行文件下面test.py文件，这个文件可以是由普通用户所有:

```
import os
print(os.getresuid())
```

我们得到结果：(1000, 0, 0)

上面分别是UID，EUID，saved UID。我们只用执行一个由普通用户拥有的python脚本，就可以得到super user的权限！所以，这样做是极度危险的，我们相当于交出了系统的保护系统。想像一下Python强大的功能，别人现在可以用这些强大的功能作为攻击你的武器了！使用下面命令来恢复到从前:

```
$sudo chmod 0755 /usr/bin/python
```

关于脚本文件的saved UID/GID，更加详细的讨论见：http://www.faqs.org/faqs/unix-faq/faq/part4/section-7.html


##五、多进程初步 (multiprocessing包)

我们已经见过了使用subprocess包来创建子进程，但这个包有两个很大的局限性：

(1) 我们总是让subprocess运行外部的程序，而不是运行一个Python脚本内部编写的函数。

(2) 进程间只通过管道进行文本交流。以上限制了我们将subprocess包应用到更广泛的多进程任务。(这样的比较实际是不公平的，因为subprocessing本身就是设计成为一个shell，而不是一个多进程管理包)。

###1、threading和multiprocessing

**multiprocessing**包是Python中的多进程管理包。与threading.Thread类似，它可以利用**multiprocessing.Process**对象来创建一个进程。该进程可以运行在Python程序内部编写的函数。该Process对象与Thread对象的用法相同，也有start(), run(), join()的方法。此外multiprocessing包中也有**Lock/Event/Semaphore/Condition**类 (这些对象可以像多线程那样，通过参数传递给各个进程)，用以**同步**进程，其用法与threading包中的同名类一致。所以，multiprocessing的很大一部份与threading使用同一套API，只不过换到了多进程的情境。

但在使用这些共享API的时候，我们要注意以下几点:

- 在UNIX平台上，当某个进程终结之后，该进程需要被其父进程调用wait，否则进程成为僵尸进程(Zombie)。所以，有必要对每个Process对象调用**join()**方法 (实际上等同于wait)。对于多线程来说，由于只有一个进程，所以不存在此必要性。

- multiprocessing提供了threading包中没有的IPC(比如Pipe和Queue)，效率上更高。应优先考虑Pipe和Queue，**避免使用** Lock/Event/Semaphore/Condition等同步方式 (因为它们占据的不是用户进程的资源)。

- 多进程应该**避免共享资源**。在多线程中，我们可以比较容易地共享资源，比如使用全局变量或者传递参数。在多进程情况下，由于每个进程有自己独立的内存空间，以上方法并不合适。此时我们可以通过共享内存和**Manager**的方法来共享资源。但这样做提高了程序的复杂度，并因为同步的需要而降低了程序的效率。

Process.PID中保存有PID，如果进程还没有start()，则PID为None。

我们可以从下面的程序中看到Thread对象和Process对象在使用上的相似性与结果上的不同。各个线程和进程都做一件事：打印PID。但问题是，所有的任务在打印的时候都会向同一个标准输出(stdout)输出。这样输出的字符会混合在一起，无法阅读。使用Lock同步，在一个任务输出完成之后，再允许另一个任务输出，可以避免多个任务同时向终端输出。

```
# Similarity and difference of multi thread vs. multi process
# Written by Vamei

import os
import threading
import multiprocessing

# worker function
def worker(sign, lock):
    lock.acquire()
    print(sign, os.getpid())
    lock.release()

# Main
print('Main:',os.getpid())

# Multi-thread
record = []
lock  = threading.Lock()
for i in range(5):
    thread = threading.Thread(target=worker,args=('thread',lock))
    thread.start()
    record.append(thread)

for thread in record:
    thread.join()

# Multi-process
record = []
lock = multiprocessing.Lock()
for i in range(5):
    process = multiprocessing.Process(target=worker,args=('process',lock))
    process.start()
    record.append(process)

for process in record:
    process.join()
```

所有Thread的PID都与主程序相同，而每个Process都有一个不同的PID。

###2、Pipe和Queue

管道PIPE和消息队列message queue，multiprocessing包中有Pipe类和Queue类来分别支持这两种IPC机制。**Pipe**和**Queue**可以用来传送常见的对象。

####(1)、 Pipe可以是**单向**(half-duplex)，也可以是**双向**(duplex)。我们通过**mutiprocessing.Pipe(duplex=False)**创建单向管道 (默认为双向)。一个进程从PIPE一端输入对象，然后被PIPE另一端的进程接收，单向管道只允许管道一端的进程输入，而双向管道则允许从两端输入。

下面的程序展示了Pipe的使用:

```
# Multiprocessing with Pipe
# Written by Vamei

import multiprocessing as mul

def proc1(pipe):
    pipe.send('hello')
    print('proc1 rec:',pipe.recv())

def proc2(pipe):
    print('proc2 rec:',pipe.recv())
    pipe.send('hello, too')

# Build a pipe
pipe = mul.Pipe()

# Pass an end of the pipe to process 1
p1   = mul.Process(target=proc1, args=(pipe[0],))
# Pass the other end of the pipe to process 2
p2   = mul.Process(target=proc2, args=(pipe[1],))
p1.start()
p2.start()
p1.join()
p2.join()
```

这里的Pipe是双向的。

Pipe对象建立的时候，返回一个含有两个元素的表，每个元素代表Pipe的一端(Connection对象)。我们对Pipe的某一端调用**send()**方法来传送对象，在另一端使用**recv()**来接收。

####(2)、 Queue与Pipe相类似，都是先进先出的结构。但Queue允许多个进程放入，多个进程从队列取出对象。Queue使用**mutiprocessing.Queue(maxsize)**创建，maxsize表示队列中可以存放对象的最大数量。

下面的程序展示了Queue的使用:

```
# Written by Vamei
import os
import multiprocessing
import time
#==================
# input worker
def inputQ(queue):
    info = str(os.getpid()) + '(put):' + str(time.time())
    queue.put(info)

# output worker
def outputQ(queue,lock):
    info = queue.get()
    lock.acquire()
    print (str(os.getpid()) + '(get):' + info)
    lock.release()
#===================
# Main
record1 = []   # store input processes
record2 = []   # store output processes
lock  = multiprocessing.Lock()    # To prevent messy print
queue = multiprocessing.Queue(3)

# input processes
for i in range(10):
    process = multiprocessing.Process(target=inputQ,args=(queue,))
    process.start()
    record1.append(process)

# output processes
for i in range(10):
    process = multiprocessing.Process(target=outputQ,args=(queue,lock))
    process.start()
    record2.append(process)

for p in record1:
    p.join()

queue.close()  # No more object will come, close the queue

for p in record2:
    p.join()
```

一些进程使用**put()**在Queue中放入字符串，这个字符串中包含PID和时间。另一些进程从Queue中取出，并打印自己的PID以及**get()**的字符串。

## 作业

####1、参照[Linux多线程与同步](http://www.cnblogs.com/vamei/archive/2012/10/09/2715393.html)中的condition variable的例子，使用Python实现。同时考虑使用面向过程和面向对象的编程方法。

####2、使用mutiprocessing包将本节内容中“多线程与同步 (threading包)”的多线程程序更改为多进程程序。



