# Python使用马尔可夫链算法实现中/英文随机文本生成
## 一、课程介绍
### 1. 课程来源

课程使用的操作系统为 `Ubuntu 14.04`。你可以在我的 [Github](https://github.com/Forec/Markov-Speaking) 上找到全部代码。

### 2. 内容简介

* 介绍马尔可夫链算法
* 使用`jieba`库做基本的中文分词
* Python编写代码实现使用马尔可夫链算法生成随机文本的库

### 3. 课程知识点

本课程项目完成过程中将学习：

* 应用马尔可夫链算法实现随机文本生成的原理
* Python代码对马尔可夫链算法随机文本生成的实现
* `jieba`库中文分词的基本使用

## 二、实验环境

打开终端，进入 `Code` 目录，创建 `work` 文件夹, 将其作为课程的工作目录，并安装`jieba`中文分词库。
```bash
$ mkdir work &amp;&amp; cd work
$ sudo pip2 install jieba
```

## 三、实验原理

马尔可夫链算法一个广为人知的应用是随机文本生成，大致过程是给出一段训练的文本，算法根据这段文本构建一个前缀、后缀表，之后不断迭代，生成一个看起来类似真实语言的随机语句。具体过程如下：
1. 给出训练文本`train.txt`，算法将该文本分词，对每个句子产生一个列表`[word1, word2, ..., wordn]`
2. 将每个句子的前两个词作为一个元素存储在一个列表`Cap`中，代表可以作为句子开始的词。
3. 根据列表构造前缀表，表中的键为列表中的两个连续的词，如`word1 word2`，该键对应的值是文本中出现过的所有紧跟在`word1 word2`后的词。举例，文本中有一个句子为`I wanna be a guy`，则在前缀表`dic`中，`dic[I wanna]`包含`be`，`dic[wanna be]`包含`a`，`dic[be a]`包含`guy`。
4. 从`Cap`中随机选取一项作为生成句子`sentence`的前两个词，我们用 fst 和 snd 两个变量来存储这两个词。
5. 进入循环
 1. `sentence`长度是否满足要求？若满足则退出并返回`sentence`。
 2. `dic[fst snd]`是否为空？为空退出，返回`sentence`。
 3. `dic[fst snd]`不为空，则从`dic[fst snd]`中随机取出一项 trd，`sentence`加上 trd。
 4. fst = snd，snd = trd，重复循环。

## 四、实验步骤

你可以通过以下命令将完整代码下载到实验楼环境中，供参考。
```bash
wget http://labfile.oss.aliyuncs.com/courses/678/markov.py
```

### 1.要编写的类和方法
首先确定要编写的类，将其命名为`Markov`，根据实验原理，它应当具有如下几个功能。
1. 对给定的文本做训练、生成前缀字典
2. 支持中、英文
3. 随机生成一段依据给定文本产生的语句

其中第 2 点对中英文的支持可以包括在第 1 点中，对给定文本做训练时，应当对中英文情况做区分考虑。
因此将要编写的两个主要方法命名为`train`和`say`，`train`方法应该可以接受一个路径作为训练文本的位置，接受训练文本的编码方式以及指定的中英文模式，`say`方法应该可以接受一个参数指定生成句子的最大长度。因此两个方法的定义如下：
```python
train(filepath, mode, coding)
say(length)
```

### 2.实现训练方法
先实现训练方法`train`。

`train`方法应当可以根据指定的编码方式读取待训练文本，该方法最终要产生一个前缀字典`dic`和一个可以用来作为句子开头的词的集合`Cap`，这两个变量应当存储在类的实例中，在训练前应当清除掉这两个变量之前的值。在`filepath`参数为空或 `None`时应当不执行任何操作。因此该方法大致框架如下，最后一句将字典长度也存储在类的实例中，以免每次调用`len`方法求字典长度。
```python
# TODO
	def train(self, filepath = &#39;&#39;, mode = 0, coding=&#34;utf8&#34;):		# 训练
		self.dic = {}
		self.Cap = []
		self.mode = mode
		self.coding = coding				# 初始化
		if filepath is None or filepath == &#39;&#39;:
			return							# 违例判断
		with codecs.open(filepath, &#34;r&#34;, coding) as f:
			for line in f.readlines():
                # TODO
			# 更新前缀字典长度
			self.dictLen = len(self.dic)
# TODO
```

下面编写英文文本情况实现的代码。首先`readlines()`读入的每一行末尾都带有 `\n`，如果是 Windows 环境还会带有 `\r`，因此通过正则去掉读入每行行末的多余空字符。其次，应当根据标点断句，因此用正则对象`en_puncmark`划分读入的每一行，将划分出的每个句子存入列表`sentences`。之后，对每个`sentence`按空格划分，并过滤掉划分出的空字符串。最后遍历每个句子产生的词集合`[word1, word2, ..., wordn]`，并对每一对紧邻的`wordi, wordi+1`构建键`keypair`。同时，如果构建出的`keypair`首字母为大写，则认为这两个单词构成的`keypair`可以作为语句开头，加入`Cap`中。英文训练部分如下。

```python
	def train(self, filepath = &#39;&#39;, mode = 0, coding=&#34;utf8&#34;):		# 训练
		self.dic = {}
		self.Cap = []
		self.mode = mode
		self.coding = coding				# 初始化
		if filepath is None or filepath == &#39;&#39;:
			return							# 违例判断
		eg_puncmark = re.compile(&#39;[\,\.\!\;\?\~\`\#\$\%\@\^&amp;\*\(\)\]\[]&#39;)	# 英文标点正则
		with codecs.open(filepath, &#34;r&#34;, coding) as f:
			for line in f.readlines():
				words = []
				line = re.sub(&#39;[\r\n]&#39;, &#34;&#34;, line)							# 清除行末回车
				if mode == 0:												# 英文模式
					sentences = eg_puncmark.split(line)						# 按标点划分语句
					sentences_words = []
					for sentence in sentences:								# 按空格划分单词并过滤空串
						sentences_words.append(filter(lambda x:x != &#39;&#39;,sentence.split(&#34; &#34;)))
					for words in sentences_words:							# 对每句中的单词
						for i in range(len(words)-2):
							keypair = words[i] + &#34; &#34; + words[i+1]			# 将两个词拼接为前缀
							if keypair[0].isupper():						
								# 若前缀首字母为大写则添加到可作为开头的前缀集合中
								self.Cap.append(keypair)
							if self.dic.get(keypair) is None:
								self.dic[keypair] = [words[i+2]]
							else:
								# 为该前缀添加后缀
								self.dic[keypair].append(words[i+2])
				else:														    # 中文模式
                    # TODO
			# 更新前缀字典长度
			self.dictLen = len(self.dic)
```

下面处理中文分词情况。因为中文标点与英文标点有差，因此我们定义`zh_puncmark`为中文断句。断句之后，每一句使用`jieba`分词，并过滤掉长度小于 2 的词语（这里粗略认为长度小于 2 的可能是量词、助词等）。之后对每句中的词语做和英文相同的处理，但中文每个句子的前两个词语都会被加入`Cap`。

```python
		zh_puncmark = re.compile(&#39;[，。！；]&#39;)								# 中文标点正则
        # ...
				else:														# 中文模式
					sentences = zh_puncmark.split(line)						# 按中文标点划分
					for sentence in sentences:
						jwords = jieba.cut(sentence, cut_all=False)			# 对每句做中文分词
						for word in jwords:
							if len(word) &gt;= 2:
								words.append(word)							# 仅考虑长度大于2的中文词语
						if len(words) &gt; 2:
							self.Cap.append(words[0] + &#34; &#34; + words[1])		# 添加该每句开头的两个词
							words = filter(lambda x:x != &#39;&#39;, words)			# 过滤空串
							for i in range(len(words)-2):
								keypair = words[i] + &#34; &#34; + words[i+1]		# 组建前缀
								if self.dic.get(keypair) is None:
									self.dic[keypair] = [words[i+2]]
								else:
									# 为该前缀添加后缀
									self.dic[keypair].append(words[i+2])
			# 更新前缀字典长度
			self.dictLen = len(self.dic)
```

根据`train`方法中使用到的变量，我们可以写出类`Markov`的`__init()__`函数。目前的`Markov`类如下：
```python
#coding=utf8
import jieba
import codecs
import random
import re

class Markov:
	def __init__(self, filepath = None, mode = 0, coding=&#34;utf8&#34;):
		self.dictLen = 0		# 前缀字典长度
		self.Cap = []			# 可作为语句开始词语的集合
		self.mode = mode		# 模式 中文为1 英文为0
		self.coding = coding	# 编码方式
		self.dic = {}			# 前缀字典
		if filepath is not None:
			self.train(filepath, mode, coding)
	def train(self, filepath = &#39;&#39;, mode = 0, coding=&#34;utf8&#34;):		# 训练
		self.dic = {}
		self.Cap = []
		self.mode = mode
		self.coding = coding				# 初始化
		if filepath is None or filepath == &#39;&#39;:
			return							# 违例判断
		eg_puncmark = re.compile(&#39;[\,\.\!\;\?\~\`\#\$\%\@\^&amp;\*\(\)\]\[]&#39;)	# 英文标点正则
		zh_puncmark = re.compile(&#39;[，。！；]&#39;)								# 中文标点正则
		with codecs.open(filepath, &#34;r&#34;, coding) as f:
			for line in f.readlines():
				words = []
				line = re.sub(&#39;[\r\n]&#39;, &#34;&#34;, line)							# 清除行末回车
				if mode == 0:												# 英文模式
					sentences = eg_puncmark.split(line)						# 按标点划分语句
					sentences_words = []
					for sentence in sentences:								# 按空格划分单词并过滤空串
						sentences_words.append(filter(lambda x:x != &#39;&#39;,sentence.split(&#34; &#34;)))
					for words in sentences_words:							# 对每句中的单词
						for i in range(len(words)-2):
							keypair = words[i] + &#34; &#34; + words[i+1]			# 将两个词拼接为前缀
							if keypair[0].isupper():						
								# 若前缀首字母为大写则添加到可作为开头的前缀集合中
								self.Cap.append(keypair)
							if self.dic.get(keypair) is None:
								self.dic[keypair] = [words[i+2]]
							else:
								# 为该前缀添加后缀
								self.dic[keypair].append(words[i+2])
				else:														# 中文模式
					sentences = zh_puncmark.split(line)						# 按中文标点划分
					for sentence in sentences:
						jwords = jieba.cut(sentence, cut_all=False)			# 对每句做中文分词
						for word in jwords:
							if len(word) &gt;= 2:
								words.append(word)							# 仅考虑长度大于2的中文词语
						if len(words) &gt; 2:
							self.Cap.append(words[0] + &#34; &#34; + words[1])		# 添加该每句开头的两个词
							words = filter(lambda x:x != &#39;&#39;, words)			# 过滤空串
							for i in range(len(words)-2):
								keypair = words[i] + &#34; &#34; + words[i+1]		# 组建前缀
								if self.dic.get(keypair) is None:
									self.dic[keypair] = [words[i+2]]
								else:
									# 为该前缀添加后缀
									self.dic[keypair].append(words[i+2])
			# 更新前缀字典长度
			self.dictLen = len(self.dic)
```

你可以添加一个`getDic`方法来观察生成的前缀字典。
```python
	def getDic(self):
		return self.dic
```

执行下面的命令观察输出。
```bash
$ python
&gt;&gt;&gt; import markov
&gt;&gt;&gt; p = markov.Markov(&#34;The_Standard_Bearer.txt&#34;)
&gt;&gt;&gt; p.getDic()
```

在实验楼环境中运行截图如下，字典过大，这里只截取了输出的最后部分：

![前缀字典输出情况](https://dn-anything-about-doc.qbox.me/document-uid242676labid2204timestamp1476437039114.png/wm)

### 3.实现随机语句生成
下面根据前缀字典随机生成语句。如果前缀字典的长度小于等于 2，则认为很难构建一个可行的句子，直接输出`I feel tired and I need food to say something`。否则， 从`Cap`中随机挑选一个前缀作为开头，之后按实验原理中的循环过程迭代生成随机文本。当当前的`keypair`没有后缀时，或者句子长度符合要求时，循环结束并输出该文本。

```python
	def say(self, length = 10):
		if self.dictLen &lt;= 2:			# 字典长度不足以构建文本
			print(&#34;I feel tired and I need food to say something.&#34;)
		else:
			keypair = self.Cap[random.randint(0, len(self.Cap)-1)]		# 随机选取可作为语句开头的前缀
			fst, snd = keypair.split(&#34; &#34;)[0], keypair.split(&#34; &#34;)[1]		# 将前缀按空格拆分为词语
			pairlen = len(self.dic[keypair])
			if self.mode == 0:
				sentence = fst + &#34; &#34; + snd
			else:
				sentence = fst + snd 									# 中文间无空格
			while length &gt; 0 and pairlen &gt; 0:
				temp = self.dic[keypair][random.randint(0, pairlen-1)]	# 随机选取后缀词语
				fst = snd
				snd = temp
				if self.mode == 0:
					sentence = sentence + &#34; &#34; + snd
				else:
					sentence = sentence + snd
				keypair = fst + &#34; &#34; + snd 								# 更新待搜索前缀
				if self.dic.get(keypair) is not None:
					pairlen = len(self.dic[keypair])
				else:
					break
				length -= 1
			if self.mode == 0:
				print sentence + &#34;.&#34;
			else:
				print sentence + &#34;。&#34;.decode(&#34;utf8&#34;)					# 需为中文标点解码再输出
```

### 4.效果演示
你可以通过下面的指令下载一本英文小说《The Standard Bearer》和《笑傲江湖》，对应的文件名分别是`The_Standard_Bearer.txt`和`swords.txt`，其中`swords.txt`使用UTF-8 编码。
```bash
$ wget http://labfile.oss.aliyuncs.com/courses/678/swords.txt
$ wget http://labfile.oss.aliyuncs.com/courses/678/The_Standard_Bearer.txt
```

确保两本小说在代码当前目录下，执行下面命令查看运行情况
```bash
$ python2
&gt;&gt;&gt; import markov
&gt;&gt;&gt; p = markov.Markov(&#34;The_Standard_Bearer.txt&#34;)
&gt;&gt;&gt; p.say(10)
&gt;&gt;&gt; p.train(&#34;swords.txt&#34;, 1)
&gt;&gt;&gt; p.say(5)
```

上面的指令在实验楼中的一次试验结果如下图。  

![示例](https://dn-anything-about-doc.qbox.me/document-uid242676labid2204timestamp1476437126628.png/wm)

你可以分别尝试中英文、生成句子长度变化产生的句子，截取其中两次在实验楼的结果如下。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid242676labid2204timestamp1476437324474.png/wm)

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid242676labid2204timestamp1476437356327.png/wm)

## 五、代码获取
你可以在我的 [Github](https://github.com/Forec/Markov-Speaking) 中获取到完整的代码。如果你有建议或想法，欢迎提 PR 沟通。
