# 无监督学习介绍及部分函数实现

## 一、课程介绍

### 1.1 内容简介 

目前许多有监督学习算法，如SVM，DNN或是boosting，决策树等，都在工业界分类或决策任务上取得了不错的成果。但是这些有监督学习需要大量带标签的数据，对数据进行上标签又是一个需耗费人力与时间的任务。有许多数据都是不带标签的，因此我们可利用无监督学习对其进行聚类或特征提取。利用无监督学习得到的特征结果也可应用到带标记数据较少的有监督学习任务中，提高其分类性能。

引用目前热门的深度学习领域大牛 LeCun, Bengio, Hiton在国外著名期刊Nature的话，我们就可得知无监督学习在机器学习领域的重要性：

> "We expect unsupervised learning to become far more important in the longer term. Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object" - LeCun, Bengio, Hinton, Nature 2015

![大牛](https://dn-anything-about-doc.qbox.me/document-uid291340labid2287timestamp1478678009791.png/wm)

这门课程，我们将介绍基于无监督学习的一个简单应用—— _自编码器_ ，一个可用于降维或者提取特征的神经网络。


### 1.2 知识点

- 无监督学习 Unsupervised learning
- 聚类 Clustering
- 特征提取 Features extraction
- 自编码器 Autoencoder

## 二、实验原理

### 无监督学习

`无监督学习`的目的是利用无标记数据推断出数据内部隐藏的结构特征。与`有监督学习`从有标记的数据中学到一个有正确答案的模型不同，由于无监督学习的数据是未经标记的，因此在学习过程中就没有所谓误差或者指导信号去评估一个可能的解决方案。

我们可以从下图中看出无监督学习与有监督学习的区别：

![supervised learning](https://dn-anything-about-doc.qbox.me/document-uid291340labid2287timestamp1478745393877.png/wm)

![unsupervised learning](https://dn-anything-about-doc.qbox.me/document-uid291340labid2287timestamp1478745074487.png/wm)

与`有监督学习`有明确的目标输出不同，`无监督学习`没有给定目标输出，而是去提取数据本身的静态结构特征，如在上图中，`有监督学习`的目的是可对输入的图形进行分类，区分这些图形是什么形状的，但是`无监督学习`开始训练前，对于机器来说这些图形没有什么区别，但在经过训练后，我们能够获得这些数据的结构特征，根据这些图形各自的特点，根据图形的相似点对其进行聚类。要注意的是，在无监督学习之后，我们也不知道这些图形分别属于什么几何形状，仅是根据`物以类聚`的概念，将其划分为几个不同的集群。

如上描述的，就是一个常见的无监督学习 —— `数据聚类`。在人工神经网络中，`自我组织映射`和`适应性共振理论` 也是常用的无监督学习，也有许多网络是利用无监督学习对数据进行特征提取，如在图像识别的应用中，可用无监督学习提取图片中物体的边缘特征。

### 自编码器

![自编码器](https://dn-anything-about-doc.qbox.me/document-uid291340labid2287timestamp1478747253090.png/wm)


在[神经网络实现手写字符识别系统](http://www.shiyanlou/courses/593)这门课程中，我们已经介绍了如何运用有监督学习的人工神经网络。在该门课中我们介绍了，如何利用目标输出与网络实际输出的误差进行反向传播，调节每一层间的权值矩阵。自编码器是基于无监督学习为对数据进行更加有效的编码的应用， 自编码器将目标输出等同于输入，同样也是使用误差反向传播机制进行权值调整。`自编码器` 是一个数据降维压缩算法，算法训练的结果是拟合一个恒等函数`$$h_{w,b}(x)=x$$`, 即通过自编码器的训练后，`$$decoder(encoder(x))=x$$`。

目前`自编码器`主要有`数据降噪`和`为数据可视化而降维`两个方面的应用。这里我们设置的自编码器是具有一层隐藏节点的三层全连接神经网络，实现的是`为数据可视化而降维`这方面的应用，上图中展示的就是对数字图像 '3' 降维后载重现的效果。

![自编码器网络](https://dn-anything-about-doc.qbox.me/document-uid291340labid2287timestamp1478758885961.png/wm)

本次课程中，我们实现的是基于`minist手写体数字数据库`中的500张图片，每个数字 (0-9)都有其对应的50张手写体图片，图片的维度为`28 * 28`，在我们设置的网络中，手写体数字图片是作为网络的外部输入结点，内部输入节点设置为1个，隐藏层结点为32个，输出结点与外部输入节点个数一致。 即使通过我们的训练的自编码器，可将原始数据的维度从 `$$784$$` 降维至 `$$32$$`。

在我们的`自编码器`的三层神经网络中，根据网络的输入`前向计算`网络中所有节点的值时，所用的激活函数为sigmoid函数：

`$$
f: sigmoid(x)= \frac{1}{1+e^{-x}}
$$` ![sigmoid function](https://dn-anything-about-doc.qbox.me/document-uid291340labid2287timestamp1478758295103.png/wm)

故在`前向计算`过程中，神经网络中隐藏层节点的值为:

`$$h_i = f(z_i^h)$$`

`$$z_i^h = \sum_{j=1}^{n}x_{j}\cdot w_{j,i} + a \cdot w_{n+1,i}$$`

输出层的节点值为：

`$$\hat{x}_i = f(z_i^{\hat x})$$`

`$$z_i^{\hat x} = \sum_{j=1}^{m}h_j \cdot w_{j,i}$$`

自编码器网络计算实际输出与输入值之间的误差函数（Cost function）`$$C$$`为：

`$$
C =\frac{1}{2}\sum_{i=1}^{n}(\hat{x}_i - x_i)^2
$$`

故在误差反向传播时，第`$$l$$`层的`$$\delta$$`值为：

`$$
\delta_i^l = \dot{f}(z_i^l) \cdot \sum_{j=1}^{n_l+1}w_{ji}^l \cdot \delta_j^{l+1}
$$`

这里所实现的自编码器并没有对编码层（隐藏层）作稀疏限制，若加上稀疏限制使编码层具有稀疏性，则在误差代价函数和误差反向传播就要做相应的修改。

> 稀疏性：只有很少的几个非零元素或只有很少的几个远大于零的元素


## 三、部分函数实现

这里介绍的自编码器作为一个简易的三层神经网络应用，在计算过程中也涉及到了前向计算及误差反向传播。这里我们将介绍如何利用Python实现这两个主要步骤。

前向计算实现：
```python
# w:权值矩阵
# a:神经网络内部节点
# x:神经网络外部节点
def feedforward(w,a,x):
    # 激活函数sigmoid
    f = lambda s: 1 / (1 + np.exp(-s)) 
    
    #将网络的内部及外部输入联合起来与权值矩阵进行加权叠加
    #这里使用的是矩阵运算使训练更加快捷
    w = np.array(w)
    temp = np.array(np.concatenate((a,x),axis=0))
    z_next = np.dot(w , temp)
    
    # 返回计算的下一层神经元的计算结果
    # 及未经过激活函数前的加权叠加结果
    return f(z_next), z_next
```
误差反向传播实现：

```python
# w: 权值矩阵
# z: 当前层的未经过激活函数前的神经元值
# delta_next： 下一层的 δ
def backprop(w,z,delta_next):

    # sigmoid 激活函数
    f = lambda s: np.array(1 / (1 + np.exp(-s)))

    # 激活函数 sigmoid 的导数
    df = lambda s: f(s) * (1 - f(s))
    
    # 误差反向传播计算上一层的 δ 并返回
    delta = df(z) * np.dot(w.T,delta_next)    

    return delta
```

## 四、总结
本节课我们学习了无监督学习和自编码器的概念，并且实现了再无监督学习中需要运用到的 _前向计算_ 和 _误差反向传播_ 这两个函数。使用矩阵运算能使我们的训练过程更加快速。下节课我们将开始自编码器的实现。

### 五、参考阅读

- [Coursera 机器学习](https://www.coursera.org/learn/machine-learning)
- [scikit-learn 无监督学习](http://scikit-learn.org/stable/unsupervised_learning.html)