# 控制 caffe 模型的训练过程

## 一、实验介绍

### 1.1 实验内容

上次实验，我们已经构建好了卷积神经网络，我们的模型已经蓄势待发，准备接受训练了。为了控制训练进程，记录训练过程中的各种数据，caffe 还需要定义另一个`solver.prototxt`文件，这次实验我们就来完成它，并开始激动人心的训练过程。

### 1.2 实验知识点

- 可变的学习速率
- 正则化

### 1.3 实验环境

- caffe 1.0.0

## 二、实验步骤

### 2.1 指定网络定义文件

在`solver.prototxt`中，我们需要先指定网络定义文件的位置，我们通过下面的语句指定：

```
net: "network.prototxt"

```

### 2.2 可变的学习速率

在课程 814 中，有这么一张图：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid49570labid2939timestamp1494557554732.png)
当学习速率固定时，每次参数更新的 “步长” 会慢慢变短。我将为什么步长会变短作为一个选做课后作业留在了那里，其实不难思考，因为学习速率固定，而损失函数图形的 “倾斜” 程度一直在变小，即损失函数对参数的梯度的数值一直在变小，所以最后更新的 “步长” 会越来越短。

这个特性有利于我们的模型训练过程，因为越接近损失函数最低点我们希望更新的步长越小，这样才能使参数更逼近最低点。但为了保证每次更新的步长越来越小，也可以在训练的过程中减小学习速率的数值，在`solver.prototxt`中可以像下面这样定义：

```
base_lr: 0.001
lr_policy: "step"
stepsize: 4000
gamma: 0.1
max_iter: 10000

```

其中`base_lr`指定在开始训练时的学习速率，这里设置为 0.001, `lr_policy`设置为`step`和`step_size`设置为 4000 就指定了学习速率每隔 4000 次训练周期 (epoch) 就自动减小。而`gamma`的数值就是每次减小学习速率时乘以的数值，这里设置为 0.1 就代表每次将学习速率减小到原来的十分之一。

`max_iter`指定训练的最大迭代周期数，这里设置成 10000 次。

### 2.3 测试周期

在前一次实验中，我们的测试数据层（phase 设置为 TEST 的数据层）中的`batch_size`被设置成了 100，而我们的测试数据总共有 10000 张图片，为了每次测试将所有图片都测试一次，这里需要如下设置：

```
test_iter: 100
test_interval:500

```

`test_iter`为 100 即测试的迭代次数为 100 次，这样 100x100 等于 10000，刚好把所有图片都测试一次。

同时，这里设置`test_interval`为 500 表明每训练 500 个周期执行一次测试。

### 2.4 正则化

其实早在课程 814 中，我就非常想讲解正则化，但是一直担心一次性介绍太多内容会让人难以消化，这里我终于迎来了必须讲解正则化的机会。
为了理解正则化在深度学习中的作用，我们以回归问题为例讲解。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid49570labid2939timestamp1494561894280.png)

如图，假设我们现在要根据图中的六个点拟合出数据的分布曲线，用于预测其他 x 坐标对应的 y 值，我们使用如下的多项式进行拟合：
`y=a0+a1*x+a2*x^2+a3*x^3+...`

假如一开始，只有 a0 和 a1 不为 0，其他系数 a 都为 0，拟合函数就变成了：`y=a0+a1*x`, 拟合曲线如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid49570labid2939timestamp1494563531177.png)
由于此时的系数比较少，曲线不够灵活，所以此时拟合的误差较大，损失函数较大。

如果我们再增加一个不为 0 的系数 a2, 拟合函数变成了：`y=a0+a1*x+a2*x^2`, 拟合曲线如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid49570labid2939timestamp1494562889967.png)

这时的拟合效果已经非常好了，但注意仍然有一些数据点的中心不在拟合曲线上，即损失函数值大于 0。
假如此时再增加一个不为 0 的系数 a3, 拟合函数变成了：`y=a0+a1*x+a2*x^2+a3*x^3`, 注意每次增加一个不为 0 的系数，相当于是拟合函数变得更加的灵活。这时拟合曲线可能如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid49570labid2939timestamp1494563145589.png)
此时拟合函数经过每一个数据点的中心，损失函数为 0。但拟合函数的形状已经有些奇怪了。
如果我们继续增加更多的不为 0 的系数，拟合函数曲线甚至可能变成这样：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid49570labid2939timestamp1494563298548.png)
拟合函数非常精确的经过了每一个数据点且此时的损失函数值为 0，但使用这个拟合函数来预测新的 x 点对应的 y 的值可能不会取得非常好的结果。联想之前我们学过的`泛化性能`，这里的泛化性能会非常差。或者说，这里出现了`过拟合（overfitting）`。

我们的深度神经网络模型就可能会出现这个问题，虽然在训练集上的损失函数值已经非常低，甚至为 0，但可能仍然无法在验证 / 测试集上获得很好的泛化性能。

为了避免`过拟合`，就需要我们这里的`正则化（regularization）`, 在`solver.prototxt`里像下面这样设置正则化参数：

```
regularization_type: "L2"
weight_decay:0.0001

```

那么具体`正则化`是如何防止`过拟合`发生的呢？实际上，这里的正则化，是通过在前向计算的过程中，将网络中所有的参数的平方与`weight_decay`相乘，再加到损失函数值上；而反向传递梯度时，则仍然根据链式法则对参数进行更新。
比如这里的`weight_decay`为 0.0001，假设只有一个参数 a=3，则损失函数值计算时就变成了：`L=L0+0.0001*3^2=L0+0.0009`，这里的`L0`是不添加正则化时的损失函数值。反向传递梯度时，对参数的更新就变成了`a=a-lr*(d0+2*0.0001*3)`, 其中`d0`是不添加正则化时的梯度值，而`2*0.0001*3`是误差函数中的正则化项对参数求导（梯度）的结果。实际上，由于这里的 a 值就是 3，之前的式子可以直接改成：`a=(1-lr*2*0.0001)*a-lr*d0`。合理的设置学习速率`lr`和正则化参数`weight_decay`的值，可以使`(1-lr*2*weight_decay)`的值小于 1，这样的效果实际上是使得参数`a`值每次都以一定的比例缩小, 防止参数变得过大。这样可以在一定程度上使高次项的系数变小，从而防止高次项对整个模型的影响过大。从而最终达到防止过拟合的目的。

`正则化`在这里属于稍难的内容，如果你不需要非常深刻的理解深度学习而只是想能够实际上手，可以暂时不去深究正则化的原理，只需要记住`caffe`里的这两个参数是用来进行正则化，从而提高模型效果就行了。

`regularization_type`设置为`L2`就是代表对损失函数加上参数的平方项，还可以将其设置为`L1`，这样加上的就是参数的绝对值。（实际上，这里的`L2`代表的是 2 - 范数，而`L1`代表的是 1 - 范数）

### 2.5 其他设置

我们的`solver.prototxt`还剩下下面这些设置项：

```
display: 100
snapshot: 2000
snapshot_prefix: "snapshot/alpha"
solver_mode: CPU

```

其中`display: 100`代表训练是每隔 100 隔训练周期显示一次损失函数值
`snapshlt: 2000`代表每隔 2000 个训练周期将当前模型的参数`caffemodel`和训练过程中的其他数据`solverstate`存入快照，快照存入的位置由`snapshot_prefix`指定
`solver_mode`指定训练是在`CPU`还是`GPU`上进行，这里是`CPU`。

### 2.6 开始训练

终于，我们的`solver.prototxt`也写好了，可以开始我们的训练了。训练前，你先要确保存放快照文件的文件夹存在，由于这里`snapshot_prefix`为`snapshot/alpha`, 所以我们的快照最终后保存在`snapshot`目录中，我们运行以下命令创建这个目录：

```
mkdir snapshot

```

通过以下命令执行训练过程：

```
caffe train -solver solver.prototxt

```

`train`代表了现在我们是要进行训练，`-solver solver.prototxt`指定了我们的`solver.prototxt`文件的位置。

如果没有出错的话，你会先看到很长的一串输出（caffe 创建模型时的日志输出），接着就是类似这样的损失函数值输出：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid49570labid2939timestamp1494572915771.png)

可以看到，每隔 100 个训练周期，会有损失函数值的输出，损失函数值大致是呈递减的趋势。每隔 500 个训练周期，会有测试准确率输出，准确率大致呈递增的趋势。

整个训练过程 10000 次迭代大致需要三分钟，训练结束后，差不多能够达到 0.994 的准确率。

就这样，我们使用 caffe 训练出了第一个卷积神经网络模型（鼓掌）。在 snapshot 文件夹下面，你可以找到训练完毕的模型参数文件。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid18510labid2939timestamp1495090082853.png/wm)

## 三、实验总结

至此，模型的构建和训练过程已经全部完成了，我们拥有了一个准确率超过 0.99 的卷积神经网络模型。下次实验，我们会利用这个模型去开发一个图片字母识别程序，让我们的模型真正的能够发挥作用。

本次实验，我们学习了：

- 让学习速率随着训练的过程逐渐变小可以使最终的参数更接近理想点。
- 正则化是保证模型获得较高的泛化性能的重要手段之一。

## 四、课后作业

1. `solver.prototxt`中的超参数我已经提前设置好了，请你自己尝试不同的超参数设置，观察超参数的变化对模型训练过程的影响。