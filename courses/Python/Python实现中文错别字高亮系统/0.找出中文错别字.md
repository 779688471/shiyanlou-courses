# 找出中文错别字

## 一、实验介绍

### 1.1 实验内容

错别字一直是挺影响读者体验的东西。实验楼作为以文档为基础的教育网站，已经由用户纠正了很多文档中的错别字。这次我们希望上线一个错别字高亮系统，输入我们的 MarkDown 文档，输出一个高亮过疑似错别字的 HTML 文件。

### 1.2 实验知识点

- Python3 的基本用法
- 中文分词库 "Jieba" 的基本使用
- BeautifulSoup 库的基本使用

### 1.3 实验环境

- Python 3.6
- Xfce 终端

### 1.4 适合人群

本课程难度为一般，属于中级级别课程，适合具有 Python 基础的用户，熟悉 python 基础知识加深巩固。

### 1.5 代码获取

你可以通过下面命令将代码下载到实验楼环境中，作为参照对比进行学习。

```
$ wget http://labfile.oss.aliyuncs.com/courses/828/Document.tar

```

## 二、实验原理

本次实验的原理部分非常简单。假设我们有一个一句话的语料库 --“欢迎来到本次实验”，我们通过中文分词库 Jieba 对这句话进行分词（）

得到结果

“欢迎 / 来到 / 本次 / 实验”

我们可以创建一个字典给每个出现的单词一个唯一的 ID(编号)

| 单词   | 编号   |
| ---- | ---- |
| 欢迎   | 0    |
| 来到   | 1    |
| 本次   | 2    |
| 实验   | 3    |

这样做的目的是方便我们储存单词与单词之间的`前后关系`。如果语料库中的两个单词是`前后出现的`，那么我们认为这个搭配就是合理的。

我们需要一个数据结构来储存语料库中单词的 `前后关系` 。这时候就有两个选择，一个是建立一个邻接矩阵：

| 开始 \ 结束 | 欢迎   | 来到   | 本次   | 实验   |
| ------- | ---- | ---- | ---- | ---- |
| 欢迎      | 0    | 1    | 0    | 0    |
| 来到      | 0    | 0    | 1    | 0    |
| 本次      | 0    | 0    | 0    | 1    |
| 实验      | 0    | 0    | 0    | 0    |

可以预想到，假设我们的语料库长度为 n ，我们可以以 `O(1)` 的时间复杂度来完成增添条目和查找条目的工作（因为我们给每个单词都设定了 ID，所以可以直接访问前后关系对应的内存块），但是这个矩阵会占据 `O(n*n)` 的空间，而且这是一个`稀疏矩阵`，很多的空间被浪费了。

我们考虑另外一种方式，使用字典来储存。

| 前后关系    | 出现频率 |
| ------- | ---- |
| 欢迎 / 来到 | 1    |
| 来到 / 本次 | 1    |
| 本次 / 实验 | 1    |

这样的字典会占据 `O(n)` 的空间，但是 Python 对于每一个键都进行了哈希，保证了我们可以以大约 `O(1)` 的时间复杂度来增添条目和查找条目 (除非我们的输入很奇怪，否则是不会遇到最坏的 `O(n)` 复杂度的情况的)。这样我们既节省了空间，又保证了我们的效率，开心。

本实验的作者一开始使用的是第一种邻接矩阵的数据结构，但是后来实在觉得浪费空间，改为使用 Python 强大的字典（真的很强大啊！）。各位同学如果有更好的数据结构，请在评论区留言~ 关于 Python 各个数据结构操作的时间复杂度，参考 [WikiPython TimeComplexity](https://wiki.python.org/moin/TimeComplexity)。

有了前后关系之后，我们就可以根据我们的字典来判断输入的句子是否合理了。例如，我们输入一个句子，“欢迎来到实验一”，根据 Jieba 词库的分词，这个句子被分为 “欢迎 / 来到 / 实验 / 一” 。这其中 “欢迎 / 来到” 这个组合存在于我们的词典中，我们认为它是合理的。然而 “来到 / 实验” 和 “实验 / 一” 不能算作是合理的搭配，我们把他们算作是可能的错误。

可以预想到，我们不可能覆盖中文中所有的组合，本次实验环境也不允许我们建立这么大的字典。我们需要场外的 “助攻”，本次实验需要借助搜索引擎来排除大部分正确的组合，找到真正的错别字。不过我们首先先完成我们的 `前后关系` 字典的创建。

## 三、开发准备

打开 Xfce 终端，进入 `Code` 目录，创建 `ChineseSpellingCheck` 文件夹, 将其作为课程的工作目录。

本次实验建议在 virtualenv 环境下进行。

关于 virtualenv，请看 [virtualenv 介绍](http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001432712108300322c61f256c74803b43bfd65c6f8d0d0000)

首先安装 virtualenv

```
$sudo pip3 install virtualenv

$ virtualenv ChineseSpellingCheck
$ cd ChineseSpellingCheck
$ source bin/activate

```

然后我们就能够在 virtualenv 下进行我们的实验了

安装 Jieba 库

```
$ pip3 install jieba
$ pip3 install beautifulsoup4
$ pip3 install numpy

```

注意这次 pip3 前面不能加 sudo，加了 sudo 就不是在 virtualenv 环境下安装了。

## 四、项目文件结构

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid122063labid2921timestamp1494498833242.png/wm)

## 五、实验步骤

### 5.1 实现前后关系字典

创建文件夹 `Code`

在 `Code` 文件夹中创建文件 `CountOfNumbers.py`

我们先导入我们这次实验需要的包

```
import collections
import jieba
import numpy as np
from isIdealString import *

```

`isIdealString` 是我们自己写的模块，因为我们是中文错别字纠错，英文、数字还有符号都不在我们考虑的范围内，所以当我们检测到我们的单词 `前后关系` 中有一个不符合标准，就不用把这个前后标准放入我们的字典当中。

在 `ChineseSellingCheck` 文件夹当中新建新的 .py 文件 `isIdealString.py` （建立新文件的目的是为了之后代码的复用）

```
# stop_symbols 是文档中可能出现的符号

stop_symbols = [".", ",","。","#", "\n", "(", "、", "`", "，", \
"##","$","###","####", "-", "+","|","/", ":", "：","*","?","!","@", \
"#"," ","\'","\"","\\",";","%",")","(","<",">","？","！","；","「","」","（","）" \
"[","]","{","}","“","”","）","《","》","=","\t","】","__"]

# word 和 PreviousWord 是我们要检测的前后关系

def isIdealString(word, PreviousWord):
    if PreviousWord not in stop_symbols \
    and word not in stop_symbols \
    and not word[0].encode("UTF-8").isalpha() \
    and not PreviousWord[0].encode("UTF-8").isalpha() \
    and not word[0].isdigit() \
    and not PreviousWord[0].isdigit():
        return True
    else:
        return False

```

word[0].encode("UTF-8") 的作用是检测英文的出现，如果一个中文字符如果没有通过 "UTF-8" 编码，也会被 `isalpha()` 函数认为是一个英文字符。

我们继续我们的 `CountOfNumbers.py` 的编写

```
WordsToIndex = collections.defaultdict(int)

counter = 0
BigramCounter = collections.defaultdict(int)

```

我们通过系统的 `collections` 库创建了一个默认字典 --defaultdict，这个字典的 值 默认是 int 类型的，这样的字典效率会比普通的字典效率要高。`WordsToIndex`字典是我们在实验原理中提到的，给每个单词赋予一个唯一编号 (Index) 的字典，在这个字典中，键（key）是我们的中文单词，值（value）则是这个单词的 ID。而 `BigramCounter` 则是描述单词 `前后关系` 的字典，在这个字典中，键（key）是两个中文单词组成的元组，值（value）则是这个元组出现的次数。

```
# 我们的测试语料共有 2 个文件

for i in range(1,3):

    f = open("../Data/File%d.txt" % i)


    # 读取数据
    file = f.read()

    # 调用 Jieba 库进行中文分词
    seg_list = jieba.cut(file, cut_all=False)


    for word in seg_list:
        # 初始化 WordsToIndex
        if word not in WordsToIndex:
            WordsToIndex[word] = counter
            counter = counter + 1

        # 初始化 BigramCounter
        if counter == 1:
            PreviousWord = word
        else:
            if isIdealString(word,PreviousWord): 
                BigramCounter[WordsToIndex[PreviousWord],WordsToIndex[word]] += 1
            PreviousWord = word

```

在 WordsToIndex 这个字典中，一个中文单词的 Index 其实是由一个计数器 Counter 给出的，每遇到一个新单词，Counter 这个变量都会自加 1。

很简单，我们完成了这么两个字典的创建与赋值，接下来就是需要把这两个字典保存起来。我们在`CountOfNumbers.py`文件后面加上

```
np.save('WordsToIndex_test.npy', WordsToIndex)
np.save('BigramCounter_test.npy', BigramCounter)

```

把字典保存成 .npy 文件，我们可以方便得传输和增添我们的数据。

### 5.2 借助搜索引擎判断单词是否常用

由于这个系统服务的对象是实验楼未来的文档，所以最佳的语料库就是实验楼现有的众多文档。在上一节中我们已经学习了怎么样创建并保存我们的 `前后关系` 字典。由于实验楼内部的文档不方便公开，我们没有直接提供语料库，而是提供了最终生成的字典。

之前也讨论了，再大的语料库，都不可能覆盖所有中文单词搭配。所以我们需要搜索引擎的场外助攻。

首先请下载我们的数据

```
$wget http://labfile.oss.aliyuncs.com/courses/828/BigramCounter.npy
$wget http://labfile.oss.aliyuncs.com/courses/828/WordsToIndex.npy

```

记得把我们下载的数据放置在 `home/shiyanlou/ChineseSpellingCheck` 这个路径下。

然后我们需要创建新的 .py 文件 `CrawlTitle.py`。

我们的基本思路是把没有在语料库中出现的单词搭配放到 必应（Bing） 中进行搜索，对于那些常用的中文搭配，搜索结果肯定会把这对搭配高亮显示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid122063labid2921timestamp1494400442438.png/wm)

查看网站的源码，我们发现有关 `创建文档` 的标题都用 `<strong>` 标签标示了出来。

如果一个搭配不常出现。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid122063labid2921timestamp1494400646668.png/wm)

搜索引擎会自动询问 `是否只需要 --- 的结果`

或者搜索引擎的标题中很少有该中文单词搭配的高亮结果。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid122063labid2921timestamp1494401051156.png/wm)

我们根据这些现象，可以把常见的中文单词搭配从嫌疑列表中剔除。

```
import sys 
from bs4 import BeautifulSoup
from urllib import parse
from urllib import request

# question_word 是我们传入的需要检查是否为常用搭配的单词元组
def GetTitle(question_word):

    url = parse.quote('http://cn.bing.com/search?q='+question_word, safe='/:?=' )

    # 假装成一个浏览器进行访问
    req = request.Request(
    url, 
    data=None, 
    headers={
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'
        }
    )

    # 得到了我们的页面
    htmlpage = request.urlopen(req).read().decode('utf8')

    # 对我们的HTML界面进行解析
    soup = BeautifulSoup(htmlpage,'html.parser')

    # 找到所有<strong>标签，res是一个列表
    res = soup.find_all('strong')


    NeedAutoCorrection = False

    # 如果搜索引擎有提示，那么这个搭配就肯定有问题
    if htmlpage.find('<div>是否只需要 <a') != -1:
        NeedAutoCorrection = True

    return res, NeedAutoCorrection

```

我们还需要一个函数来判断这个搭配是否常用。这个函数相对来说比较简单，如果我们的单词单配在搜索引擎中出现次数超过 3 次，那么我们就认为它是一个常见的搭配。

```
def CheckCorrectness(res, question_word):

    counter = 0

    for highlightWords in res:
        content = highlightWords.get_text()

        if content.find(question_word) != -1:
            counter += 1

    if counter > 3:
        return True
    else:
        return False

```

### 5.3 测试系统

我们现在需要写一个脚本来测试我们这个系统的表现。我们读入测试文档，对其进行分词，生成前后单词搭配元组，然后在 `前后搭配` 字典中寻找是否有相同的元组，如果没有就放到搜索引擎中进行搜索并判断是否常见，如果不常见就放入错误搭配列表中。

首先获取我们的测试文件, 请把 `test1.md` 放置在 `home/shiyanlou/ChineseSpellingCheck` 这个目录下。

```
$wget http://labfile.oss.aliyuncs.com/courses/828/test1.md

```

测试文件当中的是用户找出的错别字。我们可以明显的找到其中的错别字

```
###这是一个用来传输信息的同道（应该是通道）。 

*这个函数创建额（了）类型。*

**这个函数创建了累型（类型）。**

+ 这样的轮训算法（轮询）。
+ 这样害死不行（还是）。
+ 面向过程面层（面向过程编程）。

```

我们创建新的 .py 文件 `TestOfDoc.py`。

导入我们需要的库

```
import jieba
import numpy as np
from isIdealString import * 
import sys
import CrawlTitle
import time
import sys

```

然后获得我们的输入文件和输出文件（输出文件之后会说明，现在就随便填一个 .html 文件）

```
if len(sys.argv) < 3:
    print("Wrong parameter")
    print("./copyfile.py Input_File_Name Output_File_Name(.html file)")
    sys.exit(1)

```

获得我们的数据，并对测试文件进行分词。把没有在前后关系文档中出现的搭配放到 SuspiciousList 当中去。（ suspicious adj. 有嫌疑的）

```
# 从我们的 .npy 文件中读取我们的字典
WordsToIndex = np.load("WordsToIndex_test.npy").item()
BigramCounter = np.load("BigramCounter_test.npy").item()


# 读取测试文件
f = open(sys.argv[1])

file = f.read()

seg_list = jieba.cut(file,cut_all=False)

SuspiciousList = []


counter = False


# 获取我们的测试文件中的分词数据

for word in seg_list:
    if counter == False:
        PreviousWord = word
        counter = True
    else:
        if isIdealString(word,PreviousWord): 
            if BigramCounter[WordsToIndex[PreviousWord],WordsToIndex[word]] == 0:
                SuspiciousList.append((PreviousWord,word))
        PreviousWord = word

print(SuspiciousList)
print(len(SuspiciousList))
print("\n")

time.sleep(1)

```

获得了 SuspiciousList 之后，我们可以通过搜索引擎的判断来获得最终的错误单词列表 WrongWordList

```
WrongWordList = []

for pairs in SuspiciousList:
    question_word = ""
    question_word += pairs[0]
    question_word += pairs[1]
    res, NeedAutoCorrection = CrawlTitle.GetTitle(question_word)

    if not CrawlTitle.CheckCorrectness(res, question_word) or NeedAutoCorrection:
        WrongWordList.append(pairs)
    time.sleep(0.5)

print(WrongWordList)
print(len(WrongWordList))

```

我们来看一下最后的效果

```
$python3 TestOfDoc.py ../test1.md results.html

```

我们看看测试结果，最终的 SuspitiousList 和 WrongWordList:

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid122063labid2921timestamp1494499615124.png/wm)

总共 6 句句子找出了其中的 4 个错误，这个纠错率还是可以的。大家也可以自己手动打一些错别字去尝试一下~

### 5.4 把错误的单词在 HTML 文件中高亮

我们在找出错误的单词之后，肯定想看一看单词在原文中的位置方便修改。实验楼所有的文档都是 .md 的 MarkDown 格式书写的，没有办法直接高亮。但是我们可以吧 MarkDown 解析成 HTML 然后在网页上对错误的单词进行高亮操作。

我们首先需要一个 MarkDown 解析器。

```
$ pip3 install mistune

```

mistune 是用 python 编写的 MarkDown 解析器，使用起来很方便

我们创建新的 .py 文件 `ToHTML.py`

```
# -*- coding: utf-8 -*-
import mistune

# 请先看下方的 ToHTML 函数
def AddWrongWord(htmlpage, WrongWordList):
    InsertPosition = []

    # flag 变量的目的是标明前一个需要高亮的单词的末尾位置
    # 防止出现多次高亮，重复高亮的情况

    flag = -1
    for item in WrongWordList:
        WrongWords = ""
        WrongWords += item[0]
        WrongWords += item[1]

        Ins_start = htmlpage.find(WrongWords)

        # 如果找不到
        if Ins_start == -1:
            print("Cannot find the Wrong Word" + WrongWords)
            continue
        else:
            # 如果两个需要高亮的单词没有重复
            if Ins_start > flag :
                Ins_end = Ins_start + len(WrongWords)
                flag = Ins_end
            # 如果有重复
            else:
                Ins_end = Ins_start + len(WrongWords)
                Ins_start = flag
                flag = Ins_end

        InsertPosition.append((Ins_start,Ins_end))


    # 根据插入位置进行<mark> 高亮
    NewPage = ""
    PreviousStart = 0
    for item in InsertPosition:
        NewPage += htmlpage[PreviousStart:item[0]]
        NewPage += "<mark>"
        NewPage += htmlpage[item[0]:item[1]]
        NewPage += "</mark>"
        PreviousStart = item[1]

    return NewPage

# file 是我们的 MarkDown 文件
# OutputName 是我们最终生成的HTML文件的名称
# WrongWordList 是我们之前求出的错误单词列表
def ToHTML(file, OutputName, WrongWordList):
    # 生成一个 Markdown 对象
    markdown = mistune.Markdown()

    # 进行 MarkDown 解析
    htmlpage = markdown(file)

    # AddWrongWord 是高亮错误单词的函数
    htmlpage = AddWrongWord(htmlpage,WrongWordList)

    try:
        OutputFile = open(OutputName,'r+')
    except FileNotFoundError:
        OutputFile = open(OutputName,'w')

    # 向HTML文件中写入必要的信息以正确显示中文
    OutputFile.write("<head>\n")
    OutputFile.write("<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>")
    OutputFile.write("</head>\n")

    # 写入 HTML 文本
    OutputFile.write(htmlpage)

```

最后我们在 `TestOfDoc.py` 中引入我们的 TOHTML（） 函数，这里不要忘记引入`ToHTML`模块

```
ToHTML.ToHTML(file,sys.argv[2], WrongWordList)

```

重新进行测试：

```
$ python3 TestOfDoc.py ../test1.md results.html

```

这样我们就能够在 `Code` 文件夹下找到我们的 `result.html` 文件，用 firefox 打开：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid122063labid2921timestamp1494500138636.png/wm)

不仅正确完成了 MarkDown 的解析，还标出了所有的错误。

## 六、总结

其实这个纠错系统还比较傻瓜，如果你把它作用于大型的文档，会找出很多并不是错别字的中文搭配。症结还是语料库比较小。在算法方面也有很大的提升空间，但是笔者查阅了很多的论文与资料都没能找到一个简明的算法来实现这个系统。如果读者您有好的算法或者好的语料库，请在评论区多多指教~