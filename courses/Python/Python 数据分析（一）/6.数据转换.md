# 数据转换


---

## 一、实验介绍 ##

　到目前为止，木木介绍的都是数据的重排。而另一类重要操作则是过滤、清理以及其他的转换工作。今天我们就来更深入的学习数据的转换知识吧！

## 二、移除重复数据 ##

　　DataFrame 中常常会出现重复行，举个例子：

```
In [5]: data = DataFrame({&#39;k1&#39;:[&#39;one&#39;]*3+[&#39;two&#39;]*4,
                  &#39;k2&#39;:[1,1,2,3,3,4,4]})

In [6]: data
Out[6]: 
    k1  k2
0  one   1
1  one   1
2  one   2
3  two   3
4  two   3
5  two   4
6  two   4

In [7]: #DataFrame 的 duplicated 方法返回一个布尔型 Seriess

In [8]: # 表示各行是否是重复行

In [9]:  data.duplicated()
Out[9]: 
0    False
1     True
2    False
3    False
4     True
5    False
6     True
dtype: bool

In [10]: #drop_duplicates 方法，用于返回一个移除了重复行的 DataFrame

In [11]: data.drop_duplicates()
Out[11]: 
    k1  k2
0  one   1
2  one   2
3  two   3
5  two   4

In [12]: # 这个方法默认会判断全部列

In [13]: #当然我们也可以指定部分列进行重复判断

In [14]: data[&#39;v1&#39;] = range(7)

In [15]: data
Out[15]: 
    k1  k2  v1
0  one   1   0
1  one   1   1
2  one   2   2
3  two   3   3
4  two   3   4
5  two   4   5
6  two   4   6

In [16]: data.drop_duplicates([&#39;k1&#39;])
Out[16]: 
    k1  k2  v1
0  one   1   0
3  two   3   3

In [17]: # 上面是只通过k1列过滤重复项

```
　　duplicated 和 drop_duplicates 默认保留的是第一个出现的值组合。传入 take_last = True 则保留最后一个：

```
In [18]: data.drop_duplicates([&#39;k1&#39;,&#39;k2&#39;],take_last = True)
Out[18]: 
    k1  k2  v1
1  one   1   1
2  one   2   2
4  two   3   4
6  two   4   6
```


## 三、利用函数或映射进行数据转换 ##

　　在对数据集进行转换时，我们希望根据数组、Series 或 DataFrame 列中的值来实现工作。

```
In [4]: data = DataFrame({&#39;food&#39;:[&#39;bacon&#39;,&#39;pulled pork&#39;,&#39;bacon&#39;,
   ...:                           &#39;pastrami&#39;,&#39;corned beef&#39;,&#39;bacon&#39;,
   ...:                           &#39;pastrami&#39;,&#39;honey ham&#39;,&#39;nova lox&#39;],
   ...:                   &#39;ounces&#39;:[4, 3, 12, 6, 7.5, 8, 3, 5, 6]})

In [5]: data
Out[5]: 
          food  ounces
0        bacon     4.0
1  pulled pork     3.0
2        bacon    12.0
3     pastrami     6.0
4  corned beef     7.5
5        bacon     8.0
6     pastrami     3.0
7    honey ham     5.0
8     nova lox     6.0

In [6]: # 我们添加一个食物来源

In [7]:  meat_to_animal = {
   ...: &#39;bacon&#39;: &#39;pig&#39;,
   ...: &#39;pulled pork&#39;: &#39;pig&#39;,
   ...: &#39;pastrami&#39;: &#39;cow&#39;,
   ...: &#39;corned beef&#39;: &#39;cow&#39;,
   ...: &#39;honey ham&#39;: &#39;pig&#39;,
   ...: &#39;nova lox&#39;: &#39;salmon&#39;
   ...: }
   
In [8]: #Series 的 map 方法可以接受一个函数或含有映射关系的字典型对象

In [9]: data[&#39;animal&#39;] = data[&#39;food&#39;].map(meat_to_animal)

In [10]: data
Out[10]: 
          food  ounces  animal
0        bacon     4.0     pig
1  pulled pork     3.0     pig
2        bacon    12.0     pig
3     pastrami     6.0     cow
4  corned beef     7.5     cow
5        bacon     8.0     pig
6     pastrami     3.0     cow
7    honey ham     5.0     pig
8     nova lox     6.0  salmon

In [11]: # 我们也可以传入一个能够完成全部工作的函数：


In [12]: data[&#39;food&#39;].map(lambda x: meat_to_animal[x])
Out[12]: 
0       pig
1       pig
2       pig
3       cow
4       cow
5       pig
6       cow
7       pig
8    salmon
Name: food, dtype: object

```
　　使用 map 是一种实现元素级转换以及其他数据清理工作的便捷方式。

## 四、替换值 ##

　　利用 fillna 方法填充缺失数据可以看做值替换的一种特殊情况。虽然前面提到的 map 可用于修改对象的数据子集，而 replace 则提供了一种实现该功能的更简单、更灵活的方式

```
In [20]: data = Series([1., -999., 2., -999., -1000., 3.])

In [21]: data
Out[21]: 
0       1
1    -999
2       2
3    -999
4   -1000
5       3
dtype: float64

In [22]: #接下来我想将-999 换成 NAN

In [23]: data.replace(-999, np.nan)
Out[23]: 
0       1
1     NaN
2       2
3     NaN
4   -1000
5       3
dtype: float64

In [24]: # 如果我们想一次性换多个值

In [25]:  data.replace([-999,-1000],np.nan)
Out[25]: 
0     1
1   NaN
2     2
3   NaN
4   NaN
5     3
dtype: float64

In [26]: data.replace([-999,-1000],[np.nan, 0])
Out[26]: 
0     1
1   NaN
2     2
3   NaN
4     0
5     3
dtype: float64

In [27]: data.replace({-999: np.nan, -1000: 0})
Out[27]: 
0     1
1   NaN
2     2
3   NaN
4     0
5     3
dtype: float64
```

## 五、重命名轴索引 ##

　　跟 Series 中的值一样，轴标签也可以通过函数或映射进行转换，从而得到一个新对象。轴还可以被就地修改，而无需新建一个数据结构。


```
In [28]: data = DataFrame(np.arange(12).reshape((3,4)),
   ....:                  index = [&#39;Ohio&#39;,&#39;Colorado&#39;,&#39;New York&#39;],
   ....:                  columns = [&#39;one&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;])

In [29]: # 跟 Series 一样，轴标签也有一个 map 方法

In [30]: data.index.map(str.upper)
Out[30]: array([&#39;OHIO&#39;, &#39;COLORADO&#39;, &#39;NEW YORK&#39;], dtype=object)

In [31]: # 同样我们也可以赋值给 index

In [32]: data.index = data.index.map(str.upper)

In [33]: data
Out[33]: 
          one  two  three  four
OHIO        0    1      2     3
COLORADO    4    5      6     7
NEW YORK    8    9     10    11

In [34]: # 如果想要创建数据集的转换版（不是修改原始数据）

In [35]: data.rename(index = str.title,columns = str.upper)
Out[35]: 
          ONE  TWO  THREE  FOUR
Ohio        0    1      2     3
Colorado    4    5      6     7
New York    8    9     10    11

In [36]: #rename 可以结合字典对象实现部分轴标签的更新

In [37]: data.rename(index={&#39;OHIO&#39;:&#39;INDIANA&#39;},
   ....:             columns = {&#39;three&#39;:&#39;peekaboo&#39;})
Out[37]: 
          one  two  peekaboo  four
INDIANA     0    1         2     3
COLORADO    4    5         6     7
NEW YORK    8    9        10    11
```

## 六、离散化和面元划分 ##

　　为了便于分析，连续数据常常被离散化或拆分为“面元”（bin）。假设有一组人员数据，我们希望将它们划分为不同的年龄组：

```
In [39]: ages = [20, 22, 25, 27, 21, 23, 24, 18, 37, 31,61, 42, 48, 32]

In [40]: # 接下来我们将这些数据分成18~25、26~35、35~60和60岁以上

In [41]:  bins = [18, 25, 35, 60, 100]

In [42]: cats = pd.cut(ages,bins)

In [43]: cats
Out[43]: 
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]
Length: 14
Categories (4, object): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]]

In [44]: #pandas  用 cut 函数返回的是一个特殊的 Categorical 对象

In [45]: #我们可以将其看做一组表示面元名称的字符串

In [46]: #它含有一个表示不同分类名称的 levels 数组

In [47]: cats.levels
Out[47]: Index([u&#39;(18, 25]&#39;, u&#39;(25, 35]&#39;, u&#39;(35, 60]&#39;, u&#39;(60, 100]&#39;], dtype=&#39;object&#39;)

In [48]: # 还有一个为年龄数据进行标号的 labels 属性

In [49]:  cats.labels
Out[49]: array([ 0,  0,  0,  1,  0,  0,  0, -1,  2,  1,  3,  2,  2,  1], dtype=int8)

In [50]: pd.value_counts(cats)
Out[50]: 
(18, 25]     6
(35, 60]     3
(25, 35]     3
(60, 100]    1
dtype: int64

```
　　如果向 cut 传入的是面元的数量而不是确切的面元边界，则它会根据数据的最小值和最大值计算等长面元。

```
In [56]: data = np.random.rand(20)

In [57]: pd.cut(data, 4, precision = 2)
Out[57]: 
[(0.72, 0.96], (0.25, 0.49], (0.49, 0.72], (0.72, 0.96], (0.25, 0.49], ..., (0.014, 0.25], (0.72, 0.96], (0.49, 0.72], (0.25, 0.49], (0.72, 0.96]]
Length: 20
Categories (4, object): [(0.014, 0.25] &lt; (0.25, 0.49] &lt; (0.49, 0.72] &lt; (0.72, 0.96]]
```

　　qcut 是一个非常类似于 cut 的函数，它可以根据样本分位数对数据进行面元划分。根据数据的分布情况，cut 可能无法使各个面元含有相同数量的数据点。而 qcut 由于使用的是样本分位数，因此可以得到大小基本相等的面元：

```
In [58]: data = np.random.randn(1000)

In [59]: cats = pd.qcut(data,4)

In [60]: cats
Out[60]: 
[(-0.699, 0.0415], [-3.0467, -0.699], (0.0415, 0.731], (0.731, 3.0755], [-3.0467, -0.699], ..., (0.0415, 0.731], [-3.0467, -0.699], (-0.699, 0.0415], [-3.0467, -0.699], [-3.0467, -0.699]]
Length: 1000
Categories (4, object): [[-3.0467, -0.699] &lt; (-0.699, 0.0415] &lt; (0.0415, 0.731] &lt; (0.731, 3.0755]]

In [61]: pd.value_counts(cats)
Out[61]: 
(0.731, 3.0755]      250
(0.0415, 0.731]      250
(-0.699, 0.0415]     250
[-3.0467, -0.699]    250
dtype: int64

```

## 七、检测和过滤异常值 ##

　　异常值（outlier）的过滤或变换运算在很大程度上其实就是数组运算。
```
In [63]: np.random.seed(12345)

In [64]: data = DataFrame(np.random.randn(1000,4))

In [65]: data.describe()
Out[65]: 
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean     -0.067684     0.067924     0.025598    -0.002298
std       0.998035     0.992106     1.006835     0.996794
min      -3.428254    -3.548824    -3.184377    -3.745356
25%      -0.774890    -0.591841    -0.641675    -0.644144
50%      -0.116401     0.101143     0.002073    -0.013611
75%       0.616366     0.780282     0.680391     0.654328
max       3.366626     2.653656     3.260383     3.927528

In [66]: # 假设你想要找出某列中绝对值大小超过3的值

In [67]:  col = data[3]

In [68]: col[np.abs(col) &gt; 3]
Out[68]: 
97     3.927528
305   -3.399312
400   -3.745356
Name: 3, dtype: float64

In [69]: # 要选出全部的绝对值大于三的行

In [70]: data[(np.abs(data)&gt;3).any(1)]
Out[70]: 
            0         1         2         3
5   -0.539741  0.476985  3.248944 -1.021228
97  -0.774363  0.552936  0.106061  3.927528
102 -0.655054 -0.565230  3.176873  0.959533
305 -2.315555  0.457246 -0.025907 -3.399312
324  0.050188  1.951312  3.260383  0.963301
400  0.146326  0.508391 -0.196713 -3.745356
499 -0.293333 -0.242459 -3.056990  1.918403
523 -3.428254 -0.296336 -0.439938 -0.867165
586  0.275144  1.179227 -3.184377  1.369891
808 -0.362528 -3.548824  1.553205 -2.186301
900  3.366626 -2.372214  0.851010  1.332846

In [71]: #根据这些条件，既可以轻松对值进行设置

In [72]:  data[np.abs(data)&gt;3] = np.sign(data)*3

In [73]: data.describe()
Out[73]: 
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean     -0.067623     0.068473     0.025153    -0.002081
std       0.995485     0.990253     1.003977     0.989736
min      -3.000000    -3.000000    -3.000000    -3.000000
25%      -0.774890    -0.591841    -0.641675    -0.644144
50%      -0.116401     0.101143     0.002073    -0.013611
75%       0.616366     0.780282     0.680391     0.654328
max       3.000000     2.653656     3.000000     3.000000
```

## 八、排列和随机采样 ##

　　利用 numpy.random.permutation 函数可以轻松实现对 Series 和 DataFrame 的列的排列工作（permuting，随机重排序）。通过需要排列的轴的长度调用 permutation，产生一个表示新顺序的整数数组

```
In [74]: df = DataFrame(np.arange(20).reshape(5,4))

In [75]: sampler = np.random.permutation(5)

In [76]: sampler
Out[76]: array([1, 0, 2, 3, 4])

In [77]: # 然后就可以在基于 ix 的索引操作或 take 函数中使用该数组

In [78]:  df
Out[78]: 
    0   1   2   3
0   0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19

In [79]: df.take(sampler)
Out[79]: 
    0   1   2   3
1   4   5   6   7
0   0   1   2   3
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19
```
　　如果不想用替换的方式选取随机子集，则可以使用 permutation：从 permutation 返回的数组中切下前 k 个元素，其中 k 为希望的子集大小。

```
In [81]: df.take(np.random.permutation(len(df))[:3])
Out[81]: 
    0   1   2   3
1   4   5   6   7
3  12  13  14  15
4  16  17  18  19

```
## 九、计算指标/哑变量 ##

　　另一种常用于统计建模或机器学习的转换方式：将分类变量（categorical variable）转换为“哑变量矩阵”（dummy matrix）或“指标矩阵”（indicator matrix）。如果 DataFrame 的某一列中含有 k 个不同的值，则可以派生出一个 k 列矩阵或 DataFrame（其值全为1和0）。pandas 有一个 get_dummies 函数可以实现该功能：

```
In [89]: df = DataFrame({&#39;key&#39;:[&#39;b&#39;,&#39;b&#39;,&#39;a&#39;,&#39;c&#39;,&#39;a&#39;,&#39;b&#39;],
   ....:                 &#39;data1&#39;:range(6)})

In [90]: pd.get_dummies(df[&#39;key&#39;])
Out[90]: 
   a  b  c
0  0  1  0
1  0  1  0
2  1  0  0
3  0  0  1
4  1  0  0
5  0  1  0

In [91]: df
Out[91]: 
   data1 key
0      0   b
1      1   b
2      2   a
3      3   c
4      4   a
5      5   b

In [92]: # 我们可能想给指标 DataFrame 的列加上一个前缀，以便合并

In [93]: dummies = pd.get_dummies(df[&#39;key&#39;],prefix = &#39;key&#39;)

In [94]: df_with_dummy = df[[&#39;data1&#39;]].join(dummies)

In [95]: df_with_dummy
Out[95]: 
   data1  key_a  key_b  key_c
0      0      0      1      0
1      1      0      1      0
2      2      1      0      0
3      3      0      0      1
4      4      1      0      0
5      5      0      1      0
```

　　如果 DataFrame 中的某行同属于多个分类，则事情就会有点复杂。

同样的同学们，木木这里给大家一个数据文件，同学们把他们粘贴到文本文件中，等会会用到
```
1::Toy Story (1995)::Animation|Children&#39;s|Comedy
2::Jumanji (1995)::Adventure|Children&#39;s|Fantasy
3::Grumpier Old Men (1995)::Comedy|Romance
4::Waiting to Exhale (1995)::Comedy|Drama
5::Father of the Bride Part II (1995)::Comedy
6::Heat (1995)::Action|Crime|Thriller
7::Sabrina (1995)::Comedy|Romance
8::Tom and Huck (1995)::Adventure|Children&#39;s
9::Sudden Death (1995)::Action
10::GoldenEye (1995)::Action|Adventure|Thriller
11::American President, The (1995)::Comedy|Drama|Romance
12::Dracula: Dead and Loving It (1995)::Comedy|Horror
13::Balto (1995)::Animation|Children&#39;s
14::Nixon (1995)::Drama
15::Cutthroat Island (1995)::Action|Adventure|Romance
16::Casino (1995)::Drama|Thriller
17::Sense and Sensibility (1995)::Drama|Romance
18::Four Rooms (1995)::Thriller
19::Ace Ventura: When Nature Calls (1995)::Comedy
20::Money Train (1995)::Action
21::Get Shorty (1995)::Action|Comedy|Drama
22::Copycat (1995)::Crime|Drama|Thriller
23::Assassins (1995)::Thriller
24::Powder (1995)::Drama|Sci-Fi
25::Leaving Las Vegas (1995)::Drama|Romance
26::Othello (1995)::Drama
27::Now and Then (1995)::Drama
28::Persuasion (1995)::Romance
29::City of Lost Children, The (1995)::Adventure|Sci-Fi
30::Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)::Drama
```
示例如下：

```
In [103]: mnames = [&#39;movie_id&#39;,&#39;title&#39;,&#39;genres&#39;]

In [104]: movies = pd.read_table(&#39;movies.dat&#39;, sep=&#39;::&#39;,header=None,
   .....:                        names = mnames)


In [105]: movies[:10]
Out[105]: 
   movie_id                               title                        genres
0         1                    Toy Story (1995)   Animation|Children&#39;s|Comedy
1         2                      Jumanji (1995)  Adventure|Children&#39;s|Fantasy
2         3             Grumpier Old Men (1995)                Comedy|Romance
3         4            Waiting to Exhale (1995)                  Comedy|Drama
4         5  Father of the Bride Part II (1995)                        Comedy
5         6                         Heat (1995)         Action|Crime|Thriller
6         7                      Sabrina (1995)                Comedy|Romance
7         8                 Tom and Huck (1995)          Adventure|Children&#39;s
8         9                 Sudden Death (1995)                        Action
9        10                    GoldenEye (1995)     Action|Adventure|Thriller
```

　　要为每一个 genre 添加指标变量就需要做一些数据规整操作。首先，我们从数据集中抽取出不同的 genre 值（注意巧用 set.union）：

```
In [106]: genre_iter = (set(x.split(&#39;|&#39;)) for x in movies.genres)

In [107]: genres = sorted(set.union(*genre_iter))

In [108]: # 现在我们从一个全零 DataFrame 开始构建指标 DataFrame


In [109]: dummies = DataFrame(np.zeros((len(movies),len(genres))),
   .....:                               columns = genres)

In [110]: # 接下来，迭代每一部电影并将 dummies 各行的项设置为1


In [111]: for i,gen in enumerate(movies.genres):
   .....:     dummies.ix[i, gen.split(&#39;|&#39;)]=1
   .....:     

In [112]: # 在将其与 movie是合并起来

In [113]:  movies_windic = movies.join(dummies.add_prefix(&#39;Genre_&#39;))

In [114]: movies_windic.ix[0]
Out[114]: 
movie_id                                      1
title                          Toy Story (1995)
genres              Animation|Children&#39;s|Comedy
Genre_Action                                  0
Genre_Adventure                               0
Genre_Animation                               1
Genre_Children&#39;s                              1
Genre_Comedy                                  1
Genre_Crime                                   0
Genre_Drama                                   0
Genre_Fantasy                                 0
Genre_Horror                                  0
Genre_Romance                                 0
Genre_Sci-Fi                                  0
Genre_Thriller                                0
Name: 0, dtype: object

```
　　一个对统计应用有用的秘诀是：结合 get_dummies 和诸如 cut 之类的离散化函数

```
In [127]: values = np.random.rand(10)

In [128]: values
Out[128]: 
array([ 0.65158907,  0.61562033,  0.53089883,  0.13941997,  0.48344916,
        0.11734259,  0.07557764,  0.21863608,  0.90953758,  0.38441329])

In [129]: bins = [0,0.2,0.4,0.6,0.8,1]

In [130]: pd.get_dummies(pd.cut(values,bins))
Out[130]: 
   (0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1]
0         0           0           0           1         0
1         0           0           0           1         0
2         0           0           1           0         0
3         1           0           0           0         0
4         0           0           1           0         0
5         1           0           0           0         0
6         1           0           0           0         0
7         0           1           0           0         0
8         0           0           0           0         1
9         0           1           0           0         0


```

## 十、作业##

　　今天的内容有点多，希望同学们好好消化一下,将上面的代码自己输入一遍。