**Hadoop2.X 64位环境搭建**
---------------



**1	搭建环境**

部署节点操作系统为CentOS，防火墙和SElinux禁用，创建了一个shiyanlou用户并在系统根目录下创建/app目录，用于存放Hadoop等组件运行包。因为该目录用于安装hadoop等组件程序，用户对shiyanlou必须赋予rwx权限（一般做法是root用户在根目录下创建/app目录，并修改该目录拥有者为shiyanlou(chown –R shiyanlou:shiyanlou /app）。
Hadoop搭建环境：
- 虚拟机操作系统： CentOS6.6  64位，单核，1G内存
- JDK：1.7.0_55 64位
- Hadoop：2.2.0 64位（该部署包为第2个实验所编译完成）

**2	部署Hadooop2.X**

**2.1	配置Hadoop环境**

在Apache网站上提供Hadoop2.X安装包只支持32位操作系统安装，在64位服务器安装会出现3.1的错误异常。这里我们使用上一步骤编译好的hadoop-2.2.0-bin.tar.gz文件作为安装包（也可以在/home/shiyanlou/install-pack目录中找到hadoop-2.2.0.tar.gz安装包）

**2.1.1	下载并解压hadoop安装包**

解压缩并移动到/app目录下
- cd /home/shiyanlou/install-pack
- tar -xzf hadoop-2.2.0.tar.gz
- mv hadoop-2.2.0 /app

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299112815?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.1.2	在Hadoop目录下创建子目录**

在hadoop-2.2.0目录下创建tmp、name和data目录
- cd /app/hadoop-2.2.0
- mkdir tmp
- mkdir hdfs
- mkdir hdfs/name
- mkdir hdfs/data

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299186597?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.1.3	配置hadoop-env.sh**

1.打开配置文件hadoop-env.sh
- cd /app/hadoop-2.2.0/etc/hadoop
- sudo vi hadoop-env.sh

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299205857?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
2.加入配置内容，设置了hadoop中jdk和hadoop/bin路径
- export HADOOP_CONF_DIR=/app/hadoop2.2.0/etc/hadoop
- export JAVA_HOME=/app/lib/jdk1.7.0_55
- export PATH=$PATH:/app/hadoop-2.2.0/bin

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299222257?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
3.编译配置文件hadoop-env.sh，并确认生效
- source hadoop-env.sh
- hadoop version

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299233771?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.1.4	配置yarn-env.sh**

打开配置文件yarn-env.sh，设置了hadoop中jdk路径，配置完毕后使用source yarn-env.sh编译该文件
- export JAVA_HOME=/app/lib/jdk1.7.0_55

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299255842?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.1.5	配置core-site.xml**

1.使用如下命令打开core-site.xml配置文件
- cd /app/hadoop-2.2.0/etc/hadoop
- sudo vi core-site.xml

2.在配置文件中，按照如下内容进行配置
- &lt;configuration&gt;
-   &lt;property&gt;
-     &lt;name&gt;fs.default.name&lt;/name&gt;
-     &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;fs.defaultFS&lt;/name&gt;
-     &lt;value&gt;hdfs://hadoop:9000&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;io.file.buffer.size&lt;/name&gt;
-     &lt;value&gt;131072&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
-     &lt;value&gt;file:/app/hadoop-2.2.0/tmp&lt;/value&gt;
-     &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;hadoop.proxyuser.hduser.hosts&lt;/name&gt;
-     &lt;value&gt;*&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt;
-     &lt;value&gt;*&lt;/value&gt;
-   &lt;/property&gt;
- &lt;/configuration&gt;

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299287447?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.1.6	配置hdfs-site.xml**

1.使用如下命令打开hdfs-site.xml配置文件
- cd /app/hadoop-2.2.0/etc/hadoop
- sudo vi hdfs-site.xml

2.在配置文件中，按照如下内容进行配置
- &lt;configuration&gt;
-   &lt;property&gt;
-    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
-    &lt;value&gt;hadoop:9001&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
-    &lt;value&gt;file:/app/hadoop-2.2.0/hdfs/name&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
-    &lt;value&gt;file:/app/hadoop-2.2.0/hdfs/data&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-    &lt;name&gt;dfs.replication&lt;/name&gt;
-    &lt;value&gt;1&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-    &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
-    &lt;value&gt;true&lt;/value&gt;
-   &lt;/property&gt;
- &lt;/configuration&gt;

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299309438?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.1.7	配置mapred-site.xml**

1.默认情况下不存在mapred-site.xml文件，可以从模板拷贝一份，并使用如下命令打开mapred-site.xml配置文件
- cd /app/hadoop-2.2.0/etc/hadoop 
- cp mapred-site.xml.template mapred-site.xml 
- sudo vi mapred-site.xml

2.在配置文件中，按照如下内容进行配置
- &lt;configuration&gt;
-   &lt;property&gt;
-     &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
-     &lt;value&gt;yarn&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
-     &lt;value&gt;hadoop:10020&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
-     &lt;value&gt;hadoop:19888&lt;/value&gt;
-   &lt;/property&gt;
- &lt;/configuration&gt;

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299325315?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.1.8	配置yarn-site.xml**

1.使用如下命令打开yarn-site.xml配置文件
- cd /app/hadoop-2.2.0/etc/hadoop
- sudo vi yarn-site.xml
 
2.在配置文件中，按照如下内容进行配置
- &lt;configuration&gt;
-   &lt;property&gt;
-     &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
-     &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
-     &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
-     &lt;value&gt;hadoop:8032&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
-     &lt;value&gt;hadoop:8030&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
-     &lt;value&gt;hadoop:8031&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
-     &lt;value&gt;hadoop:8033&lt;/value&gt;
-   &lt;/property&gt;
-   &lt;property&gt;
-     &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
-     &lt;value&gt;hadoop:8088&lt;/value&gt;
-   &lt;/property&gt;
- &lt;/configuration&gt;

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299450574?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.1.9	配置slaves文件**

在slaves配置文件中设置从节点，这里设置为hadoop，与Hadoop1.X区别的是Hadoop2.X不需要设置Master
- cd /app/hadoop-2.2.0/etc/hadoop
- vi slaves

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299471414?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 

**2.1.10	格式化namenode**

- cd /app/hadoop-2.2.0/bin
- ./hdfs namenode -format
 
![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299491126?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299517803?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.2	启动Hadoop**

**2.2.1	启动hdfs**

- cd /app/hadoop-2.2.0/sbin
- ./start-dfs.sh

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299545063?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.2.2	验证当前进行**

使用jps命令查看运行进程，此时在hadoop上面运行的进程有：namenode、secondarynamenode和datanode三个进行
![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299562771?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 

**2.2.3	启动yarn**

- cd /app/hadoop-2.2.0/sbin
- ./start-yarn.sh

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299582263?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10) 

**2.2.4	验证当前进行**

使用jps命令查看运行进程，此时在hadoop上运行的进程除了：namenode、secondarynamenode和datanode，增加了resourcemanager和nodemanager两个进程：
![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299596249?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.3	测试Hadoop**

**2.3.1	创建测试目录**

- cd /app/hadoop-2.2.0/bin
- ./hadoop fs -mkdir -p /class3/input
 
![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299615151?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)

**2.3.2	准备测试数据**

- ./hadoop fs -copyFromLocal ../etc/hadoop/* /class3/input

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299631467?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.3.3	运行wordcount例子**
- cd /app/hadoop-2.2.0/bin
- ./hadoop jar ../share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount /class3/input /class3/output

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299676697?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
 ![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299687818?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**2.3.4	查看结果**

使用如下命令查看运行结果：
- ./hadoop fs -ls /class3/output/  
- ./hadoop fs -cat /class3/output/part-r-00000 | less

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299702625?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
**3	问题解决**

**3.1	CentOS 64bit安装Hadoop2.2.0中出现文件编译位数异常**

在安装hadoop2.2.0过程中出现如下异常：Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299717993?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
通过分析是由于lib/native目录中有些文件是在32位编译，无法适应CentOS 64位环境造成

![图片描述信息](https://dn-anything-about-doc.qbox.me/userid29778labid1029time1433299731479?watermark/1/image/aHR0cDovL3N5bC1zdGF0aWMucWluaXVkbi5jb20vaW1nL3dhdGVybWFyay5wbmc=/dissolve/60/gravity/SouthEast/dx/0/dy/10)
 
有两种办法解决：
- 重新编译hadoop，然后重新部署
- 暂时办法是修改配置，忽略有问题的文件
 


