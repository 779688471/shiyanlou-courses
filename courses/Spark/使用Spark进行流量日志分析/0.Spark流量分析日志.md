# 流量日志分析

## 一、实验介绍

### 1.1 实验内容

日志在计算机系统中是一个非常广泛的概念，任何程序都有可能输出日志：操作系统内核、各种应用服务器等等。日志包含很多有用的信息，例如访问者的 IP、访问的时间、访问的目标网页、来源的地址以及访问者所使用的客户端的 UserAgent 信息等，分析日志能帮助企业营销做出决策，本节课将介绍如何用 Spark 分析日志。

### 1.2 先学课程

[Spark 系列课程](https://www.shiyanlou.com/search?search=spark)

### 1.3 实验知识点

- 二次排序
- 序列化
- Spark RDD

### 1.4 实验环境

- spark-1.5.1-bin-hadoop2.6
- Xfce 终端

### 1.5 适合人群

本课程属于中级难度级别，适合具有 Spark 基础的用户，如果对 Scala 熟悉 ，能够更好的上手本课程。

## 二、实验步骤

### 2.1　日志格式简介

随着互联网的发展，产生了大量的 Web 日志，日志包含了用户最重要的信息，通过日志分析，用户可以获取到网站或应用的访问量，哪个网页访问人数最多，哪个网页有价值等。一般中型的网站 (10 万的 PV), 每天会产生 1GB 以上的 Web 日志文件，大型的网站，可能每天产生 500G 及以上的数据量。常见的日志格式主要有两类：一种是 Apache 的 NCSA 格式，另一种是 IIS 的 W3C 格式。对于日志的这种规模的数据，通过 Spark 进行分析与处理，能够达到很好的效果。

一种 Nginx 日志格式举例。

`注意:这是一条数据，即一行数据，包含29个字段。`

```
1368607178148    1368607184454    10.80.102.29    10.80.102.29    49273                    GET    113.31.42.4    80    360安全卫士    工具软件    安全杀毒    http://cdn.weather.hao.360.cn/sed_api_weather_info.php?code=101290101&v=2&param=weather&app=hao360&_jsonp=__jsonp1__&t=2281012    cdn.weather.hao.360.cn    360安全中心    互联网    信息安全    奇虎360    4    3    909    192    200    Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)    2    1368607178245

```

日志各字段说明如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493965844380.png/wm)

部分分析日志指标如下：

`PV（Page View）`: 网站页面访问数。

`UV（Page View）`: 页面 IP 的访问量统计，即独立 IP。

`PVPU（Page View Per User）`: 平均每位用户访问页面数。

`Time`: 用户每小时 PV 的统计。

`Source`: 用户来源域名的统计。

`Browser`: 用户的访问设备统计。

`.....`

如上图所示，数据字段太多，我们抽取黄色的字段，汇总如下，您可以学习 [linux 操作工具进行过滤](https://www.shiyanlou.com/courses/68)，限于篇幅，在此不作介绍。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493966981687.png/wm)

过滤后的部分数据如下 (选取其中一条仅作示例)：

```
1363157986072     18320173382    84-25-DB-4F-10-1A:CMCC-EASY    120.196.100.99    input.shouji.sogou.com    搜索引擎    21    18    9531    2412    200

```

继续抽取上图红色框里的数据，作为本节课实验数据，各字段之间用 `\t`分隔符分开，最终数据如下所示：

```
1493901764492    13326854564    16193    4743
1493910704492    13426854534    73434    2809
1493888804492    13626854565    81858    15477
1493909204492    13726854561    30144    3965
1493900564492    13926854564    79050    56108
1493887604492    13826854534    96411    4914
1493884124492    13726854569    37707    55218
1493909504493    13226854545    98537    8529
1493909744493    13526854569    44872    99132
1493887364493    13626854545    88181    57680
1493898884493    13526854555    37478    7935
1493899784493    13532434342    27011    88622
1493884064493    13776854556    69130    14111
1493909444493    13426854544    9424    7944
1493881124493    13726455555    60670    9896
1493888144495    13026854539    90589    20091
1493878244495    13245546774    10157    70072
1493890424495    13685456946    54542    92281
1493886644496    13656853434    78754    6029
1493898584496    13026832435    28938    57137
1493895164496    13026854539    13804    45769
1493883644496    13026324344    64352    8799
1493896064496    13368545391    96531    57707
1493883464496    13026855696    50270    51114
1493876624496    13435545456    42725    7474
1493895764496    13434354354    1292    44650
1493906624496    13324234355    36891    66919
1493881244496    13554654765    33422    3859
1493894684496    13734546757    18356    95838
1493894564496    13335435466    40851    54767
1493900864496    13432436546    52265    9739
1493907224497    13943354354    83598    77744
1493895044497    13677878785    1493    71046
1493904044497    13826852343    8337    10934
1493881724497    13026854539    60948    1681
1493887964497    13626854564    52019    92273
1493901164497    13026854534    84997    36689
1493889164497    13232435435    99434    16073
1493888504497    13534543546    15049    2980
1493910284497    13434343444    37073    191
1493891024497    13789654643    6593    93537
1493880044497    13212432434    39299    49042
1493902484497    13226854569    41811    42073
1493890124498    13435435646    9565    69941
1493905724498    14324354354    69444    72664
1493899724498    13743543546    82207    11035
1493904884498    13726854523    34207    32098
1493885624498    13324325434    12683    2862
1493909864498    13026854539    11500    9944
1493893484498    13776854556    77444    5137

```

其中第一列为`reportTime`（报告时间戳），

其中第二列为`phoneNum`（手机号，上图中为 msisdn，这里修改为 phoneNum, 方便识别），

其中第三列为`upPayLoad`（上行流量），

其中第四列为`downPayLoad`（下行流量）。

为了操作方便，数据集已上传[实验楼](http://labfile.oss.aliyuncs.com/courses/825/access_20170504.log)，您可以使用以下命令下载。

```
cd /opt/
sudo  wget http://labfile.oss.aliyuncs.com/courses/825/access_20170504.log

```

### 2.2　需求分析

需求描述：对文本中数据记录进行排序，排序规则如下：

以`phoneNum`为基准，分别按照 `upPayLoad`，`downPayLoad`，`reportTime`进行降序排序，即先按照`upPayLoad`排序，如果`upPayLoad`相同，再比较`downPayLoad`，如果`downPayLoad`相同，再比较`reportTime`，最后选择前 6 条记录输出。

`核心编程思想描述:`

所谓二次排序就是排序的时候，考虑两个维度。有一个纬度相同的时候，考虑另外一个维度。首先，将需要将待排序的字段封装成一个类，该实现了 Serializable 接口，实现接口中的方法。同时为待排序的属性字段提供 getter 、setter 、hashcode 以及 equals 方法。在 application 应用程序中 将 key 封装为之前我们定义好的对象，之后调用 sortedByKey() 方法进行排序。

`具体实现步骤:`

第一步：按照 Serrializable 接口实现自定义排序的 Key

第二步：将要进行二次排序的文件加载进来生成 key-value 类型的 RDD

第三步：使用 sortByKey 基于自定义的 Key 进行二次排序

第四步：去除掉排序的 key，只保留排序结果

### 2.3　准备工作

#### 1). 环境准备

双击打开桌面上的 Xfce 终端，用 `sudo` 命令切换到 hadoop 用户，hadoop 用户密码为 hadoop，用 `cd` 命令进入 `/opt` 目录，因为用的本地的环境，可以不用启动任何进程也可以完成本节实验。

```
$ su hadoop
$ cd /opt/
$ jps

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493964415737.png/wm)

#### 2 准备数据

```
$ sudo vi access_20170504.log
#添加上述数据，保存
$ more access_20170504.log

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493969811974.png/wm)

### 2.3 代码实现

#### 1). 创建 maven 项目

双击桌面上的图标打开 Scala IDE -> OK。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493888495343.png/wm)

再次点击 Ok。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493888582779.png/wm)

点击 File -> New ->Other

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493888945968.png/wm)

搜索 maven -> 点击 Maven Project -> Next

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493889065804.png/wm)

继续点击 Next

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493889121262.png/wm)

选中 "quickstart" -> Next

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493889182728.png/wm)

输入 "Group Id"， "Artifact Id"， "Package" -> Finish 。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493889282340.png/wm)

点击 spark 项目 -> 修改 pom.xml -> 保存后会自动下载 jar 包

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2875timestamp1493193193534.png/wm)

```
#添加如下内容并保存
<dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.10</artifactId>
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_2.10</artifactId>
      <version>1.5.1</version>
      </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_2.10</artifactId>
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_2.10</artifactId>
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>2.6.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming-kafka_2.10</artifactId>    
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming-flume_2.10</artifactId>    
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.httpcomponents</groupId>
      <artifactId>httpclient</artifactId>
      <version>4.4.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.httpcomponents</groupId>
      <artifactId>httpcore</artifactId>
      <version>4.4.1</version>
    </dependency>

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2875timestamp1493214139767.png/wm)

```
<build>
    <sourceDirectory>src/main/java</sourceDirectory>
    <testSourceDirectory>src/main/test</testSourceDirectory>

    <plugins>
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
          <archive>
            <manifest>
              <mainClass></mainClass>
            </manifest>
          </archive>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <version>1.2.1</version>
        <executions>
          <execution>
            <goals>
              <goal>exec</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <executable>java</executable>
          <includeProjectDependencies>true</includeProjectDependencies>
          <includePluginDependencies>false</includePluginDependencies>
          <classpathScope>compile</classpathScope>
          <mainClass>cn.com.syl.spark.App</mainClass>
        </configuration>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <source>1.6</source>
          <target>1.6</target>
        </configuration>
      </plugin>

    </plugins>
  </build>

```

最终 `pom.xml` 如下：

```
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>


  <groupId>cn.com.syl</groupId>
  <artifactId>spark</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <packaging>jar</packaging>

  <name>spark</name>
  <url>http://maven.apache.org</url>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
  </properties>

  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.10</artifactId>
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_2.10</artifactId>
      <version>1.5.1</version>
      </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_2.10</artifactId>
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_2.10</artifactId>
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>2.6.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming-kafka_2.10</artifactId>    
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming-flume_2.10</artifactId>    
      <version>1.5.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.httpcomponents</groupId>
      <artifactId>httpclient</artifactId>
      <version>4.4.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.httpcomponents</groupId>
      <artifactId>httpcore</artifactId>
      <version>4.4.1</version>
    </dependency>
  </dependencies>

  <build>
    <sourceDirectory>src/main/java</sourceDirectory>
    <testSourceDirectory>src/main/test</testSourceDirectory>

    <plugins>
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
          <archive>
            <manifest>
              <mainClass></mainClass>
            </manifest>
          </archive>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <version>1.2.1</version>
        <executions>
          <execution>
            <goals>
              <goal>exec</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <executable>java</executable>
          <includeProjectDependencies>true</includeProjectDependencies>
          <includePluginDependencies>false</includePluginDependencies>
          <classpathScope>compile</classpathScope>
          <mainClass>cn.com.syl.App</mainClass>
        </configuration>
      </plugin>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <source>1.6</source>
          <target>1.6</target>
        </configuration>
      </plugin>

    </plugins>
  </build>
</project>

```

`注意：文件下载的依赖项较多，需要一定的时间，请耐心等待完成。`

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2875timestamp1493211273210.png/wm)

可能会出现的问题一：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2875timestamp1493197687865.png/wm)

解决办法：

选中 "Project configuration ..."-> Quick Fix -> Finish

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2875timestamp1493197810479.png/wm)

可能会出现的问题二：

项目上出现小红叉。

解决办法：

项目上右键 ->maven ->update project，或者使用快捷键 `alt+F5`。

#### 2). 编写代码

选中 cn.com.syl.spark 包 -> New -> Other

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493971096764.png/wm)

搜索 "class"-> 选中 java class -> Next

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493971219308.png/wm)

输入类名 "SecondarySortApp" -> Finish

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493971358479.png/wm)

```
package cn.com.syl.spark;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;

import scala.Tuple2;

public class SecondarySortApp {

    public static void main(String[] args) throws Exception {
        //创建sparkcontext对象，sparkcontext是程序的唯一入口

        SparkConf conf = new SparkConf()
                .setAppName("SecondarySortApp")  
                .setMaster("local"); 
        JavaSparkContext sc = new JavaSparkContext(conf);
        //去掉不必要的输出信息
        sc.setLogLevel("WARN");

        // 调用textFile()方法，读取日志文件，这里指定本地磁盘文件，您也可以指定是HDFS上的文件

        JavaRDD<String> tf = sc.textFile(
                "/opt/access_20170504.log");   

        // 调用mapTfRDD2Pair方法 将tf映射为键值对
        JavaPairRDD<String, cmbInfo> tfPairRDD = 
                mapTfRDD2Pair(tf);

        // 获取每个手机号的总上行流量、总下行流量、最早报告时间戳
        JavaPairRDD<String, cmbInfo> agTfPairRDD = 
                aggregateByDeviceID(tfPairRDD);
        //聚合，封装的RDD作为key,手机号作为值
        JavaPairRDD<SecondarySortKey, String> tfSortRDD = 
                mapRDDKey2SortKey(agTfPairRDD);

        // 依次按照上行流量、下行流量以及报告时间戳倒序排序
        JavaPairRDD<SecondarySortKey ,String> sortedTfRDD =
                tfSortRDD.sortByKey(false);
        // 根据您的需要获得输出，这里仅显示前8行
        List<Tuple2<SecondarySortKey, String>> dtTop = 
                sortedTfRDD.take(8);
        System.out.println("============ result ============");
        for(Tuple2<SecondarySortKey, String> dt : dtTop) {
            System.out.println("phoneNum "+ dt._2 + ", " + dt._1);  
        }

        // 记住执行结束关闭资源
        sc.close();
    }

    //mapTfRDD2Pair方法，封装键值对
    private static JavaPairRDD<String, cmbInfo> mapTfRDD2Pair(
            JavaRDD<String> accessLogRDD) {
        return accessLogRDD.mapToPair(new PairFunction<String, String, cmbInfo>() {

            private static final long serialVersionUID = 1L;

            @Override
            public Tuple2<String, cmbInfo> call(String lines)
                    throws Exception {
                // 根据数据格式进行切分
                String[] split = lines.split("\t");  

                // 获取切分的字段
                long reportTime = Long.valueOf(split[0]);
                String phoneNum = split[1];
                long upPayLoad = Long.valueOf(split[2]);
                long downPayload = Long.valueOf(split[3]);  

                // 创建cmbInfo对象，有参构造 ，将上行流量、下行流量，报告时间戳封装为自定义的可序列化对象
                cmbInfo dataInfo = new cmbInfo(reportTime,
                        upPayLoad, downPayload);

                return new Tuple2<String, cmbInfo>(phoneNum, dataInfo);
            }

        });
    }

    //根据手机号进行聚合，依次按照上行流量、下行流量以及报告时间戳倒序排序
    private static JavaPairRDD<String, cmbInfo> aggregateByDeviceID(
            JavaPairRDD<String, cmbInfo> accessLogPairRDD) {
        return accessLogPairRDD.reduceByKey(new Function2<cmbInfo, cmbInfo, cmbInfo>() {

            private static final long serialVersionUID = 1L;

            @Override
            public cmbInfo call(cmbInfo d1, cmbInfo d2)
                    throws Exception {
                long reportTime = d1.getReportTime()< d2.getReportTime() ? 
                        d1.getReportTime() : d2.getReportTime();
                long upPayLoad = d1.getUpPayLoad() + d2.getUpPayLoad() ;
                long downPayload = d1.getDownPayload() + d2.getDownPayload();

                cmbInfo accessLogInfo = new cmbInfo();
                accessLogInfo.setReportTime(reportTime);
                accessLogInfo.setUpPayLoad(upPayLoad); 
                accessLogInfo.setDownPayload(downPayload);

                return accessLogInfo;
            }

        });
    }

    //二次排序，手机号作为值
    private static JavaPairRDD<SecondarySortKey, String> mapRDDKey2SortKey(
            JavaPairRDD<String, cmbInfo> aggrAccessLogPairRDD) {
        return aggrAccessLogPairRDD.mapToPair(

                new PairFunction<Tuple2<String,cmbInfo>, SecondarySortKey, String>() {

                    private static final long serialVersionUID = 1L;

                    @Override
                    public Tuple2<SecondarySortKey, String> call(
                            Tuple2<String, cmbInfo> tuple) throws Exception {
                        // 获取元祖里的数据
                        String phoneNum = tuple._1;
                        cmbInfo accessLogInfo = tuple._2;

                        // 封装为二次排序key 
                        SecondarySortKey accessLogSortKey = new SecondarySortKey(
                                accessLogInfo.getUpPayLoad(), 
                                accessLogInfo.getDownPayload(), 
                                accessLogInfo.getReportTime());

                        // 返回新的Tuple
                        return new Tuple2<SecondarySortKey, String>(accessLogSortKey, phoneNum);
                    }

                });
    }

}

```

选中 cn.com.syl.spark 包, 同理继续创建`SecondarySortKey.java`，代码如下：

`SecondarySortKey.java` 代码如下：

```
package cn.com.syl.spark;

import java.io.Serializable;
import scala.math.Ordered;

class cmbInfo implements Serializable {

    private static final long serialVersionUID = 5749943279909593929L;

    private long reportTime;        // 时间戳
    private long upPayLoad;        // 上行流量
    private long downPayload;    // 下行流量
    public cmbInfo() {
        super();
        // TODO Auto-generated constructor stub
    }
    public cmbInfo(long reportTime, long upPayLoad, long downPayload) {
        super();
        this.reportTime = reportTime;
        this.upPayLoad = upPayLoad;
        this.downPayload = downPayload;
    }
    public long getReportTime() {
        return reportTime;
    }
    public void setReportTime(long reportTime) {
        this.reportTime = reportTime;
    }
    public long getUpPayLoad() {
        return upPayLoad;
    }
    public void setUpPayLoad(long upPayLoad) {
        this.upPayLoad = upPayLoad;
    }
    public long getDownPayload() {
        return downPayload;
    }
    public void setDownPayload(long downPayload) {
        this.downPayload = downPayload;
    }
    public static long getSerialversionuid() {
        return serialVersionUID;
    }



}
public class SecondarySortKey implements Ordered<SecondarySortKey>, Serializable {

    private static final long serialVersionUID = 3702442700882342403L;

    private long upPayLoad;
    private long downPayLoad;
    private long reportTime;



    public SecondarySortKey() {
        super();
        // TODO Auto-generated constructor stub
    }


    public SecondarySortKey(long upPayLoad, long downPayLoad, long reportTime) {
        super();
        this.upPayLoad = upPayLoad;
        this.downPayLoad = downPayLoad;
        this.reportTime = reportTime;
    }


    public long getUpPayLoad() {
        return upPayLoad;
    }


    public void setUpPayLoad(long upPayLoad) {
        this.upPayLoad = upPayLoad;
    }


    public long getDownPayLoad() {
        return downPayLoad;
    }


    public void setDownPayLoad(long downPayLoad) {
        this.downPayLoad = downPayLoad;
    }


    public long getReportTime() {
        return reportTime;
    }


    public void setReportTime(long reportTime) {
        this.reportTime = reportTime;
    }


    public static long getSerialversionuid() {
        return serialVersionUID;
    }


    @Override
    public boolean $greater(SecondarySortKey other) {
        if(upPayLoad > other.upPayLoad) {
            return true;
        } else if(upPayLoad == other.upPayLoad && 
                downPayLoad > other.downPayLoad) {
            return true;
        } else if(upPayLoad == other.upPayLoad && 
                downPayLoad == other.downPayLoad &&
                        reportTime > other.reportTime) {
            return true;
        }
        return false;
    }

    @Override
    public boolean $greater$eq(SecondarySortKey other) {
        if($greater(other)) {
            return true;
        } else if(upPayLoad == other.upPayLoad && 
                downPayLoad == other.downPayLoad &&
                        reportTime == other.reportTime) {
            return true;
        }
        return false;
    }

    @Override
    public boolean $less(SecondarySortKey other) {
        if(upPayLoad < other.upPayLoad) {
            return true;
        } else if(upPayLoad == other.upPayLoad && 
                downPayLoad < other.downPayLoad) {
            return true;
        } else if(upPayLoad == other.upPayLoad && 
                downPayLoad == other.downPayLoad &&
                        reportTime < other.reportTime) {
            return true;
        }
        return false;
    }

    @Override
    public boolean $less$eq(SecondarySortKey other) {
        if($less(other)) {
            return true;
        } else if(upPayLoad == other.upPayLoad && 
                downPayLoad == other.downPayLoad &&
                        reportTime == other.reportTime) {
            return true;
        }
        return false;
    }

    @Override
    public int compare(SecondarySortKey other) {
        if(upPayLoad - other.upPayLoad != 0) {
            return (int) (upPayLoad - other.upPayLoad); 
        } else if(downPayLoad - other.downPayLoad != 0) {
            return (int) (downPayLoad - other.downPayLoad);
        } else if(reportTime - other.reportTime != 0) {
            return (int) (reportTime - other.reportTime);
        }
        return 0;
    }

    @Override
    public int compareTo(SecondarySortKey other) {
        if(upPayLoad - other.upPayLoad != 0) {
            return (int) (upPayLoad - other.upPayLoad); 
        } else if(downPayLoad - other.downPayLoad != 0) {
            return (int) (downPayLoad - other.downPayLoad);
        } else if(reportTime - other.reportTime != 0) {
            return (int) (reportTime - other.reportTime);
        }
        return 0;
    }


    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result + (int) (downPayLoad );
        result = prime * result + (int) (reportTime );
        result = prime * result + (int) (upPayLoad );
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        SecondarySortKey other = (SecondarySortKey) obj;
        if (downPayLoad != other.downPayLoad)
            return false;
        if (reportTime != other.reportTime)
            return false;
        if (upPayLoad != other.upPayLoad)
            return false;
        return true;
    }


    @Override
    public String toString() {
        return "SortResult： [上行流量： " + upPayLoad + ", 下行流量：  " + downPayLoad + ", 报告时间戳："
                + reportTime + "]";
    }


}

```

右键点击 SecondarySortApp -> Run as -> 1 Java Application

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493994181644.png/wm)

观察 console 控制台输出信息：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2895timestamp1493994223283.png/wm)

可以看出是符合我们的需求的，至此，流量日志分析完成。

## 三、实验总结

本节课简单介绍了日志的分类，然后从日志不断简化数据，最终只留下 4 个字段，然后以手机号标准，分别按照，上行流量，下行流量，报告时间戳进行倒序排序， 希望学完本节课，能帮助您理解学会运用 Spark 去处理复杂日志分析。

然后

## 四、参考阅读

- [http://blog.csdn.net/dwb1015/article/details/52207945](http://blog.csdn.net/dwb1015/article/details/52207945)
- [http://www.cnblogs.com/zlslch/p/5921572.html](http://www.cnblogs.com/zlslch/p/5921572.html)