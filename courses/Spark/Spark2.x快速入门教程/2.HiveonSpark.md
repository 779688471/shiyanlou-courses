# Hive on Spark

## 一、实验介绍

### 1.1 实验内容

Spark 2.0 是支持读写 hive 中存储的数据的，但是因为 hive 有较多的依赖，所以默认情况下，这些依赖没有包含在 spark 的发布包中。本节课将二者整合起来使用，过程稍微复杂，请耐心配置。

### 1.2 先学课程

HIVE 教程：[https://www.shiyanlou.com/courses/38](https://www.shiyanlou.com/courses/38)

### 1.3 实验知识点

- Hive/MySQL 安装
- Hive 整合 Spark
- Spark SQL 测试

### 1.4 实验环境

- hadoop-2.6.1
- spark-2.1.0-bin-hadoop2.6.tgz
- apache-hive-1.2.1-bin.tar.gz
- mysql-connector-java-5.0.8.tar.gz
- Xfce 终端

### 1.5 适合人群

本课程属于中等难度级别，适合具有大数据基础的用户，如果对 Hive /Spark SQL 了解能够更好的上手本课程。

## 二、实验步骤

### 2.1　准备工作

本节课是在上节课的基础上进行，请确保一下服务启动。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2837timestamp1492569982238.png/wm)

### 2.2 启动并配置 MySQL

#### 1). 实验楼环境默认已经安装 mysql ，启动即可。

```
$ sudo service mysql start
$ mysql -uroot -p # 无需输入密码,回车即可
$ exit

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492591283604.png/wm)

#### 2). 设置 mysql root 用户密码。

```
$ mysqladmin -uroot password "123456"
$ mysql -root -p123456 #用root密码登录验证是否成功

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492592848938.png/wm)

#### 3). 在 mysql 上创建 hive 元数据库，并对 hive 进行授权

```
$ create database if not exists hive_meta;
$ grant all privileges on hive_meta.* to 'root'@'%' identified by '123456';
$ grant all privileges on hive_meta.* to 'root'@'localhost' identified by '123456';
$ flush privileges;
$ use hive_meta;
$ exit

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492593442193.png/wm)

在 `/opt` 目录下下载 mysql-connector 并解压。

```
$ sudo wget http://labfile.oss.aliyuncs.com/courses/809/mysql-connector-java-5.0.8.tar.gz
$ sudo tar -zxvf mysql-connector-java-5.0.8.tar.gz

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492592953172.png/wm)

### 2.3 下载配置 Hive

#### 1). 下载解压 hive

```
#在/opt 目录下下载并解压
$ sudo wget http://labfile.oss.aliyuncs.com/courses/809/apache-hive-1.2.1-bin.tar.gz
$ sudo tar -zxvf apache-hive-1.2.1-bin.tar.gz

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492594984446.png/wm)

#### 2). 修改 hive-site.xml。

```
$ sudo mv apache-hive-1.2.1-bin hive
$ cd hive/conf
$ sudo cp hive-default.xml.template hive-site.xml
$ sudo vi hive-site.xml

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492595159370.png/wm)

查找以下选项进行修改。

- `javax.jdo.option.ConnectionURL`

  把`value`的值改为`jdbc:mysql://localhost:3306/hive_meta?createDatabaseIfNotExist=true`

- `javax.jdo.option.ConnectionDriverName`

  把`value`的值改为`com.mysql.jdbc.Driver`

- `javax.jdo.option.ConnectionUserName`

  把`value`的值改为`root`

- `javax.jdo.option.ConnectionPassword`

  把`value`的值改为`123456`

继续查找`hive.metastore.uris`

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492596305804.png/wm)

修改为

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492596409608.png/wm)

把`hive-site.xml`中所有`${system:java.io.tmpdir}`全部替换为 `/opt/hive/iotmp`

把`hive-site.xml`中所有`${system:user.name}`全部替换为 `root`

修改全部结束后，用`:wq`保存退出。

#### 3). 拷贝 mysql 驱动。

```
$ sudo cp /opt/mysql-connector-java-5.0.8/mysql-connector-java-5.0.8-bin.jar  /opt/hive/lib/

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492652619286.png/wm)

#### 4). 拷贝 jline-2.12.jar。

```
# 备份hadoop的jline-0.9.94.jar
$  mv /opt/hadoop-2.6.1/share/hadoop/yarn/lib/jline-0.9.94.jar  /opt/hadoop-2.6.1/share/hadoop/yarn/lib/jline-0.9.94.jar_bak
# 拷贝hive的jline-2.12.jar
$ cp /opt/hive/lib/jline-2.12.jar  /opt/hadoop-2.6.1/share/hadoop/yarn/lib/

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492653032696.png/wm)

#### 5). 复制 `hive-env.sh`。

```
$ sudo cp hive-env.sh.template  hive-env.sh

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492653449339.png/wm)

#### 6). 修改 `hive-config.sh`。

```
$ sudo vi ../bin/hive-config.sh

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492653565461.png/wm)

添加以下变量：

```
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HIVE_HOME=/opt/hive
export HADOOP_HOME=/opt/hadoop-2.6.1

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492654174663.png/wm)

#### 7). 将`hive-site.xml`放置到 spark 的`conf`目录下。

```
$ cp hive-site.xml  /opt/spark-2.1.0-bin-hadoop2.6/conf/

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492654460067.png/wm)

#### 8). 在 `/opt`目录下对 hive 授权。

```
$ sudo chmod 777 -R hive

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492656463530.png/wm)

#### 9). 启动 MySQL，查看 hive 元数据信息是否存在。

```
$ mysql -uroot -p123456
$ show  databases;
$ use hive_meta;
$ show tables;
$ exit

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492657319376.png/wm)

#### 10). 在 hive 的`bin`目录下启动 `hive`。

```
#启动metastore
$ ./hive --service metastore &

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid18510labid2850timestamp1493875929050.png/wm)

```
#启动hive
$ ./hive
$ show databases;
$ show tables;
$ exit

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492657939957.png/wm)

#### 10). 再次启动 MySQL，查看 hive 元数据信息是否存在。

```
$ mysql -uroot -p123456
$ use hive_meta;
$ show tables;
$ exit

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492658133445.png/wm)

以上步骤说明配置 hive 成功，hive 的元数据信息存在于 mysql 中。

## 三、hive on spark 测试

#### 1). 准备测试数据。

用 `vi` 创建一份数据 student.txt：

```
John,20,88
Marry,21,93
Pet,22,78
Tom,22,89
Judy,22,90
Andy,24,91

```

这里我已经创建并上传至 `hdfs` 文件系统，请您创建。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492667366892.png/wm)

#### 2). 打开 hive 客户端，创建表加载数据。

```
$ ./hive
$ create  table student(name string, age int, score double) row format delimited fields terminated by ',' stored as textfile;;

$ load data inpath 'hdfs://localhost:9000/testdata/student.txt' into table student;
#查看数据是否load进来
$select * from student;
$ exit;

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492667629964.png/wm)

#### 3). spark-shell 运行针对 hive 的 sql 语句, 验证 hive on spark 是否成功。

在 spark 的 `bin` 目录下启动 spark-shell。

```
$ cd spark-2.1.0-bin-hadoop2.6/bin
# a4cd888f4ca9 是主机名，要与您 hostnane 显示的名字一致
$ ./spark-shell --master spark://a4cd888f4ca9:7077  --executor-memory 600m --driver-memory 600m

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492668571745.png/wm)

执行 hive 的 sql 语句。

```
spark.sql("select * from student").show();
spark.sql("select name from student where age=22").show();
spark.sql("select count(*) from student").show();

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492669246378.png/wm)

当然也可以在 Spark webui 中检查是否有运行的作业记录。

双击打开浏览器，输入：[localhost:4040](http://www.shiyanlou/) 可以看到当前已经完成的 `Jobs`。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492669876015.png/wm)

继续点击 `SQL`，可以看到已经完成的`SQL`查询操作，点击去会有更详细的执行过程等。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid214292labid2850timestamp1492670026065.png/wm)

## 四、实验总结

本节课详细的介绍了 Hive 与 Spark 的整合过程，涉及的知识点较多，比如 MySQL 的安装，MySQL 与 Hive 的打通，尤其`hive-site.xml`的配置项较多，一定要仔细配置，每一步都很关键，最终实现了 Hive on Spark，希望能帮助你，少走一些弯路。

## 五、参考阅读

- [http://blog.csdn.net/kimsungho/article/details/51584016](http://blog.csdn.net/kimsungho/article/details/51584016)
- [http://www.jianshu.com/p/a7f75b868568](http://www.jianshu.com/p/a7f75b868568)