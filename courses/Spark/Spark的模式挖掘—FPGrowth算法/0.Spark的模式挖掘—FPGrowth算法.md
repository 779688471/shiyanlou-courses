# Spark 的模式挖掘—FPGrowth 算法

## 一、实验介绍

### 1.1 内容介绍

模式挖掘也叫关联规则，其实就是从大量的数据中挖掘出比较有用的数据，挖掘频繁项。比如说超市有大量的购物数据，从而可以根据用户的购物数据找到哪些商品关联性比较大。也可以进行用户推荐。下面我们说一个真实的案例。

我想可能有些同学一定听说过尿布和啤酒的故事吧。就是在一家超市里尿布和啤酒摆在一起出售。但是这个奇怪的举措却使尿布和啤酒的销量双双增加了。这不是一个笑话，而是发生在美国沃尔玛连锁店超市的真实案例，并一直为商家所津津乐道。沃尔玛拥有世界上最大的数据仓库系统，为了能够准确了解顾客在其门店的购买习惯，沃尔玛对其顾客的购物行为进行购物篮分析，想知道顾客经常一起购买的商品有哪些。沃尔玛数据仓库里集中了其各门店的详细原始交易数据。在这些原始交易数据的基础上，沃尔玛利用数据挖掘方法对这些数据进行分析和挖掘。一个意外的发现是："跟尿布一起购买最多的商品竟是啤酒！经过大量实际调查和分析，揭示了一个隐藏在" 尿布与啤酒 " 背后的美国人的一种行为模式：在美国，一些年轻的父亲下班后经常要到超市去买婴儿尿布，而他们中有 30%～40% 的人同时也为自己买一些啤酒。产生这一现象的原因是：美国的太太们常叮嘱她们的丈夫下班后为小孩买尿布，而丈夫们在买尿布后又随手带回了他们喜欢的啤酒。该节内容选在百度百科。

沃尔玛就是根据自己超市里面的售物数据，通过数据挖掘，这里指的就是关联规则算法分析，从而从大量的数据中找到了啤酒和尿布是最搭配的，然后他们就将两者放在一起出售，取得了较好的经济效应。可想而知，数据挖掘在现实生活中还是挺有用的吧。

### 1.2 实验知识点

- Scala 基础编程
- RDD 的基本操作。Transformation 和 Actions
- Spark Mlib 的 FPGrowth 的算法应用

### 1.3 实验环境

- Ubuntu14.04
- Spark1.6.1
- Xfce 终端

### 1.4 适合人群

该项目难度一般，属于中等偏下难度，该项目适合有一定的 Spark 基础, 对 MLib 有一定的了解，并热爱机器学习。

## 二、实验原理

通过将数据转换成需要的数据类型，然后将数据使用 Mlib 算法进行模式挖掘。在该实验中的数据格式是 RDD[Array[String]], 当然在 Spark 中还有其他的数据类型，比如说 Vector，LabeledPoint，Matrix 等数据类型。

### 2.1 实验流程图

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493038917500.png/wm)

### 2.2 算法详解

关联规则用于表示数据内隐藏的关联性。比如说上面说到的故事。啤酒和尿布湿的关系，买尿布湿的消费者往往也会买啤酒。这就表明了数据之间的某种隐藏联系。但是分开看两者没有任何联系。关联规则算法发展到现在一共有三个算法：FP-Tree 算法、Apriori 算法、Eclat 算法，这三个算法我们主要是讲解 FP-Tree 算法。FP-Tree 是一种不产生候选模式而采用频繁模式增长的方法挖掘频繁模式的算法；此算法只扫描两次，第一次扫描数据是得到频繁项集，第二次扫描是利用支持度过滤掉非频繁项，同时生成 FP 树。然后后面的模式挖掘就是在这棵树上进行。此算法与 Apriori 算法最大不同的有两点：不产生候选集，只遍历两次数据，大大提升了效率。

下面我们就来讲讲该算法的实现过程：

假设有这么一批数据：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493351514164.png/wm)

第一次扫描数据得到频繁项集合，统计所有商品出现的次数：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493353297142.png/wm)

假设最小支持度为 3，那调整后的频繁项集合就是：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493353765798.png/wm)

第二次遍历数据构建频繁项集合树：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493355068723.png/wm)

FPTree 建立的规则就是树的根，为根节点，ROOT，定义为 NULL，然后将数据里面的每一条数据进行插入节点操作，每个节点包含一个名称，和一个次数的属性，例如上图的 a:4，a 就是 Item 中的名称，4 代表的是出现的次数。如果插入新事务时，树中已经包含该节点，这不用创建树节点，直接在节点上的次数加一就行。

### 2.3 实验拓展

当然这只是个简单的项目，想要拓展的话，可以加上前端效果，进行效果展示，同理该算法也可以应用到其他的场景上，比如说书本推荐，博客推荐系统等等。

## 三、项目实现

### 3.1 进入开发环境

进入到实验环境，双击桌面上的 Xfce，效果如下图

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493039475080.png/wm)

### 3.2 数据准备

下载 Groceries.csv 杂货店数据。在数据是在 CSDN 上面分享的免积分文件。下载地址为：[http://download.csdn.net/detail/sanqima/9301589](http://download.csdn.net/detail/sanqima/9301589) 数据我已经整理好上传到实验楼上了。Get 地址为：[http://labfile.oss.aliyuncs.com/courses/815/Groceries.txt](http://labfile.oss.aliyuncs.com/courses/815/Groceries.txt)

请输入以下代码获取数据集

```
wget http://labfile.oss.aliyuncs.com/courses/815/Groceries.txt

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493040670106.png/wm)

下载好数据后，我们看看里面的数据格式。在 Linux 中打开 csv 文件，查看该数据，如下图所示，这个 csv 文件包含 9835 条数据，下面我们就来介绍该数据文件，首先第一行 items 指的是标题栏是没有用的数据，其中 {} 中的数据，就是一次性购买的物品，每一行都有一个数字作为他的标签，在这里我们不将他作为行数标记，我们将他认为是用户 ID，我们也可以针对某一用户进行商品推荐。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid18510labid2862timestamp1493190683217.png/wm)

### 3.3 启动 spark shell

请在终端中输入如下代码：

```
spark-shell

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493039910041.png/wm)

进入到 spark 的 REPL 环境，相当于就是 spark 的 Console。

### 3.4 导入数据并转换数据

下面我们就进入到实验的关键位置，进行数据的导入，并进行必要的数据格式处理。将数据清洗成我们需要的数据格式。

#### 3.4.1 导入数据

拓展：Spark textFile 进行数据的导入，将外部数据导入到 Spark 中来，并将其转换成 RDD。

键入如下命令：

```
val data = sc.textFile("file:////home/shiyanlou/Groceries.txt")

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493042025359.png/wm)

从图中我们可以看到我们引入了 / home/shiyanlou 下面的 Groceries.txt 文件。在最后我们也了解到了当前的数据格式为 RDD[String] 类型的数据。

现在我们可以看看当前 data 的数据样式。我们查看 data 的前十行数据。

拓展：这里使用到 RDD Action 操作，take(num) 函数，take 的作用就是获取 RDD 中下标 0 到 num-1 的元素。foreach 遍历，并打印

键入命令：

```
data.take(10).foreach(println)

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493042171586.png/wm)

#### 3.4.2 去除无用数据

在之前我们就已经了解到，我们不需要该文件的第一行数据，他只是一个标签行，并没有真正的购物数据。

键入如下命令删除第一行数据：

拓展： RDD filter，filter 函数是应用于 RDD 的每一个元素，会过滤掉不符合条件的元素，返回新的 RDD，其实新的 RDD 的内容就是 filter 里面返回值为 True 的元素。

```
val dataNoHead = data.filter(line => !line.contains("items"))

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493042564462.png/wm)

现在我们查看 dataNoHead 的前五行，看是否已经去除了第一行数据。结果如下：

键入命令：

```
dataNoHead.take(5).foreach(println)

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493042971440.png/wm)

从图上可以看出，我们已经去除了第一行。

#### 3.4.3 获取购物数据

我们的目的是针对购物数据进行数据关联分析，在此用户 id 是没有用的数据，在这里我们选择将其抛弃，只获取我们需要的购物数据。具体操作如下：使用 rdd 的 map 函数，我们将用户 id 和购物商品分开，然后只取购物信息。

键入如下命令：

```
val dataS = dataNoHead.map(line => line.split("\\{"))

```

结果如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493043100448.png/wm)

从上图可以了解到，dataS 的数据格式为 RDD[Array[String]] 数据，接下来我们看看数据内容。如下图：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493043312016.png/wm)

然后我们获取购物数据就简单了，就是 dataS 数据里面的 Array 里面的第二个元素，并去除里面的 “}”“字符。

键入如下代码：

```
val dataGoods = dataS.map(s => s(1).replace("}\"",""))

```

实现效果如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493043390972.png/wm)

同理，我们可以看到 dataGoods 的数据格式是 RDD[String] 了，我们看看该 rdd 里面的数据样式。

键入命令和上面的一样，我相信通过这几步练习，已经有所了解了吧。效果如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493043458544.png/wm)

在这里，我们能够看到我们已经完全获取到了我们需要的购物数据。然后我们将这些数据以逗号分割开，用于 FPGrowth 的建模数据。

拆分数据，将其转换为建模数据的格式。需要的格式可以查看 FPGrowth 的 API 查看我们需要传入什么格式的数据，通过了解需要传入 RDD[Array[String]]。将购物数据按逗号拆分，转换成建模数据。并将其 cache 到内存中，方便我们多次使用该数据。

键入如下命令：

```
val fpData = dataGoods.map(_.split(",")).cache

```

结果如下图：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493043540918.png/wm)

同样，我们查看一下该 RDD 里面的内容。

键入如下命令：

```
fpData.take(5).foreach(line => line.foreach(print))

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493043670925.png/wm)

这样我们就将数据按照每一行，拆分成单独的商品，以 RDD[Array[String]] 的数据格式存在。到此我们整个的数据准备过程就结束了。

#### 3.4.4 FPGrowth 模型建立

我们先了解 FP 的三个基本概念。支持度与置信度与提升度。在这里主要介绍支持度和置信度两个概念。

支持度：比如说 A 对 B 的支持度，就是表示 AB 同时出现的事件除以总事件的概率。

置信度：比如说 A 对 B 的置信度，就是 AB 同时出现的事件除以 A 的总事件的概率。

提升度：同样 A 对 B，表示在包含 A 的条件下同时包含 B 的概率，除以不含 A 的条件下含有 B 的概率。

下面我们举一个列子。比如说有 1000 个顾客，有 400 人买了物品 A，400 人买了物品 B，有 200 人买了 AB 两个商品。

那么 A 的支持度为 (400+200)/1000

AB 的支持度为：200/1000

A 对 B 的置信度为: 200/400 (AB/A)

B 对 A 的置信度为：200/400 (AB/B)

接下来我们开始建立模型：

1. 引入 FPGrowth 包 键入命令：

```
import org.apache.spark.mllib.fpm.FPGrowth

```

实例化 FPGrowth 并且设置支持度为 0.05，不满足该支持度的数据将被去除和分区为 3。

输入如下命令：

```
val fpGroup = new FPGrowth().setMinSupport(0.05).setNumPartitions(3)

```

结果图如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493043963791.png/wm)

1. 开始创建模型 使用向前准备好的 fpData 数据，进行 FP 模型的建立。调用 FPGrowth 里面的 run 方法，进行模型的创建。 输入如下命令：

```
val fpModel = fpGroup.run(fpData)

```

创建模型后最后会有如下输出：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493044248296.png/wm)

获取满足支持度条件的频繁项集。

输入如下命令：

```
val freqItems = fpModel.freqItemsets.collect

```

查找频繁项，看哪些商品关联性高

打印频繁项内容。输入命令：

```
freqItems.foreach(f=>println("FrequentItem:"+f.items.mkString(",")+"OccurrenceFrequency:"+f.freq))
//FrequentItem:频繁项,OccurrenceFrequency:出现次数

```

结果如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493044845855.png/wm)

从中我们可以看出牛奶和酸奶酪还是比较配的，频繁项次数是 551 次，两个同时出现的次数。当然我只截取了一部分数据，这也是一部分小数据。随着数据的增加就会有更好的效果。

1. 进行用户商品推荐

前面我们看到用户 ID 为 3 的用户只购买了牛奶 (whole milk)，我们可以向他推荐什么商品呢。

定义一个用户 ID 为 3 的变量 (排序从 0 开始)。

命令如下:

```
val userID = 2

```

获取用户 3 购买的物品。

键入命令：

```
val usrList = fpData.take(3)( userID)

```

结果如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493045892471.png/wm)

我们可以看看 userList 的输出.

键入命令

```
usrList

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493045938724.png/wm)

我们获取了该用户的商品后，在频繁项中找出该商品出现的次数，用于后面计算商品之间的置信度。

定义一个变量用于存储该商品的次数。

输入命令：

```
var goodsFreq = 0L

```

查找频繁项集的次数

输入如下命令：

```
for(goods <- freqItems){
         if(goods.items.mkString == usrList.mkString){
        goodsFreq = goods.freq }}
println(GoodsNumber：" + goodsFreq)

```

结果如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493046189276.png/wm)

由于数据量的原因，在这里我们置信度设置为 0.1，当商品的置信度大于 0.1 这个阈值，我们就将其推荐给用户。在推荐过程中需要去除用户已经购买了的商品。

输入如下命令：

```
 for(f <- freqItems){
      if(f.items.mkString.contains(usrList.mkString) && f.items.size > usrList.size) {
        val conf:Double = f.freq.toDouble / goodsFreq.toDouble
        if(conf >= 0.1) {
          var item = f.items
          for (i <- 0 until usrList.size) {
            item = item.filter(_ != usrList(i)) 
          }
          for (str <- item) {
          println(str+"  ==="+conf)
          }  }   }   }

```

结果如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid85573labid2862timestamp1493046305324.png/wm)

红色框中是输入的代码，紫色的框中就是推荐的商品，后面就是其置信度的值。我们看看当 3 号用户买牛奶的时候会给我们推荐什么商品？

Yogurt: 酸乳酪 vegettables: 菜蔬 rolls/buns: 小面包或点心

看到推荐的这些商品，是不是感觉和牛奶很配呢。

到此我们的实验就结束了。当然也会存在推荐商品没有的情况，这种情况我们就可以将频繁项集里面的出现次数最高的几件商品推荐给用户。