# 航班数据分析实战

## 一、实验说明

> “我们很抱歉地通知您，您乘坐的由XX飞往XX的XXXX航班延误。”

相信很多在机场等待飞行的旅客都不愿意听到这句话。随着乘坐飞机这种交通方式的逐渐普及，航延延误问题也一直困扰着我们。航班延误通常会造成两种结果，一种是航班取消，一种是航班晚点。

在本课程中，我们将通过Spark提供的 DataFrame、 SQL 和机器学习框架等工具，结合数据可视化技术，对航班起降的记录数据进行分析，尝试找出造成航班延误的原因，以及对航班延误情况进行预测。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1471921867828.png/wm)

*图片来自pexels.com*

### 1.1 知识点

- Spark DataFrame操作
- Spark SQL常用操作
- Spark MLlib 机器学习框架使用

### 1.2 准备工作

#### 1.2.1 Spark应用基础

本课程需要你具有一定的Spark基础，以下为推荐在本课程之前需要学习的课程（已按先后顺序进行排列）：

- [Spark大数据动手实验](https://www.shiyanlou.com/courses/456)
- [Spark DataFrame入门](https://www.shiyanlou.com/courses/536)
- [Spark DataFrame详解](543)
- [Spark 讲堂之 SQL 入门](https://www.shiyanlou.com/courses/586)

#### 1.2.2 准备草稿纸

**在学习过程中，建议手边能够准备纸和笔做相应的记录。**代码写起来是非常快的，然而更重要的是如何通过思考去设计这些代码。因此我们会有大量的工作在书写伪代码和记录相关的字段上。

这也是数据分析工作中常见的一个习惯。

## 二、数据集简介及准备

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479742069627.png/wm)

*图片来自pexels.com*

### 2.1 数据集简介

本节实验用到的航班数据集仍然是 [2009 年 Data Expo 上提供的飞行准点率统计数据](http://stat-computing.org/dataexpo/2009/the-data.html)。

此次我们选用1998年的数据集。你可以通过[官方下载链接](http://stat-computing.org/dataexpo/2009/1998.csv.bz2)来下载，也可以获取实验楼为你提供的副本。

&gt; 如果你是在自己的Spark集群上进行学习，则可以选用 2007、2008 年等年份的数据集。它们含有的航班数量更多，能够得到更多有趣的信息。

该数据集的各个字段解释如下：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479780876873.png/wm)

此外，我们还会用到一些补充信息。如机场信息数据集等。


### 2.2 下载数据集

双击打开桌面上的 Xfce 终端，然后输入下面的命令以下载航班数据集：

```
wget http://labfile.oss-cn-hangzhou.aliyuncs.com/courses/610/1998.csv.bz2
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1471917110086.png/wm)

然后使用解压缩命令对其进行解压：

```
bunzip2 1998.csv.bz2
```

解压后的 CSV 数据文件位于你使用解压命令时的工作目录中，默认情况是在 `/home/shiyanlou` 目录中。

同样地，下载 airports 机场信息数据集，命令如下所示。

```
wget http://labfile.oss.aliyuncs.com/courses/610/airports.csv
```

### 2.3 数据清洗

由于 airports 数据集中含有一些非常用字符，我们需要对其进行清洗处理，以防止部分记录字符的不能被识别错误引起后续检索的错误。

在终端中输入命令 `refine` 来启动 OpenRefine。 OpenRefine 是 Google 主导开发的一款开源数据清洗工具。启动命令如下。

```
refine
```

当出现下图所示的提示信息后，在浏览器中打开 URL `http://127.0.0.1:3333/`。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479738572679.png/wm)

浏览器中会出现 OpenRefine 的应用网页，如下图所示。请选择刚刚下载的机场信息数据集，并点击 Next 按钮进入下一步。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479738721040.png/wm)

在数据解析步骤中，直接点击右上角的 `Create Project` 按钮创建数据清洗项目。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479738779043.png/wm)

稍作等待，项目创建完成后，就可以对数据进行各种操作。实验楼在稍后会提供 OpenRefine 的详细教程，此处只需要按照提示对数据集进行相应操作即可。

点击 airport 列旁边的下拉菜单按钮，然后在菜单中选择 Edit Column -&gt; Remove this column 选项，以移除 airport 列。具体操作如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479739059383.png/wm)

请按照同样的方法，移除 lat 和 long 列。最后的数据集应只包含 iata 、city、state、country 四列。

最后我们点击右上角的 Export 按钮导出数据集。导出选项选择 Comma-separated value，即 CSV 文件。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479739309230.png/wm)

然后在弹出的下载提示对话框中选择“保存文件”，并确定。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479739350131.png/wm)

该文件位于 `/home/shiyanlou/下载` 目录中，请在文件管理器中将其剪切至 `/home/shiyanlou` 目录，并覆盖源文件。步骤如下图所示。

首先双击打开桌面上的 `主文件夹`，找到其中的 `下载` 目录。右键点击 CSV 文件，选择剪切。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479739501214.png/wm)

然后回到主目录，在空白处右键点击，选择“粘贴”即可。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479739610812.png/wm)

最后关闭浏览器和运行着 OpenRefine 的终端即可。

### 2.4 启动 Spark Shell

为了更好地处理 CSV 格式的数据集，我们可以直接使用由 DataBricks 公司提供的第三方 Spark CSV 解析库来读取。

首先是启动 Spark Shell。在启动的同时，附上参数`--packages com.databricks:spark-csv_2.11:1.1.0`

&gt; 请在终端中输入以下代码。

```
spark-shell --packages com.databricks:spark-csv_2.11:1.1.0
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479716651675.png/wm)

&gt; **注意：**该操作需要联网权限。如果遇到网络访问较慢，或者是您当前不具备访问互联网的权限时，请参考文末的常见问题“无法访问外网时，应如何通过加载CSV解析库的方式进入Spark Shell”，问题解答中提供了解决方案。

### 2.5 导入数据及处理格式

等待 Spark Shell 启动完成后，输入以下命令来导入数据集。

```
val flightData = sqlContext.read.format(&#34;com.databricks.spark.csv&#34;).option(&#34;header&#34;,&#34;true&#34;).load(&#34;/home/shiyanlou/1998.csv&#34;)
```

在上述命令中，我们调用了 sqlContext 提供的 read 接口，指定加载格式 format 为第三方库中定义的格式 `com.databricks.spark.csv` 。同时设置了一个读取选项 header 为 `true`，这表示将数据集中的首行内容解析为字段名称。最后在 load 方法中
指明了待读取的数据集文件为我们刚刚下载的这个数据集。

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479635455893.png/wm)

此时， `flightData` 的数据类型为 Spark SQL 中常用的 DataFrame。 

接着将 flightData 其注册为临时表，命令为：

```
flightData.registerTempTable(&#34;flights&#34;)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479643416938.png/wm)

使用相同的方法导入机场信息数据集 airports.csv ，并将其注册为临时表。

```
val airportData = sqlContext.read.format(&#34;com.databricks.spark.csv&#34;).option(&#34;header&#34;,&#34;true&#34;).load(&#34;/home/shiyanlou/airports-csv.csv&#34;)
airportData.registerTempTable(&#34;airports&#34;)
```

稍后我们将基于这些临时表来做一些 SQL 查询。

## 三、问题设计

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479733741763.png/wm)

*（图片来自airlines.net）*

在探索数据之前，我们已经知道该数据共有29个字段。根据出发时间、出发/抵达延误时间等信息，我们可以大胆地提出下面这些问题：

- **每天航班最繁忙的时间段是哪些？**通常早晚都容易有大雾等极端天气，是否中午的时候到港和离港航班更多呢？
- **飞哪最准时？**在设计旅行方案时，如果达到某个目的地有两个相邻的机场，我们似乎可以比较到哪里更准时，以减少可能发生的延误给我们出行带来的影响。
- **出发延误的重灾区都有哪些？**同样，从哪些地方出发最容易遭到延误？下次再要从这些地方出发的时候，就要考虑是不是要改乘地面交通工具了。

请用一张纸记录下上述问题。如果你还有自己想要探索的问题，也请记录下来，稍后尝试自己完成。

## 四、问题解答

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479741445144.png/wm)

*图片来自pexels.com*

我们已经把数据注册为临时表，对于上述问题的解答实际上就变成了如何设计合适的 SQL 查询语句。在数据量非常大的时候，Spark SQL 的使用尤为方便，它能够直接从 HDFS 等分布式存储系统中拉取数据进行查询，并且能够最大化地利用集群性能进行查询计算。

### 4.1 每天航班最繁忙的时间段是哪些

分析某个问题时，要想办法将解答问题的来源落实在数据集的各个指标上。当问题不够详细时，可以取一些具有代表性的值作为该问题的答案。

例如，航班分为到港（Arrive）和离港（Depart）航班，若统计所有机场在每天的某个时间段内离港航班数量，就能在一定程序上反映这个时段的航班是否繁忙。

那么，当我们提到“统计……的数量”的时候，我们在说什么？

数据集中的每一条记录都朴实地反映了航班的基本情况，但它们并不会直接告诉我们每一天、每一个时段都发生了什么。为了得到后者这样的信息，我们需要对数据进行筛选和统计。

于是我们会顺理成章地用到 AVG（平均值）、COUNT（计数）和 SUM（求和）等统计函数。

为了分时间段统计航班数量，我们可以大致地将一天的时间分为以下五段：

- 凌晨（00:00 - 06:00）：大部分人在这个时段都在休息，所以我们可以合理假设该时间段内航班数量较少。
- 早上（06:01 - 10:00）：一些早班机会选择在此时间出发，机场也通常从这个时间段起逐渐进入高峰。
- 中午（10:01 - 14:00）：早上从居住地出发的人们通常在这个时候方便抵达机场，因此选择在该时间段出发的航班可能更多。
- 下午（14:01 - 19:00）：同样，在下午出发更为方便，抵达目的地是刚好是晚上，又不至于太晚，方便找到落脚之处。
- 晚上（19:01 - 23:59）：在一天结束之际，接近凌晨的航班数量可能会更少。

这样的分段都是基于一些假设的。如果你认为你有更合理的分段方式，不妨将它们用到后续的分析工作中。

&gt; 不要忘了在草稿纸上记录下这些重要的时间段，在设计代码时会用到它们。

当我们所需的数据不是单个离散的数据而是基于一定范围的时候，我们可以用关键字 `BETWEEN x AND y` 来设置数据的起止范围。

有了上述准备，我们可以尝试写出统计离港时间在 0 点 至 6 点 间的航班总数。首先选取的目标是 flights 这张表，即 `FROM flights`。航班总数可以对 FlightNum 进行统计（使用COUNT函数），即 `COUNT(FlightNum)`。限定的条件是离港时间在 0 （代表 00:00）至 600 （代表 6:00）之间，即 `WHERE DepTime BETWEEN 0 AND 600`。所以我们要写出的语句是：

&gt; 请在 Spark Shell 中输入以下代码。

```
val queryFlightNumResult = sqlContext.sql(&#34;SELECT COUNT(FlightNum) FROM flights WHERE DepTime BETWEEN 0 AND 600&#34;)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479794404986.png/wm)

查看其中 1 条结果。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤比较耗时，请耐心等待计算完成。

```
queryFlightNumResult.take(1)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479794510992.png/wm)

在此基础上我们可以细化一下，计算出每天的平均离港航班数量，并且每次只选择 1 个月的数据。这里我们选择的时间段为 10:00 至 14:00 。

&gt; 请在 Spark Shell 中输入以下代码。

```
// COUNT(DISTINCT DayofMonth)的作用是计算每个月的天数
val queryFlightNumResult1 = sqlContext.sql(&#34;SELECT COUNT(FlightNum)/COUNT(DISTINCT DayofMonth) FROM flights WHERE Month = 1 AND DepTime BETWEEN 1001 AND 1400&#34;)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479795011052.png/wm)

查询得到的结果只有一条，即该月每天的平均离港航班数量。查看一下：

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤比较耗时，请耐心等待计算完成。

```
queryFlightNumResult1.take(1)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479795035090.png/wm)

你可以尝试计算出其他时间段的平均离港航班数量，并作记录。你的结论是否与下方的结论相同呢？

&gt; 最终统计的结果表明：1998年1月，每天最繁忙的时段为下午。该时段的平均离港航班数量为4356.7个。

### 4.2 飞哪最准时

要看飞哪最准时，实际上就是统计航班到港准点率。

可以先来查询到港延误时间为 0 的航班都是飞往哪里的。

在上面这句话中，有几个信息：

- 要查询的主要信息为目的地代码。
- 信息的来源为 flights 表。
- 查询的条件为到港延误时间（ArrDelay）为 0 。

在面对任何一个问题时，我们都可以仿照上面的思路对问题进行拆解，然后将每一条信息转化为对应的 SQL 语句。

&gt; 请尝试根据已提供的信息，完成 SQL 语句的设计。

于是最终我们可以得到这样的查询代码：

&gt; 请在 Spark Shell 中输入以下代码。

```
val queryDestResult = sqlContext.sql(&#34;SELECT DISTINCT Dest, ArrDelay FROM flights WHERE ArrDelay = 0&#34;)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479795898497.png/wm)

取出其中 5 条结果来看看。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤比较耗时，请耐心等待计算完成。

```
queryDestResult.head(5)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479795969616.png/wm)

在此基础上，我们尝试加入更多的限定条件。

我们可以统计出到港航班延误时间为 0 的次数（准点次数），并且最终输出的结果为 [目的地， 准点次数] ，并且按照降序对它们进行排列。

这一次我们不再给出信息拆分的提示，请尝试自己完成该步骤，然后与下方的代码进行比对。

最后查询的代码为：

&gt; 请在 Spark Shell 中输入以下代码。

```
val queryDestResult2 = sqlContext.sql(&#34;SELECT DISTINCT Dest, COUNT(ArrDelay) AS delayTimes FROM flights where ArrDelay = 0 GROUP BY Dest ORDER BY delayTimes DESC&#34;)
```

&gt; DISTINCT 关键字的作用是去除重复的结果。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479737110361.png/wm)


查看其中 10 条结果。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤比较耗时，请耐心等待计算完成。

```
queryDestResult2.head(10)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479737141676.png/wm)

在美国，一个州通常会有多个机场。我们在上一步得到的查询结果都是按照目的地的机场代码进行输出的。那么抽象到每一个州都有多少个准点的到港航班呢？

我们可以在上一次查询的基础上，再次进行嵌套的查询。并且，我们会用到另一个数据集 airports 中的信息：目的地中的三字代码（Dest）即该数据集中的 IATA 代码（iata），而每个机场都给出了它所在的州的信息（state）。我们可以通过一个联结操作将 airports 表加入到查询中。

此处直接给出查询的代码，请尝试理解每一段关键字的作用都是什么。

&gt; 请在 Spark Shell 中输入以下代码。

```
val queryDestResult3 = sqlContext.sql(&#34;SELECT DISTINCT state, SUM(delayTimes) AS s FROM (SELECT DISTINCT Dest, COUNT(ArrDelay) AS delayTimes FROM flights WHERE ArrDelay = 0 GROUP BY Dest ) a JOIN airports b ON a.Dest = b.iata GROUP BY state ORDER BY s DESC&#34;)

```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479783554645.png/wm)


查看其中 10 条结果。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤比较耗时，请耐心等待计算完成。

```
queryDestResult3.head(10)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479783521913.png/wm)

最后还可以将结果输出为 CSV 格式，保存在用户主目录下。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤比较耗时，请耐心等待计算完成。

```
// QueryDestResult.csv只是保存结果的文件夹名
queryDestResult3.save(&#34;/home/shiyanlou/QueryDestResult.csv&#34;, &#34;com.databricks.spark.csv&#34;)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479740449401.png/wm)

保存完毕后，我们还需要手动将其合并为一个文件。新打开一个终端，在终端中输入以下命令来进行文件合并。

```
# 进入到结果文件的目录
cd ~/QueryDestResult.csv/

# 使用通配符将每个part文件中的内容追加到 result.csv 文件中
cat part-* &gt;&gt; result.csv
```

最后打开 result.csv 文件就能看到最终结果，如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479785474137.png/wm)

**请注意**，在这一小节中，我们只是统计了每个州的到港准点航班数量。请仿照上面的思路，尝试自己动手计算每个州的航班准点率。

&gt; 提示：到港航班准点率 = 到港准点航班数量 / 到港航班总数

那么，到港航班准点率最高的州是哪个呢？

### 4.3 出发延误的重灾区都有哪些

解决问题的方式似乎变得越来越简单了。我们继续回答下一个问题：出发延误的重灾区都有哪些？

可以大胆地设置查询条件为离港延误时间大于 60 分钟，写出查询语句如下：

&gt; 查询语句从来都不是一次就写好的，尝试多次修改关键字和参数，以得到一个最优的结果。

&gt; 请在 Spark Shell 中输入以下代码。

```
val queryOriginResult = sqlContext.sql(&#34;SELECT DISTINCT Origin, DepDelay FROM flights where DepDelay &gt; 60 ORDER BY DepDelay DESC&#34;)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479735798575.png/wm)

因为数据已经按照降序的形式进行排列，所以我们取出前 10 个查询结果即为 1998 年内，延误最严重的十次航班及所在的离港机场。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤比较耗时，请耐心等待计算完成。

```
queryOriginResult.head(10)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479797650879.png/wm)

是时候完全依靠自己了。请你仿照解答“飞哪最准确”这个问题时的解答方式，联结 airports 表，查询哪些**机场**拥有最高的离港航班**晚点率**。

如果你有足够的能力，可以结合 airports 表中每个机场的经纬度（lat 和 long 字段）和相关的气象数据，得到某些机场是否是因为常年的气象原因（如频繁的飓风、雷暴等）导致了经常发生延误。

## 五、航班延误时间预测

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479741500194.png/wm)

*图片来自pexels.com*

### 5.1 引言

历史数据是对于过去已经发生的事情的一种记录，我们可以根据历史数据对过去进行总结。那么我们是否还能否据此来展望未来呢？

也许你首先想到的方法就是预测。谈到预测就不得不提到当下最热门的学科之一——机器学习。预测也是机器学习相关知识能够完成的任务之一。

在课程[《Spark 讲堂之 MLlib 入门》](https://www.shiyanlou.com/courses/600)中，我们知道：机器学习算法主要分为有监督学习和无监督学习。你可以仿照很多例子来学习，但更重要的是如何将其应用到实际的数据分析场景中。

能够做出合适的机器学习算法的选择，取决于你对于机器学习算法有多少的了解。机器学习算法的类型都有哪些，这类算法是用于解决什么问题的（例如对数据进行分类、找出数据的共同点等等），算法的输入输出都是什么？只有熟知这些问题的答案，在面临对一个场景进行算法的选择时才能信手拈来。此处也推荐大家阅读《机器学习实战》这本书，该书对于如何选择合适的算法有较为详尽的说明。

作为数据分析人员，我们学习机器学习的主要目的不是对机器学习算法进行各方面的改进（机器学习专家们在为此努力），最低的要求应当是能够将机器学习算法应用到实际的数据分析问题中。

### 5.2 引入相关的包

为了使用 Spark ML 的相关功能，我们需要引入下面这些包：

&gt; 请在 Spark Shell 中输入下面的代码。

```
import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479716786373.png/wm)

### 5.3 DataFrame 转换为 RDD

Spark ML 中的操作大部分是基于 RDD （分布式弹性数据集）来进行的。而之前我们读进来的数据集的数据类型为 DataFrame 。在 DataFrame 中的每一条记录即对应于 RDD 中的每一行值。为此，我们需要将 DataFrame 转换为 RDD。

首先数据从 DataFrame 类型转换为 RDD 类型。从 row 中取值时是按照数据集中各个字段取出 Flight 类中对应字段的值。例如排在第二的 `row(3)` 取出的是 DataFrame 中 DayofWeek 字段的值，对应的是 Flight 类中的 `dayOfWeek` 成员变量。

&gt; 请在 Spark Shell 中输入以下代码。

```
val tmpFlightDataRDD = flightData.map(row =&gt; row(2).toString+&#34;,&#34;+row(3).toString+&#34;,&#34;+row(5).toString+&#34;,&#34;+row(7).toString+&#34;,&#34;+row(8).toString+&#34;,&#34;+row(12).toString+&#34;,&#34;+ row(16).toString+&#34;,&#34;+row(17).toString+&#34;,&#34;+row(14).toString+&#34;,&#34;+row(15).toString)
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479716823473.png/wm)

接着需要建立一个类，将 RDD 中的部分字段映射到类的成员变量中。

&gt; 请在 Spark Shell 中输入以下代码。

```
case class Flight(dayOfMonth:Int, dayOfWeek:Int, crsDepTime:Double, crsArrTime:Double, uniqueCarrier:String, crsElapsedTime:Double, origin:String, dest:String, arrDelay:Int, depDelay:Int, delayFlag:Int)
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479716883528.png/wm)

在类 `Flight` 中，最后一个成员变量为 `delayFlag`。通过对数据的观察，我们知道部分航班的延误时间仅仅为几分钟（无论是出发还是抵达时），而通常此类延误都是可以容忍的。为了减少待处理的数据量，我们可以将延误定义为出发或抵达的延迟时间大于半个小时（即 30 分钟），从而将抵达延误时间和出发延误时间简化为延误标记 `delayFlag`。

可以先尝试写出伪代码：

```
if ArrDelayTime or DepDelayTime &gt; 30
	delayFlag = True
else
	delayFlag = False
```

接着我们按照上述逻辑，定义一个解析方法。该方法用于将 DataFrame 中的记录转换为 RDD 。

&gt; 请在 Spark Shell 中输入以下代码。

```
def parseFields(input: String): Flight = {
    val line = input.split(&#34;,&#34;)

    // 针对可能出现的无效值“NA”进行过滤
    var dayOfMonth = 0
    if(line(0) != &#34;NA&#34;){
        dayOfMonth = line(0).toInt
	}
    var dayOfWeek = 0
    if(line(1) != &#34;NA&#34;){
        dayOfWeek = line(1).toInt
	}

	var crsDepTime = 0.0
    if(line(2) != &#34;NA&#34;){
        crsDepTime = line(2).toDouble
	}

    var crsArrTime = 0.0
    if(line(3) != &#34;NA&#34;){
        crsArrTime = line(3).toDouble
	}

    var crsElapsedTime = 0.0
    if(line(5) != &#34;NA&#34;){
        crsElapsedTime = line(5).toDouble
	}

	var arrDelay = 0
    if(line(8) != &#34;NA&#34;){
        arrDelay = line(8).toInt
	}
    var depDelay = 0
    if(line(9) != &#34;NA&#34;){
        depDelay = line(9).toInt
    }

    // 根据延迟时间决定延迟标志是否为1
    var delayFlag = 0
    if(arrDelay &gt; 30 || depDelay &gt; 30){
        delayFlag = 1
    }
    Flight(dayOfMonth, dayOfWeek, crsDepTime, crsArrTime, line(4), crsElapsedTime, line(6), line(7), arrDelay, depDelay, delayFlag)
}
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479716981029.png/wm)

解析方法定义完成后，我们就是用 map 操作来解析 RDD 中的各个字段。

&gt; 请在 Spark Shell 中输入以下代码。

```
val flightRDD = tmpFlightDataRDD.map(parseFields)
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479717023764.png/wm)

可以尝试随机取出一个值检查解析是否成功。

&gt; 请在 Spark Shell 中输入以下代码。

```
flightRDD.take(1)
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479717189930.png/wm)

### 5.4 提取特征

为了建立分类模型，我们需要提取出航班数据的特征。在刚刚解析数据的一步中，我们设立 delayFlag 的目的就是为了定义两个类用于分类。因此你可以将其称之为标签（Label），这是分类中常用的一个手段。标签有两种，如果 delayFlag 为 1 ，则代表航班有延误；如果为 0 ，则代表没有延误。区分延误与否的标准正如之前所讨论的那样：抵达或出发的延误时间是否超过了 30 分钟。

对于数据集中的每条记录，现在它们都包含了标签和特征信息。特征则是每条记录在 Flight 类中对应的所有属性（从 dayOfMonth 一直到 delayFlag）。

下面，我们需要将上述的这些特征转换为数值特征。在 Flight 类中，有些属性已经是数值特征，而诸如 crsDepTime 和 uniqueCarrier 等属性还不是数值特征。在这一步中我们都要将它们转换为数值特征。例如，uniqueCarrier 这个特征通常是航空公司代码（ “WN” 等），我们按照先后顺序为它们进行编号，将字符串类型的特征转换为含有唯一 ID 的数值特征（如 “AA” 变成了 0，“AS” 变成了1，以此类推，实际运算时是按照字母先后顺序进行标记的）。

&gt; 请在 Spark Shell 中输入以下代码。

```
var id: Int = 0
var mCarrier: Map[String, Int] = Map()
flightRDD.map(flight =&gt; flight.uniqueCarrier).distinct.collect.foreach(x =&gt; {mCarrier += (x -&gt; id); id += 1})
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479717346077.png/wm)

计算完成后，我们来查看一下 carrier 中是否已经完成了从表示航空公司代码的字符串到对应的唯一 ID 之间的转换。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤比较耗时，请耐心等待计算完成。

```
mCarrier.toString
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479717407529.png/wm)

按照同样的逻辑，我们要为出发地 Origin 、目的地 Dest 进行字符串到数值的转换。

先是对 Origin 进行转换：

&gt; 请在 Spark Shell 中输入以下代码。

```
var id_1: Int = 0
var mOrigin: Map[String, Int] = Map()
// 这里的origin相当于一个“全局”变量，在每次map中我们都在对其进行修改

flightRDD.map(flight =&gt; flight.origin).distinct.collect.foreach(x =&gt; {mOrigin += (x -&gt; id_1); id_1 += 1})
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479717487497.png/wm)

最后是对 Dest 进行转换，不要忘了我们转换的目的是为了建立数值特征。

&gt; 请在 Spark Shell 中输入以下代码。

```
var id_2: Int = 0
var mDest: Map[String, Int] = Map()
flightRDD.map(flight =&gt; flight.dest).distinct.collect.foreach(x =&gt; {mDest += (x -&gt; id_2); id_2 += 1})
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479717568735.png/wm)

至此我们就将所有的特征都准备好了。

### 5.5 定义特征数组

我们在上一步用不同的数字代表了不同的特征，这些特征最后都将放入数组中，可以将其理解为建立了特征向量。

接下来，我们将所有的标签（延迟与否）和特征都以数值的形式存储到一个新的 RDD 中，用作机器学习算法的输入。

&gt; 请在 Spark Shell 中输入以下代码。

```
val featuredRDD = flightRDD.map(flight =&gt; {
  val vDayOfMonth = flight.dayOfMonth - 1
  val vDayOfWeek = flight.dayOfWeek - 1
  val vCRSDepTime = flight.crsDepTime
  val vCRSArrTime = flight.crsArrTime
  val vCarrierID = mCarrier(flight.uniqueCarrier)
  val vCRSElapsedTime = flight.crsElapsedTime
  val vOriginID = mOrigin(flight.origin)
  val vDestID = mDest(flight.dest)
  val vDelayFlag = flight.delayFlag

  // 返回值中，将所有字段都转换成Double类型以利于建模时使用相关API
  Array(vDelayFlag.toDouble, vDayOfMonth.toDouble, vDayOfWeek.toDouble, vCRSDepTime.toDouble, vCRSArrTime.toDouble, vCarrierID.toDouble, vCRSElapsedTime.toDouble, vOriginID.toDouble, vDestID.toDouble)
})
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479717658537.png/wm)


经历这个map 阶段后，我们得到了包含所有信息的特征数组，并且这些特征都是数值类型的。

尝试取出其中一个值来查看转换是否成功。

&gt; 请在 Spark Shell 中输入以下代码。

```
featuredRDD.take(1)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479717699388.png/wm)

### 5.6 创建标记点

此步骤中，我们需要将含有特征数组的 featuredRDD 转换为含有 `org.apache.spark.mllib.regression.LabeledPoint` 包中定义的标记点 LabeledPoints 的新 RDD 。在分类中，标记点含有两类信息，一是代表了数据点的标记，二是代表了特征向量类。

下面我们来完成这个转换。

&gt; 请在 Spark Shell 中输入以下代码。

```
// Label设定为 DelayFlag，Features设定为其他所有字段的值
val LabeledRDD = featuredRDD.map(x =&gt; LabeledPoint(x(0), Vectors.dense(x(1), x(2), x(3), x(4), x(5), x(6), x(7), x(8))))
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479718108836.png/wm)

尝试取出其中一个值来查看转换是否成功。

&gt; 请在 Spark Shell 中输入以下代码。

```
LabeledRDD.take(1)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479718141854.png/wm)

回顾一下之前所做的工作：我们得到了含有延误标记 DelayFlag 的数据，所有的航班都可以被标记为延误了或者没有延误。下面我们会将上述数据使用随机划分的方法，划分为训练集和测试集。

以下是详细比例说明：

- 在 LabeledRDD 中，数据标记为 DelayFlag = 0 的数据为**未延迟航班**；数据标记为 DelayFlag = 1 的数据为**已延迟航班**。
- 未延迟航班总数的 80% ，将与所有的已延迟航班组成新的数据集。新数据集的 70% 和 30% 将被划分为训练集和测试集。
- 不直接使用 LabeledRDD 中的数据来划分训练集和测试集的目的是：尽可能提高已延迟航班在测试集中的比例，让训练得到的模型能更精确地描述延迟的情况。

因此，我们首先来提取 LabeledRDD 中的所有未延迟航班，再随机提取其中的 80% 。

&gt; 请在 Spark Shell 中输入以下代码。

```
// 末尾的(1)是为了取这 80% 的部分
val notDelayedFlights = LabeledRDD.filter(x =&gt; x.label == 0).randomSplit(Array(0.8, 0.2))(1)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479722914430.png/wm)

接着我们提取所有的已延迟航班。

&gt; 请在 Spark Shell 中输入以下代码。

```
val delayedFlights = LabeledRDD.filter(x =&gt; x.label == 1)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479722957240.png/wm)

将上述二者组合成新的数据集，用于后续划分训练集和测试集。

&gt; 请在 Spark Shell 中输入以下代码。

```
val tmpTTData = notDelayedFlights ++ delayedFlights
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479723112288.png/wm)

最后我们将这个数据集按照约定的比例随机划分为训练集和测试集。

&gt; 请在 Spark Shell 中输入以下代码。

```
// TT意为Train &amp; Test
val TTData = tmpTTData.randomSplit(Array(0.7, 0.3))
val trainingData = TTData(0)
val testData = TTData(1)
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479723295528.png/wm)

### 5.7 训练模型

接下来，我们将会从训练集中提取特征（Feature）。这里会用到 Spark MLlib中的[决策树](http://spark.apache.org/docs/1.6.1/mllib-decision-tree.html)。决策树是一个预测模型，代表的是对象属性与对象值之间的一种映射关系。你可以在[百度百科](http://baike.baidu.com/item/%E5%86%B3%E7%AD%96%E6%A0%91)中详细了解决策树的原理。

希望在进行接下来的工作之前，你能够利用一些时间了解决策树，以便于更好地理解各项参数设置的含义。

在官方文档中，决策树的参数分为三类：
- 问题规格参数（Problem specification parameters）：这些参数描述了待求解问题和数据集。我们需要设置 `categoricalFeaturesInfo`这一项，它指明了哪些特征是已经明确的，以及这些特征都可以取多少明确的值。返回值是一个 Map 。例如 `Map(0 -&gt; 2, 4 -&gt; 10)` 表示特征 `0` 的取值有2个（0 和 1），特征 `4` 的取值有10个（从 0 到 9）。
- 停止准则（Stopping criteria）：这些参数决定了树的构造在什么时候停止（即停止添加新节点）。我们需要设置 `maxDepth` 这一项，它表示树的最大深度。更深的树可能更有表达力，但它也更难训练并且容易过拟合。
- 可调参数（Tunable parameters）：这些参数都是可选的。我们需要设置两个。第一个是 `maxBins`，表示离散连续特征时的桶信息数量。第二个是 `impurity` ，表示在选择候选分支时的杂质度。

我们要训练的模型是通过建立输入特征与已标记的输出间的联系。要用到的方法是决策树类 `DecisionTree` 自带的 `trainClassifier` 方法。通过使用该方法，我们能够得到一个决策树模型。

下面来尝试构造训练逻辑。

&gt; 请在 Spark Shell 中输入以下代码。

```
// 仿照 API 文档中的提示，构造各项参数
var paramCateFeaturesInfo = Map[Int, Int]()

// 第一个特征信息：下标为 0 ，表示 dayOfMonth 有 0 到 30 的取值。
paramCateFeaturesInfo += (0 -&gt; 31)

// 第二个特征信息：下标为 1 ，表示 dayOfWeek 有 0 到 6 的取值。
paramCateFeaturesInfo += (1 -&gt; 7)

// 第三、四个特征是出发和抵达时间，这里我们不会用到，故省略。

// 第五个特征信息：下标为 4 ，表示 uniqueCarrier 的所有取值。
paramCateFeaturesInfo += (4 -&gt; mCarrier.size)

// 第六个特征信息为飞行时间，同样忽略。

// 第七个特征信息：下标为 6 ，表示 origin 的所有取值。
paramCateFeaturesInfo += (6 -&gt; mOrigin.size)

// 第八个特征信息：下标为 7， 表示 dest 的所有取值。
paramCateFeaturesInfo += (7 -&gt; mDest.size)

// 分类的数量为 2，代表已延误航班和未延误航班。
val paramNumClasses = 2

// 下面的参数设置为经验值
val paramMaxDepth = 9
val paramMaxBins = 7000
val paramImpurity = &#34;gini&#34;
```

参数构造完成后，我们调用 trainClassfier 方法进行训练。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤需要耗费一定的时间，请耐心等候。

```
val flightDelayModel = DecisionTree.trainClassifier(trainingData, paramNumClasses, paramCateFeaturesInfo, paramImpurity, paramMaxDepth, paramMaxBins)
```

执行过程如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479726381049.png/wm)

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479726592994.png/wm)

等待训练完成后，我们可以尝试打印出这棵决策树。

```
tmpDM = flightDelayModel.toDebugString
print(tmpDM)
```

执行结果如下图所示，此处未显示所有的结果。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479726782168.png/wm)

决策树的内容看起来大致是多重的分支结构。如果有足够的耐心，你可以在草稿纸上将决策的条件逐一画出来。按照上述这些条件，我们就能对今后的一个输入值作出预测了。当然，预测的结果就是会延误或者不会延误。

### 5.8 测试模型

在模型训练完成之后，我们还需要检验模型的构造效果。因此，最后一步是使用测试集对模型进行测试。

&gt; 请在 Spark Shell 中输入以下代码。

```
// 使用决策树模型的predict方法按照输入进行预测，预测结果临时存放于 tmpPredictResult 中。最后与输入信息的标记组成元祖，作为最终的返回结果。
val predictResult = testData.map{flight =&gt; 
  val tmpPredictResult = flightDelayModel.predict(flight.features)
  (flight.label, tmpPredictResult)
}
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479727426992.png/wm)

尝试取出 10 组预测结果，看一下效果。

&gt; 请在 Spark Shell 中输入以下代码。

```
predictResult.take(10)
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479727538599.png/wm)

可以看到， 若 Label 的 0.0 与 PredictResult 的 0.0 是对应的，则表明预测结果是正确的。并且不是每一条预测值都是准确的。 

因此，我们可以按照这个评价标准来统计有多少条预测记录是准确的。

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤需要耗费一定的时间，请耐心等候。

```
val numOfCorrectPrediction = predictResult.filter{case (label, result) =&gt; (label == result)}.count()
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479732430762.png/wm)

最后计算预测的正确率：

&gt; 请在 Spark Shell 中输入以下代码。
&gt; **注意：**此步骤需要耗费一定的时间，请耐心等候。

```
// 使用toDouble是为了提高正确率的精度，否则两个long值相除仍然是long值。
val predictAccuracy = numOfCorrectPrediction/testData.count().toDouble
```

执行结果如下图所示。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479732517507.png/wm)

我们得到了该模型的预测正确率约为 64.8% ，可以说在实际的预测中还是有一定的应用价值的。为了提高预测的正确率，你可以考虑使用更多的数据进行模型的训练，并且在建立决策树时将参数调至最优。

## 六、实验总结

在本课程中，我们通过 Spark 对航班起降的记录数据进行分析，找出了造成航班延误的原因，并对航班延误情况进行了预测。

对于没有给出详细代码的问题，请尽量通过自己的努力，得到这些问题的答案。听一遍不如看一遍，看一遍不如做一遍。对于想要从事数据科学和数据分析工作的人而言，思考如何去做并付诸行动是最快的学习方式。

如果你在学习过程有任何疑问或者建议，都欢迎到实验楼的讨论区与我们交流。

## 常见问题

- Q：为什么不用国内的数据而使用国外的数据呢？
- A：国外的部分数据是开放使用的，更容易得到，而国内的数据大多是收费的。如果有条件，可以从 [飞常准](http://www.variflight.com/) 购买国内航班数据服务，进而对国内的航班延误情况进行分析。


- Q：无法访问外网时，应如何通过加载CSV解析库的方式进入Spark Shell？
- A：请按照下面的步骤进行配置。
  1. 在终端中输入命令： `wget http://labfile.oss.aliyuncs.com/courses/610/spark_csv.tar.gz` 下载相关的 jar 包。
  2. 将该压缩文件解压至 `/home/shiyanlou/.ivy/jars/` 目录中，确保该目录含有如图所示的以下三个 jar 包。
     ![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid162034labid2076timestamp1479635229366.png/wm)
  3. 在终端中输入命令 `spark-shell --packages com.databricks:spark-csv_2.11:1.1.0` 启动即可。

## 版权声明

本课程由[实验楼在线教育](https://www.shiyanlou.com/aboutus)独家发布，版权所有。任何单位或个人在未经授权的情况下，严禁转载或利用其它任何方式使用本作品。