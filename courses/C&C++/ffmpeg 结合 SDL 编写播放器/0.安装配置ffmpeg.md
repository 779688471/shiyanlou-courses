# 简介

## 一、实验简介

`ffmpeg ` 是一个很好的开源的多媒体库，它包括了编解码，格式转化，复用器，解复用器，数据流处理，过滤和播放大多数多媒体格式，既包括较老的格式也支持很新的格式，它具有高度的可移植性。`ffmpeg` 可以在各种环境下构建，运行。从 Linux , 到 Mac OS ， windows ，Android 都可以适用。

本实验将学习使用 `ffmpeg`  ，它提供了一套丰富的视音频解码库，和方便使用的工具。我们将学习编译 `ffpemg ` ，使用它的库来转换视频文件，并结合 `sdl` 编写播放器，从中学习到编解码的知识。

先放一张完成后的效果：

![](http://labfile.oss.aliyuncs.com/courses/670/9.gif)

### 1.1 知识点

- ffmpeg 库介绍
- 相关结构介绍
- 动画屏幕捕捉
- 格式转换

## 二、编译安装

虽然有源可以直接安装，但是如果你想安装某一特定版本，或者最新版的 ffmpeg ，源码安装无疑是最好的。

首先安装相关的依赖：

```
sudo apt-get update
sudo apt-get -y install autoconf automake build-essential libass-dev libfreetype6-dev \
  libsdl1.2-dev libtheora-dev libtool libva-dev libvdpau-dev libvorbis-dev libxcb1-dev libxcb-shm0-dev \
  libxcb-xfixes0-dev pkg-config texinfo zlib1g-dev
sudo apt-get install yasm
sudo apt-get install libx264-dev		# H.264 依赖库
sudo apt-get install cmake mercurial
sudo apt-get install libfdk-aac-dev		# aac audio 依赖库
sudo apt-get install libmp3lame-dev		# mp3 audio 依赖库
sudo apt-get install libopus-dev		# Opus audio 依赖库
sudo apt-get install ctags				# 文档工具
```

本次实验选择的是 ffmpeg 2.8.8 ，因为最新版 ffmpeg 更新了很多函数、宏，但是查看具体更新的信息比较麻烦，所以我们选择较老一点的版本，学习起来容易一点。以下为编译安装的过程，你也可以选择我提前准备好的包：

```
wget http://labfile.oss.aliyuncs.com/courses/682/ffmpeg.tar.gz
tar xvf ffmpeg-2.8.8.tar.xz 
sudo make install
```

手动安装的过程，在环境里大概需要1个小时：

```
wget http://labfile.oss.aliyuncs.com/courses/682/ffmpeg-2.8.8.tar.xz
tar xvf ffmpeg-2.8.8.tar.xz
cd ffmpeg-2.8.8
./configure
sudo make
sudo make install
```

安装完成后在 shell  中即可查看相关的信息：

```
 ffmpeg -v 
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid59274labid2234timestamp1477552879064.png/wm)

## 三、ffmpeg 工具介绍

### 3.1 ffmpeg 命令行工具

ffmpeg 工具是一个高效快速的命令行工具，进行视音频不同格式之间的转换，

ffmpeg可以读取任意数量的输入“文件”（可以是常规文件，管道，网络流，抓取设备等）读取，由 -i 选项指定，并写入任意数量的输出“文件” （由纯输出文件名指定，在命令行中发现的不能被解释为选项的任何东西被认为是输出文件名。）

每个输入或输出文件原则上可以包含任意数量的不同类型的流（视频/音频/字幕/附件/数据）。 流的数量或类型可以由容器格式规定。 选择哪些输入并输出到哪个要么自动完成，要么使用-map选项。

- 将输出文件的视频比特率设置为64 kbit/s

  ```
  ffmpeg -i input.avi -b:v 64k -bufsize 64k output.avi
  ```

- 强制输出文件的帧速率24帧：

  ```
  ffmpeg -i input.avi -r 24 output.avi
  ```

- 强制输入文件的帧速率(仅对raw格式有效)为1帧和输出文件的帧速率为24帧的：

  ```
  ffmpeg -r 1 -i input.m2v -r 24 output.avi
  ```

ffmpeg 的转码流程可以由下图表示：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid59274labid2234timestamp1477552888125.png/wm)

（ ffmpeg 调用 libavformat 库（包含 demuxers ）来读取输入文件，并从中获取包含编码数据的数据包。 当有多个输入文件时，ffmpeg 尝试通过输入流上的时间戳来保持它们的同步。

编码分组后传递到解码器。 解码器可以通过滤波进一步处理的未压缩帧（原始视频/ PCM音频/ ...）。 在过滤之后，帧被传递到编码器，编码它们并输出编码分组。 最后，它们被传递到复用器，复用器将编码的数据包写入输出文件。）

### 3.2 ffserver

ffserver 是用于音频和视频的流服务器。 它支持多种推送方式，从文件流传输或者实时流推送。 如果有足够大的推送存储空间，还可以查找每个实时流的位置。

ffserver 从一些实例接收预录制文件或 FFM 流作为输入，然后通过 RTP / RTSP / HTTP 将其流式传输。

对于每个推送，可以有不同格式的不同输出流，每个格式由配置文件中的 <Stream> 部分指定。

推送发布的网址如：

```
http://ffserver_ip_address:http_port/feed_name
```

其中 ffserver_ip_address 是安装 ffserver 的计算机的 IP 地址，http_port 是 HTTP 服务器的端口号（通过 HTTPPort 选项配置），feed_name 是配置文件中定义的相应订阅源的名称。

多个流可以连接到同一个推送上。例如，可以使用以下图形描述的情况：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid59274labid2234timestamp1477552896301.png/wm)

我们可以使用下面的命令来开启一个简单的范例：

```
ffserver -f doc/ffserver.conf &
ffmpeg -i INPUTFILE http://localhost:8090/feed1.ffm
```

然后可以在其他的电脑上输入 url 查看：

```
http://<linuxbox>:8090/test.asf
```

### 3.3 ffplay

ffplay 是一个使用 ffmpeg 库和 sdl 库的非常简单和便携的媒体播放器。 它主要用作各种 ffmpeg api 的测试平台。

每个流使用一些选项，例如比特率或编解码器。 流指定符用于精确地指定选项所属的流。流指定符是一个字符串，通常添加选项名称，并用冒号分隔。 例如。 -codec：a：1 ac3 包含a：1 流标识符，它匹配第二个音频流。 因此，它将为第二个音频流选择 ac3 编解码器。流指定符可以匹配多个流，以便将选项应用于所有流。 例如。 -b：128k 中的流说明符匹配所有音频流。空流说明符匹配所有流。 例如，-codec copy 或 -codec：copy 将复制所有流，而不重新编码。流标识符的可能形式是：


stream_index ：				  使用此索引匹配流。 例如。 -threads：1 4 将第二个流的线程计数设置为4。

stream_type[:stream_index]： stream_type 是以下之一：视频的'v'或'V'，音频的'a'，字幕的's'，数据的'd'和							          附件的't'。 'v'匹配所有视频流，'V'仅匹配未附加图片。如果给出                                            stream_index，则它匹配此类型的流 stream_index。 否则，它匹配此类型的所有流。

p:program_id[:stream_index]：如果给出 stream_index，则它将流与程序中具有 id program_id 的数字											stream_index 匹配。 否则，它匹配程序中的所有流。

stream_id or i:stream_id ：  通过流 id 匹配流（例如 MPEG-TS 容器中的 PID ）。

m:key[:value]	：			 匹配具有指定值的元数据标记键的流。 如果未指定值，则将包含给定标记的流与任何值							  进行匹配。匹配具有可用配置的流，必须定义编解码器，并且存在诸如视频尺寸或音频采                             样率的基本信息。请注意，在ffmpeg中，通过元数据进行的匹配仅适用于输入文件。


### 3.4 ffprobe

ffprobe从多媒体流收集信息，并以人和机器可读的方式打印。

例如，它可以用于检查由多媒体流使用的容器的格式以及包含在其中的每个媒体流的格式和类型。

如果在输入中指定了文件名，ffprobe 将尝试打开并探测文件内容。如果文件无法打开或识别为多媒体文件，则返回正的退出代码。

ffprobe 可以用作独立应用或与文本过滤器组合，其可以执行更复杂的处理，例如，统计处理或绘图。

选项用于列出 ffprobe 支持的某些格式，或用于指定要显示的信息，以及用于设置 ffprobe 将如何显示它。

ffprobe 输出设计为可以通过文本过滤器轻松解析，并且由选定 writer（由 print_format 选项指定）定义的一个或多个部分组成。

存储在容器或流中的元数据标签被识别并打印在相应的 “FORMAT” ，“STREAM” 或 “PROGRAM_STREAM” 部分中。

## 四、ffmpeg 为开发者准备的库

### 4.1 libavutil

libavutil 库是一个实用程序库，以辅助多媒体编程。 它包含安全可移植字符串函数，随机数生成器，数据结构，附加数学函数，加密和多媒体相关功能。 它不是 libavcodec 和 libavformat 所需的代码的库。

这个库的目标是：

```
模块化		它应该具有很少的相互依赖性和在 ./configure 期间禁用单个部件的可能性。
小		  源和对象都应该小。
高效		 它应该具有低CPU和内存使用。
有用		 它应该添加人们很需要的功能。
```

### 4.2 libavcodec

libavcodec 库提供通用编码/解码框架，并包含用于音频，视频和字幕流的多个解码器和编码器，以及多个比特流滤波器。

共享架构提供从比特流 i/o 到 dsp 优化的各种服务，并且使得它适合于实现健壮和快速的编解码器。

### 4.3 libavformat

`libavformat` 库为音频、视频和字幕流的复用和解复用（多路复用和解复用）提供了一个通用框架。 它包括多媒体容器格式的多个多路复用器和多路分解器。

它还支持几种输入和输出协议来访问媒体资源。

### 4.4 libavdevice

libavdevice 库提供了一个通用框架，用于从许多常见的多媒体输入/输出设备抓取和渲染，并支持多个输入和输出设备，包括 Video4Linux2，VfW，DShow和ALSA。

### 4.5 libavfilter

libavfilter 库提供了一个通用的音频/视频过滤框架，包含几个过滤器，源和接收器模块。

### 4.6 libswscale

libswscale 库执行高度优化的图像缩放和颜色空间和像素格式转换操作。

具体来说，此库执行以下转换：

```
重新调整：是更改视频大小的过程。 有几个重新调整选项和算法可用。 这通常是有损耗的过程。

像素格式转换：是转换图像的图像格式和颜色空间的过程，例如从平面 YUV420P 到 RGB24 转换。 它还可以处理 packed 转换，即从 packed 布局（属于在同一缓冲器中交织的不同平面的所有像素）转换为平面布局（属于存储在专用缓冲器或“平面”中的相同平面的所有样本）的转换。

如果源和目标颜色空间不同，这通常是有损过程。
```

### 4.7 libswresample

libswresample 库执行高度优化的音频重采样，重新矩阵化和样本格式转换操作。

具体来说，此库执行以下转换：

```
重新采样：是改变音频速率的过程，例如从 44100Hz 的高采样率到 8000Hz 。从高到低采样率的音频转换是有损耗的过程。有几个重采样选项和算法可用。

格式转换：是将采样类型（例如从16位有符号采样转换为无符号8位或浮点采样）的过程。当从 packed 布局（所有属于在相同缓冲器中交错的不同通道的样本）到平面布局（属于存储在专用缓冲器或“平面”中的相同通道的所有样本）时，它还处理 packed 转换。

重新矩阵化：是改变通道布局的过程，例如从立体声到单声道。当输入通道不能映射到输出流时，该过程是有损的，因为它涉及不同的增益因子和混合。
通过专用选项启用各种其他音频转换（例如拉伸和填充）。
```

## 五、制作动画屏幕捕捉

电影文件有几个部分组成，文件本身称为容器，容器的类型决定文件中信息的位置。 本节实验容器的示例是 avi 和quicktime。 接下来，我们需要处理很多流：例如通常有一个音频流和一个视频流。流中的数据元素被称为帧。 每个流由不同种类的编解码器编码。 编解码器定义实际数据如何被 COTED和 DECDODE --- 因此名称 CODEC。 编解码器的示例是 DivX 和 MP3 。 然后从流中读取分组。 数据包是可以包含数据位的数据片段，这些数据被解码成我们最终可以为我们的应用程序处理的原始帧。 为了我们的目的，每个分组包含完整的帧，或者在音频的情况下多个帧。

处理视频和音频流是非常容易的，属于基本层次：

```
从文件中找到视频流
从视频流中读取数据包
如果不是一帧完整的数据就继续执行上一步
对帧数据做一些处理
然后循环执行以上操作，直到结束
```

在本实验中我们将对帧数据做一些处理，比如将每一帧的图像转为 jpg或者bmp或者ppm等格式保存下来。

### 5.1 打开文件

首先注册 ffmpeg 库：

```
#include <libavcodec/avcodec.h>
#include <libavformat/avformat.h>
#include <ffmpeg/swscale.h>

#include <stdio.h>
...
int main(int argc, charg *argv[]) {
av_register_all();
```

这会将所有可用的文件格式和编解码器注册到我们的库中，以便在打开具有相应格式/编解码器的文件时自动使用。 注意，我们只需要 `av_register_all（）`一次，所以在 main（）中调用。 如果你喜欢，可以只注册某些文件格式和编解码器，但是这样比较麻烦。

接下来打开文件：

```
AVFormatContext *pFormatCtx = NULL;

// Open video file
if(avformat_open_input(&pFormatCtx, argv[1], NULL, 0, NULL)!=0)
  return -1; // Couldn't open file
```

我们从第一个参数获取文件名。 此函数读取文件头并将有关文件格式的信息存储在我们给出的 `AVFormatContext` 结构中。 最后三个参数用于指定文件格式，缓冲区大小和格式选项，但通过将其设置为NULL或0，libavformat将自动检测这些。
此函数只查看头文件，所以接下来我们需要检查文件中的流信息：

```
// Retrieve stream information
if(avformat_find_stream_info(pFormatCtx, NULL)<0)
  return -1; // Couldn't find stream information
```

此函数获取有效的数据流，没有就返回 -1。接下来编写一个方便的调试功能，可以显示我们资源的数据信息：

```
av_dump_format(pFormatCtx, 0, argv[1], 0);		//打印资源的信息
```

举例：vim test.c

```
#include <libavcodec/avcodec.h>
#include <libavformat/avformat.h>
#include <libswscale/swscale.h>

#include <stdio.h>
int main(int argc, char *argv[]) {
   AVFormatContext   *pFormatCtx = NULL;
   if(argc < 2) {
    	printf("Please provide a movie file\n");
    	return -1;
  }
   av_register_all();	
   if(avformat_open_input(&pFormatCtx, argv[1], NULL, NULL)!=0)
    	return -1; // Couldn't open file
   if(avformat_find_stream_info(pFormatCtx, NULL)<0)
    	return -1; // Couldn't find stream information
    
   av_dump_format(pFormatCtx, 0, argv[1], 0);
   avformat_close_input(&pFormatCtx);
   return 0;
   
}
```

```
gcc -Wall -ggdb test.c  -I/usr/local/include     -L/usr/local/lib -lavformat -lavcodec -lva-x11 -lva -lxcb-shm -lxcb-xfixes -lxcb-render -lxcb-shape -lxcb -lX11 -lasound -lz -lswresample -lswscale -lavutil -lm -pthread -o test.out 
```

```
wget http://labfile.oss.aliyuncs.com/courses/682/thoseyears.mp4
./test.out ./thoseyears.mp4
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid59274labid2234timestamp1477552916134.png/wm)

### 5.2 获取视频数据流

现在 `pFormatCtx->streams` 只是一个指针数组，大小为 `pFormatCtx-> nb_streams` ，所以让我们来看一下它，直到我们找到一个视频流。

```
int i;
AVCodecContext *pCodecCtxOrig = NULL;
AVCodecContext *pCodecCtx = NULL;

// Find the first video stream
videoStream=-1;
for(i=0; i<pFormatCtx->nb_streams; i++)
  if(pFormatCtx->streams[i]->codec->codec_type==AVMEDIA_TYPE_VIDEO) {
    videoStream=i;
    break;
  }
if(videoStream==-1)
  return -1; // Didn't find a video stream

// Get a pointer to the codec context for the video stream
pCodecCtx=pFormatCtx->streams[videoStream]->codec;
```

流的编解码器信息被称为编解码器上下文， 这包含了流使用的编解码器的所有信息，现在我们有一个指向它的指针。 但是我们还是要找到实际的编解码器并打开它：

```
AVCodec *pCodec = NULL;

// Find the decoder for the video stream
pCodec=avcodec_find_decoder(pCodecCtx->codec_id);
if(pCodec==NULL) {
  fprintf(stderr, "Unsupported codec!\n");
  return -1; // Codec not found
}
// Copy context
pCodecCtx = avcodec_alloc_context3(pCodec);
if(avcodec_copy_context(pCodecCtx, pCodecCtxOrig) != 0) {
  fprintf(stderr, "Couldn't copy codec context");
  return -1; // Error copying codec context
}
// Open codec
if(avcodec_open2(pCodecCtx, pCodec)<0)
  return -1; // Could not open codec
```

注意我们不能直接从视频流中使用 `AVCodecContext` ， 因此，我们必须使用 `avcodec_alloc_context()` 将上下文复制到一个新的位置（为它分配内存）。

### 5.3 存储数据

现在需要一个空间来存储一帧的数据：

```
AVFrame *pFrame = NULL;

//Allocate video frame
pFrame=av_frame_alloc();
```

现在我们计划输出 PPM 图片文件，PPM 存储 24-bit 的 RGB ，它是 portable 像素图片，是有 netpbm 项目定义的一系列的 portable 图片格式中的一个。这些图片格式都相对比较容易处理，跟平台无关，所以称之为 portable，简单理解，就是比较直接的图片格式，比如 PPM，其实就是把每一个点的 RGB 分别保存起来。所以，PPM 格式的文件是没有压缩的，相对比较大，但是由于图片格式简单，一般作为图片处理的中间文件（不会丢失文件信息），或者作为简单的图片格式保存。

我们将帧从其原格式转换为RGB。 ffmpeg 函数库可以做这些转换。 对于大多数项目，都需要将初始格式转换为特定的格式。 

```
//Allocate an AVFrame structure
pFrameRGB = av_frame_alloc();
if(pFrameRGB == NULL)
{
  return -1;
}
```

有了一帧的数据后，还需要将它存储在起来，`avpicture_get_size` 可以获取资源大小，然后我们分配内存空间：

```
uint8_t *buffer = NULL;
int numBytes;
//Detemine required buffer size and allocate buffer
numBytes = avpicture_get_size(PIX_FMT_RGB24, pCodecCtx->width, pCodecCtx->height);
buffer = (uint8_t* )av_malloc(numBytes* sizeof(uint8_t));
```

`av_malloc` 是 `ffmpeg` 分配内存用的，它可以合理分配内存，避免泄漏，多次释放和其他的问题，接着用 `avpicture_fill` 来处理分配空间后的帧数据。`AVPicture` 结构是 `AVFrame` 结构的子集，早期的时候 `AVFrame` 和 `AVPicture` 是一样的结构：

```
avpicture_fill((AVPicture *)pFrameRGB, buffer, PIX_FMT_RGB24, pCodecCtx->width, pCodecCtx->height);
```

### 5.4 读取数据

现在我们可以读取 video 中的数据流，将包中的数据转为 frame ，然后将 frame 转为我们需要的格式：

```
struct SwsContent *sws_ctx = NULL;     
int frameFinished;
AVPacket packet;
// initialize SWS context for software scaling
sws_ctx = sws_getContext(pCodecCtx->width,
    pCodecCtx->height,
    pCodecCtx->pix_fmt,
    pCodecCtx->width,
    pCodecCtx->height,
    PIX_FMT_RGB24,
    SWS_BILINEAR,
    NULL,
    NULL,
    NULL
    );

i=0;
while(av_read_frame(pFormatCtx, &packet)>=0) {
  // Is this a packet from the video stream?
  if(packet.stream_index==videoStream) {
	// Decode video frame
    avcodec_decode_video2(pCodecCtx, pFrame, &frameFinished, &packet);
    
    // Did we get a video frame?
    if(frameFinished) {
    // Convert the image from its native format to RGB
        sws_scale(sws_ctx, (uint8_t const * const *)pFrame->data,
		  pFrame->linesize, 0, pCodecCtx->height,
		  pFrameRGB->data, pFrameRGB->linesize);
	
        // Save the frame to disk
          SaveFrame(pFrameRGB, pCodecCtx->width, 
                    pCodecCtx->height, i);
          printf("decode %d frame\n", i);
          i++;
    }
  }
    
  // Free the packet that was allocated by av_read_frame
  av_free_packet(&packet);
}
```

该过程同样很简单：`av_read_frame()` 读取一个数据包并将其存储在 `AVPacket struct` 中。 注意，我们只分配了数据包结构。`ffmpeg` 为我们分配了内部数据，由 `packet.data` 指向。 这可以通过 `av_free_packe()` 释放。 `avcodec_decode_video()` 将数据包转换为我们的帧。`avcodec_decode_video()` 为我们设置 `frameFinished` 。 最后，我们使用 `sws_scale()` 从格式 `pCodecCtx-> pix_fmt` 转换为 RGB 。将 `AVFrame` 指针转换为 `AVPicture` 指针。 最后，我们将 frame 和 height 和 width 信息传递给 SaveFrame 函数。
现在我们需要做的是使 `SaveFrame` 函数将 RGB 信息写入 PPM 格式的文件。

```
void SaveFrame(AVFrame *pFrame, int width, int height, int iFrame) {
  FILE *pFile;
  char szFilename[32];
  int  y;
  
  // Open file
  sprintf(szFilename, "frame%d.ppm", iFrame);
  pFile=fopen(szFilename, "wb");
  if(pFile==NULL)
    return;
  
  // Write header
  fprintf(pFile, "P6\n%d %d\n255\n", width, height);
  
  // Write pixel data
  for(y=0; y<height; y++)
    fwrite(pFrame->data[0]+y*pFrame->linesize[0], 1, width*3, pFile);
  
  // Close file
  fclose(pFile);
}
```

文件打开后，然后写入 RGB 数据。 我们一次写一行文件。 PPM文件是一个在长字符串中布局的 RGB 信息的文件。 
现在，回到我们的main（）函数。 一旦我们完成从视频流读取，我们只需要清理内存：

```
// Free the RGB image
av_free(buffer);
av_free(pFrameRGB);

// Free the YUV frame
av_free(pFrame);

// Close the codecs
avcodec_close(pCodecCtx);
avcodec_close(pCodecCtxOrig);

// Close the video file
avformat_close_input(&pFormatCtx);

return 0;
```

用 av_free 来清理通过 avcode_alloc_frame 和 av_malloc 分配的内存：

```
gcc -Wall -ggdb project1.c  -I/usr/local/include     -L/usr/local/lib -lavformat -lavcodec -lva-x11 -lva -lxcb-shm -lxcb-xfixes -lxcb-render -lxcb-shape -lxcb -lX11 -lasound -lz -lswresample -lswscale -lavutil -lm -pthread -o project1.out 
```

```
./project1.out ./thoseyears.mp4
```

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid59274labid2234timestamp1477552928899.png/wm)

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid59274labid2234timestamp1477553025219.png/wm)

完成后可以看到在目录下多了很多的 ppm 文件，可以选择将 ppm 转为 jpg ，或者安装一个可以打开 ppm 文件格式的软件。所以我编写了一个 ffmpeg 的转码脚本：

```
#!/bin/sh
ff=`ls *.ppm`
for f in $ff
do
file=`echo  ${f%.*}`
ffmpeg -i "$file".ppm "$file".jpg
done 
mkdir -p jpgs
mv *.jpg jpgs
rm *.ppm
```

```
sh 1.sh
```

进入我们的 jpgs 目录，就可以看到相关的图片了。可以通过 firefox 查看：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid59274labid2234timestamp1477552941087.png)

这里也有完整的代码：

```
http://labfile.oss.aliyuncs.com/courses/682/project1.c
```

## 六、实验总结

本节实验学习了 ffmpeg 的基本内容，并编程实现了视频内容转为图片，这也是一个获取壁纸的好办法嘛，你学会了吗?

